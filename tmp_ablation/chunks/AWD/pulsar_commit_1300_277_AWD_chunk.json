{
  "antipattern_type": "AWD",
  "project_name": "pulsar",
  "commit_number": "commit_1300",
  "id": "277",
  "group_id": 5,
  "chunks": [
    {
      "file_path": "D:\\Disaster\\Codefield\\Code_Python\\Anti-patternRAG\\data\\AWD\\apache\\pulsar\\commit_1300\\277\\before\\pulsar-broker/src/test/java/org/apache/pulsar/broker/service/ReplicatorGlobalNSTest.java",
      "chunk_type": "clientClass",
      "ast_subtree": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n */\npackage org.apache.pulsar.broker.service;\n\nimport com.google.common.collect.Sets;\nimport lombok.Cleanup;\nimport org.apache.pulsar.broker.auth.MockedPulsarServiceBaseTest;\nimport org.apache.pulsar.client.api.MessageRoutingMode;\nimport org.apache.pulsar.client.api.PulsarClient;\nimport org.apache.pulsar.client.impl.ConsumerImpl;\nimport org.apache.pulsar.client.impl.ProducerImpl;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.testng.Assert;\nimport org.testng.annotations.AfterClass;\nimport org.testng.annotations.BeforeClass;\nimport org.testng.annotations.BeforeMethod;\nimport org.testng.annotations.Test;\n\nimport java.lang.reflect.Method;\nimport java.util.concurrent.TimeUnit;\n\n@Test(groups = \"broker-impl\")\npublic class ReplicatorGlobalNSTest extends ReplicatorTestBase {\n\n    protected String methodName;\n\n    @BeforeMethod\n    public void beforeMethod(Method m) {\n        methodName = m.getName();\n    }\n\n    @Override\n    @BeforeClass(timeOut = 300000)\n    public void setup() throws Exception {\n        super.setup();\n    }\n\n    @Override\n    @AfterClass(alwaysRun = true, timeOut = 300000)\n    public void cleanup() throws Exception {\n        super.cleanup();\n    }\n\n    /**\n     * If local cluster is removed from the global namespace then all topics under that namespace should be deleted from\n     * the cluster.\n     *\n     * @throws Exception\n     */\n    @Test\n    public void testRemoveLocalClusterOnGlobalNamespace() throws Exception {\n        log.info(\"--- Starting ReplicatorTest::testRemoveLocalClusterOnGlobalNamespace ---\");\n\n        final String namespace = \"pulsar/global/removeClusterTest\";\n        admin1.namespaces().createNamespace(namespace);\n        admin1.namespaces().setNamespaceReplicationClusters(namespace, Sets.newHashSet(\"r1\", \"r2\", \"r3\"));\n\n        final String topicName = \"persistent://\" + namespace + \"/topic\";\n\n        @Cleanup\n        PulsarClient client1 = PulsarClient.builder().serviceUrl(url1.toString()).statsInterval(0, TimeUnit.SECONDS)\n                .build();\n        @Cleanup\n        PulsarClient client2 = PulsarClient.builder().serviceUrl(url2.toString()).statsInterval(0, TimeUnit.SECONDS)\n                .build();\n\n        ProducerImpl<byte[]> producer1 = (ProducerImpl<byte[]>) client1.newProducer().topic(topicName)\n                .enableBatching(false).messageRoutingMode(MessageRoutingMode.SinglePartition).create();\n        ConsumerImpl<byte[]> consumer1 = (ConsumerImpl<byte[]>) client1.newConsumer().topic(topicName)\n                .subscriptionName(\"sub1\").subscribe();\n        ConsumerImpl<byte[]> consumer2 = (ConsumerImpl<byte[]>) client2.newConsumer().topic(topicName)\n                .subscriptionName(\"sub1\").subscribe();\n\n        admin1.namespaces().setNamespaceReplicationClusters(namespace, Sets.newHashSet(\"r2\", \"r3\"));\n\n        MockedPulsarServiceBaseTest\n                .retryStrategically((test) -> !pulsar1.getBrokerService().getTopics().containsKey(topicName), 50, 150);\n\n        Assert.assertFalse(pulsar1.getBrokerService().getTopics().containsKey(topicName));\n        Assert.assertFalse(producer1.isConnected());\n        Assert.assertFalse(consumer1.isConnected());\n        Assert.assertTrue(consumer2.isConnected());\n\n    }\n\n    @Test\n    public void testForcefullyTopicDeletion() throws Exception {\n        log.info(\"--- Starting ReplicatorTest::testForcefullyTopicDeletion ---\");\n\n        final String namespace = \"pulsar/removeClusterTest\";\n        admin1.namespaces().createNamespace(namespace);\n        admin1.namespaces().setNamespaceReplicationClusters(namespace, Sets.newHashSet(\"r1\"));\n\n        final String topicName = \"persistent://\" + namespace + \"/topic\";\n\n        @Cleanup\n        PulsarClient client1 = PulsarClient.builder().serviceUrl(url1.toString()).statsInterval(0, TimeUnit.SECONDS)\n                .build();\n\n        ProducerImpl<byte[]> producer1 = (ProducerImpl<byte[]>) client1.newProducer().topic(topicName)\n                .enableBatching(false).messageRoutingMode(MessageRoutingMode.SinglePartition).create();\n        producer1.close();\n\n        admin1.topics().delete(topicName, true);\n\n        MockedPulsarServiceBaseTest\n                .retryStrategically((test) -> !pulsar1.getBrokerService().getTopics().containsKey(topicName), 50, 150);\n\n        Assert.assertFalse(pulsar1.getBrokerService().getTopics().containsKey(topicName));\n    }\n\n    private static final Logger log = LoggerFactory.getLogger(ReplicatorGlobalNSTest.class);\n\n}\n"
    },
    {
      "file_path": "D:\\Disaster\\Codefield\\Code_Python\\Anti-patternRAG\\data\\AWD\\apache\\pulsar\\commit_1300\\277\\before\\pulsar-client/src/main/java/org/apache/pulsar/client/impl/ProducerBase.java",
      "chunk_type": "superType",
      "ast_subtree": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n */\npackage org.apache.pulsar.client.impl;\n\nimport static com.google.common.base.Preconditions.checkArgument;\nimport java.util.concurrent.CompletableFuture;\nimport org.apache.pulsar.client.api.Message;\nimport org.apache.pulsar.client.api.MessageId;\nimport org.apache.pulsar.client.api.Producer;\nimport org.apache.pulsar.client.api.PulsarClientException;\nimport org.apache.pulsar.client.api.Schema;\nimport org.apache.pulsar.client.api.SchemaSerializationException;\nimport org.apache.pulsar.client.api.TypedMessageBuilder;\nimport org.apache.pulsar.client.api.transaction.Transaction;\nimport org.apache.pulsar.client.impl.conf.ProducerConfigurationData;\nimport org.apache.pulsar.client.impl.transaction.TransactionImpl;\nimport org.apache.pulsar.common.protocol.schema.SchemaHash;\nimport org.apache.pulsar.common.util.FutureUtil;\nimport org.apache.pulsar.common.util.collections.ConcurrentOpenHashMap;\n\npublic abstract class ProducerBase<T> extends HandlerState implements Producer<T> {\n\n    protected final CompletableFuture<Producer<T>> producerCreatedFuture;\n    protected final ProducerConfigurationData conf;\n    protected final Schema<T> schema;\n    protected final ProducerInterceptors interceptors;\n    protected final ConcurrentOpenHashMap<SchemaHash, byte[]> schemaCache;\n    protected volatile MultiSchemaMode multiSchemaMode = MultiSchemaMode.Auto;\n\n    protected ProducerBase(PulsarClientImpl client, String topic, ProducerConfigurationData conf,\n            CompletableFuture<Producer<T>> producerCreatedFuture, Schema<T> schema, ProducerInterceptors interceptors) {\n        super(client, topic);\n        this.producerCreatedFuture = producerCreatedFuture;\n        this.conf = conf;\n        this.schema = schema;\n        this.interceptors = interceptors;\n        this.schemaCache =\n                ConcurrentOpenHashMap.<SchemaHash, byte[]>newBuilder().build();\n        if (!conf.isMultiSchema()) {\n            multiSchemaMode = MultiSchemaMode.Disabled;\n        }\n    }\n\n    @Override\n    public MessageId send(T message) throws PulsarClientException {\n        return newMessage().value(message).send();\n    }\n\n    @Override\n    public CompletableFuture<MessageId> sendAsync(T message) {\n        try {\n            return newMessage().value(message).sendAsync();\n        } catch (SchemaSerializationException e) {\n            return FutureUtil.failedFuture(e);\n        }\n    }\n\n    public CompletableFuture<MessageId> sendAsync(Message<?> message) {\n        return internalSendAsync(message);\n    }\n\n    @Override\n    public TypedMessageBuilder<T> newMessage() {\n        return new TypedMessageBuilderImpl<>(this, schema);\n    }\n\n    public <V> TypedMessageBuilder<V> newMessage(Schema<V> schema) {\n        checkArgument(schema != null);\n        return new TypedMessageBuilderImpl<>(this, schema);\n    }\n\n    @Override\n    public TypedMessageBuilder<T> newMessage(Transaction txn) {\n        checkArgument(txn instanceof TransactionImpl);\n\n\n        return new TypedMessageBuilderImpl<>(this, schema, (TransactionImpl) txn);\n    }\n\n    abstract CompletableFuture<MessageId> internalSendAsync(Message<?> message);\n\n    abstract CompletableFuture<MessageId> internalSendWithTxnAsync(Message<?> message, Transaction txn);\n\n    public MessageId send(Message<?> message) throws PulsarClientException {\n        try {\n            // enqueue the message to the buffer\n            CompletableFuture<MessageId> sendFuture = internalSendAsync(message);\n\n            if (!sendFuture.isDone()) {\n                // the send request wasn't completed yet (e.g. not failing at enqueuing), then attempt to triggerFlush\n                // it out\n                triggerFlush();\n            }\n\n            return sendFuture.get();\n        } catch (Exception e) {\n            throw PulsarClientException.unwrap(e);\n        }\n    }\n\n    @Override\n    public void flush() throws PulsarClientException {\n        try {\n            flushAsync().get();\n        } catch (Exception e) {\n            throw PulsarClientException.unwrap(e);\n        }\n    }\n\n    abstract void triggerFlush();\n\n    @Override\n    public void close() throws PulsarClientException {\n        try {\n            closeAsync().get();\n        } catch (Exception e) {\n            throw PulsarClientException.unwrap(e);\n        }\n    }\n\n    @Override\n    public abstract CompletableFuture<Void> closeAsync();\n\n    @Override\n    public String getTopic() {\n        return topic;\n    }\n\n    public ProducerConfigurationData getConfiguration() {\n        return conf;\n    }\n\n    public CompletableFuture<Producer<T>> producerCreatedFuture() {\n        return producerCreatedFuture;\n    }\n\n    protected Message<?> beforeSend(Message<?> message) {\n        if (interceptors != null) {\n            return interceptors.beforeSend(this, message);\n        } else {\n            return message;\n        }\n    }\n\n    protected void onSendAcknowledgement(Message<?> message, MessageId msgId, Throwable exception) {\n        if (interceptors != null) {\n            interceptors.onSendAcknowledgement(this, message, msgId, exception);\n        }\n    }\n\n    protected void onPartitionsChange(String topicName, int partitions) {\n        if (interceptors != null) {\n            interceptors.onPartitionsChange(topicName, partitions);\n        }\n    }\n\n    @Override\n    public String toString() {\n        return \"ProducerBase{\" + \"topic='\" + topic + '\\'' + '}';\n    }\n\n    public enum MultiSchemaMode {\n        Auto, Enabled, Disabled\n    }\n}\n"
    },
    {
      "file_path": "D:\\Disaster\\Codefield\\Code_Python\\Anti-patternRAG\\data\\AWD\\apache\\pulsar\\commit_1300\\277\\before\\pulsar-client/src/main/java/org/apache/pulsar/client/impl/ProducerImpl.java",
      "chunk_type": "subType",
      "ast_subtree": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n */\npackage org.apache.pulsar.client.impl;\n\nimport static com.google.common.base.Preconditions.checkArgument;\nimport static com.google.common.base.Preconditions.checkState;\nimport static com.scurrilous.circe.checksum.Crc32cIntChecksum.computeChecksum;\nimport static com.scurrilous.circe.checksum.Crc32cIntChecksum.resumeChecksum;\nimport static java.lang.String.format;\nimport static org.apache.pulsar.client.impl.MessageImpl.SchemaState.Broken;\nimport static org.apache.pulsar.client.impl.MessageImpl.SchemaState.None;\nimport static org.apache.pulsar.client.impl.ProducerBase.MultiSchemaMode.Auto;\nimport static org.apache.pulsar.client.impl.ProducerBase.MultiSchemaMode.Enabled;\nimport static org.apache.pulsar.common.protocol.Commands.hasChecksum;\nimport static org.apache.pulsar.common.protocol.Commands.readChecksum;\nimport static org.apache.pulsar.common.util.Runnables.catchingAndLoggingThrowables;\nimport com.google.common.annotations.VisibleForTesting;\nimport io.netty.buffer.ByteBuf;\nimport io.netty.util.AbstractReferenceCounted;\nimport io.netty.util.Recycler;\nimport io.netty.util.Recycler.Handle;\nimport io.netty.util.ReferenceCountUtil;\nimport io.netty.util.ReferenceCounted;\nimport io.netty.util.Timeout;\nimport io.netty.util.TimerTask;\nimport io.netty.util.concurrent.ScheduledFuture;\nimport java.io.IOException;\nimport java.nio.ByteBuffer;\nimport java.util.ArrayDeque;\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Optional;\nimport java.util.Queue;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.CopyOnWriteArrayList;\nimport java.util.concurrent.Semaphore;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.atomic.AtomicInteger;\nimport java.util.concurrent.atomic.AtomicIntegerFieldUpdater;\nimport java.util.concurrent.atomic.AtomicLongFieldUpdater;\nimport java.util.function.Consumer;\nimport org.apache.commons.lang3.StringUtils;\nimport org.apache.pulsar.client.api.BatcherBuilder;\nimport org.apache.pulsar.client.api.CompressionType;\nimport org.apache.pulsar.client.api.Message;\nimport org.apache.pulsar.client.api.MessageCrypto;\nimport org.apache.pulsar.client.api.MessageId;\nimport org.apache.pulsar.client.api.Producer;\nimport org.apache.pulsar.client.api.ProducerAccessMode;\nimport org.apache.pulsar.client.api.ProducerCryptoFailureAction;\nimport org.apache.pulsar.client.api.PulsarClientException;\nimport org.apache.pulsar.client.api.PulsarClientException.CryptoException;\nimport org.apache.pulsar.client.api.PulsarClientException.TimeoutException;\nimport org.apache.pulsar.client.api.Schema;\nimport org.apache.pulsar.client.api.transaction.Transaction;\nimport org.apache.pulsar.client.impl.conf.ProducerConfigurationData;\nimport org.apache.pulsar.client.impl.crypto.MessageCryptoBc;\nimport org.apache.pulsar.client.impl.schema.JSONSchema;\nimport org.apache.pulsar.client.impl.transaction.TransactionImpl;\nimport org.apache.pulsar.client.util.MathUtils;\nimport org.apache.pulsar.common.allocator.PulsarByteBufAllocator;\nimport org.apache.pulsar.common.api.proto.MessageMetadata;\nimport org.apache.pulsar.common.api.proto.ProtocolVersion;\nimport org.apache.pulsar.common.compression.CompressionCodec;\nimport org.apache.pulsar.common.compression.CompressionCodecProvider;\nimport org.apache.pulsar.common.naming.TopicName;\nimport org.apache.pulsar.common.protocol.ByteBufPair;\nimport org.apache.pulsar.common.protocol.Commands;\nimport org.apache.pulsar.common.protocol.Commands.ChecksumType;\nimport org.apache.pulsar.common.protocol.schema.SchemaHash;\nimport org.apache.pulsar.common.protocol.schema.SchemaVersion;\nimport org.apache.pulsar.common.schema.SchemaInfo;\nimport org.apache.pulsar.common.schema.SchemaType;\nimport org.apache.pulsar.common.util.DateFormatter;\nimport org.apache.pulsar.common.util.FutureUtil;\nimport org.apache.pulsar.common.util.RelativeTimeUtil;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\npublic class ProducerImpl<T> extends ProducerBase<T> implements TimerTask, ConnectionHandler.Connection {\n\n    // Producer id, used to identify a producer within a single connection\n    protected final long producerId;\n\n    // Variable is updated in a synchronized block\n    private volatile long msgIdGenerator;\n\n    private final OpSendMsgQueue pendingMessages;\n    private final Optional<Semaphore> semaphore;\n    private volatile Timeout sendTimeout = null;\n    private final long lookupDeadline;\n    private int chunkMaxMessageSize;\n\n    @SuppressWarnings(\"rawtypes\")\n    private static final AtomicLongFieldUpdater<ProducerImpl> PRODUCER_DEADLINE_UPDATER = AtomicLongFieldUpdater\n            .newUpdater(ProducerImpl.class, \"producerDeadline\");\n    @SuppressWarnings(\"unused\")\n    private volatile long producerDeadline = 0; // gets set on first successful connection\n\n    private final BatchMessageContainerBase batchMessageContainer;\n    private CompletableFuture<MessageId> lastSendFuture = CompletableFuture.completedFuture(null);\n    private LastSendFutureWrapper lastSendFutureWrapper = LastSendFutureWrapper.create(lastSendFuture);\n\n    // Globally unique producer name\n    private String producerName;\n    private final boolean userProvidedProducerName;\n\n    private String connectionId;\n    private String connectedSince;\n    private final int partitionIndex;\n\n    private final ProducerStatsRecorder stats;\n\n    private final CompressionCodec compressor;\n\n    static final AtomicLongFieldUpdater<ProducerImpl> LAST_SEQ_ID_PUBLISHED_UPDATER = AtomicLongFieldUpdater\n            .newUpdater(ProducerImpl.class, \"lastSequenceIdPublished\");\n    private volatile long lastSequenceIdPublished;\n\n    static final AtomicLongFieldUpdater<ProducerImpl> LAST_SEQ_ID_PUSHED_UPDATER = AtomicLongFieldUpdater\n            .newUpdater(ProducerImpl.class, \"lastSequenceIdPushed\");\n    protected volatile long lastSequenceIdPushed;\n    private volatile boolean isLastSequenceIdPotentialDuplicated;\n\n    private final MessageCrypto msgCrypto;\n\n    private ScheduledFuture<?> keyGeneratorTask = null;\n\n    private final Map<String, String> metadata;\n\n    private Optional<byte[]> schemaVersion = Optional.empty();\n\n    private final ConnectionHandler connectionHandler;\n\n    // A batch flush task is scheduled when one of the following is true:\n    // - A message is added to a message batch without also triggering a flush for that batch.\n    // - A batch flush task executes with messages in the batchMessageContainer, thus actually triggering messages.\n    // - A message was sent more recently than the configured BatchingMaxPublishDelayMicros. In this case, the task is\n    //   scheduled to run BatchingMaxPublishDelayMicros after the most recent send time.\n    // The goal is to optimize batch density while also ensuring that a producer never waits longer than the configured\n    // batchingMaxPublishDelayMicros to send a batch.\n    // Only update from within synchronized block on this producer.\n    private ScheduledFuture<?> batchFlushTask;\n    // The time, in nanos, of the last batch send. This field ensures that we don't deliver batches via the\n    // batchFlushTask before the batchingMaxPublishDelayMicros duration has passed.\n    private long lastBatchSendNanoTime;\n\n    private Optional<Long> topicEpoch = Optional.empty();\n    private final List<Throwable> previousExceptions = new CopyOnWriteArrayList<Throwable>();\n\n    private boolean errorState;\n\n    public ProducerImpl(PulsarClientImpl client, String topic, ProducerConfigurationData conf,\n                        CompletableFuture<Producer<T>> producerCreatedFuture, int partitionIndex, Schema<T> schema,\n                        ProducerInterceptors interceptors, Optional<String> overrideProducerName) {\n        super(client, topic, conf, producerCreatedFuture, schema, interceptors);\n        this.producerId = client.newProducerId();\n        this.producerName = conf.getProducerName();\n        this.userProvidedProducerName = StringUtils.isNotBlank(producerName);\n        this.partitionIndex = partitionIndex;\n        this.pendingMessages = createPendingMessagesQueue();\n        this.chunkMaxMessageSize = conf.getChunkMaxMessageSize() > 0\n                ? Math.min(conf.getChunkMaxMessageSize(), ClientCnx.getMaxMessageSize())\n                : ClientCnx.getMaxMessageSize();\n        if (conf.getMaxPendingMessages() > 0) {\n            this.semaphore = Optional.of(new Semaphore(conf.getMaxPendingMessages(), true));\n        } else {\n            this.semaphore = Optional.empty();\n        }\n        overrideProducerName.ifPresent(key -> this.producerName = key);\n\n        this.compressor = CompressionCodecProvider.getCompressionCodec(conf.getCompressionType());\n\n        if (conf.getInitialSequenceId() != null) {\n            long initialSequenceId = conf.getInitialSequenceId();\n            this.lastSequenceIdPublished = initialSequenceId;\n            this.lastSequenceIdPushed = initialSequenceId;\n            this.msgIdGenerator = initialSequenceId + 1L;\n        } else {\n            this.lastSequenceIdPublished = -1L;\n            this.lastSequenceIdPushed = -1L;\n            this.msgIdGenerator = 0L;\n        }\n\n        if (conf.isEncryptionEnabled()) {\n            String logCtx = \"[\" + topic + \"] [\" + producerName + \"] [\" + producerId + \"]\";\n\n            if (conf.getMessageCrypto() != null) {\n                this.msgCrypto = conf.getMessageCrypto();\n            } else {\n                // default to use MessageCryptoBc;\n                MessageCrypto msgCryptoBc;\n                try {\n                    msgCryptoBc = new MessageCryptoBc(logCtx, true);\n                } catch (Exception e) {\n                    log.error(\"MessageCryptoBc may not included in the jar in Producer. e:\", e);\n                    msgCryptoBc = null;\n                }\n                this.msgCrypto = msgCryptoBc;\n            }\n        } else {\n            this.msgCrypto = null;\n        }\n\n        if (this.msgCrypto != null) {\n            // Regenerate data key cipher at fixed interval\n            keyGeneratorTask = client.eventLoopGroup().scheduleWithFixedDelay(catchingAndLoggingThrowables(() -> {\n                try {\n                    msgCrypto.addPublicKeyCipher(conf.getEncryptionKeys(), conf.getCryptoKeyReader());\n                } catch (CryptoException e) {\n                    if (!producerCreatedFuture.isDone()) {\n                        log.warn(\"[{}] [{}] [{}] Failed to add public key cipher.\", topic, producerName, producerId);\n                        producerCreatedFuture.completeExceptionally(\n                                PulsarClientException.wrap(e,\n                                        String.format(\"The producer %s of the topic %s \"\n                                                        + \"adds the public key cipher was failed\",\n                                                producerName, topic)));\n                    }\n                }\n            }), 0L, 4L, TimeUnit.HOURS);\n        }\n\n        if (conf.getSendTimeoutMs() > 0) {\n            sendTimeout = client.timer().newTimeout(this, conf.getSendTimeoutMs(), TimeUnit.MILLISECONDS);\n        }\n\n        this.lookupDeadline = System.currentTimeMillis() + client.getConfiguration().getLookupTimeoutMs();\n        if (conf.isBatchingEnabled()) {\n            BatcherBuilder containerBuilder = conf.getBatcherBuilder();\n            if (containerBuilder == null) {\n                containerBuilder = BatcherBuilder.DEFAULT;\n            }\n            this.batchMessageContainer = (BatchMessageContainerBase) containerBuilder.build();\n            this.batchMessageContainer.setProducer(this);\n        } else {\n            this.batchMessageContainer = null;\n        }\n        if (client.getConfiguration().getStatsIntervalSeconds() > 0) {\n            stats = new ProducerStatsRecorderImpl(client, conf, this);\n        } else {\n            stats = ProducerStatsDisabled.INSTANCE;\n        }\n\n        if (conf.getProperties().isEmpty()) {\n            metadata = Collections.emptyMap();\n        } else {\n            metadata = Collections.unmodifiableMap(new HashMap<>(conf.getProperties()));\n        }\n\n        this.connectionHandler = new ConnectionHandler(this,\n            new BackoffBuilder()\n                .setInitialTime(client.getConfiguration().getInitialBackoffIntervalNanos(), TimeUnit.NANOSECONDS)\n                .setMax(client.getConfiguration().getMaxBackoffIntervalNanos(), TimeUnit.NANOSECONDS)\n                .setMandatoryStop(Math.max(100, conf.getSendTimeoutMs() - 100), TimeUnit.MILLISECONDS)\n                .create(),\n            this);\n\n        grabCnx();\n    }\n\n    protected void semaphoreRelease(final int releaseCountRequest) {\n        if (semaphore.isPresent()) {\n            if (!errorState) {\n                final int availableReleasePermits =\n                        conf.getMaxPendingMessages() - this.semaphore.get().availablePermits();\n                if (availableReleasePermits - releaseCountRequest < 0) {\n                    log.error(\"Semaphore permit release count request greater then availableReleasePermits\"\n                                    + \" : availableReleasePermits={}, releaseCountRequest={}\",\n                            availableReleasePermits, releaseCountRequest);\n                    errorState = true;\n                }\n            }\n            semaphore.get().release(releaseCountRequest);\n        }\n    }\n\n    protected OpSendMsgQueue createPendingMessagesQueue() {\n        return new OpSendMsgQueue();\n    }\n\n    public ConnectionHandler getConnectionHandler() {\n        return connectionHandler;\n    }\n\n    private boolean isBatchMessagingEnabled() {\n        return conf.isBatchingEnabled();\n    }\n\n    private boolean isMultiSchemaEnabled(boolean autoEnable) {\n        if (multiSchemaMode != Auto) {\n            return multiSchemaMode == Enabled;\n        }\n        if (autoEnable) {\n            multiSchemaMode = Enabled;\n            return true;\n        }\n        return false;\n    }\n\n    @Override\n    public long getLastSequenceId() {\n        return lastSequenceIdPublished;\n    }\n\n    @Override\n    CompletableFuture<MessageId> internalSendAsync(Message<?> message) {\n        CompletableFuture<MessageId> future = new CompletableFuture<>();\n\n        MessageImpl<?> interceptorMessage = (MessageImpl) beforeSend(message);\n        // Retain the buffer used by interceptors callback to get message. Buffer will release after complete\n        // interceptors.\n        interceptorMessage.getDataBuffer().retain();\n        if (interceptors != null) {\n            interceptorMessage.getProperties();\n        }\n        sendAsync(interceptorMessage, new SendCallback() {\n            SendCallback nextCallback = null;\n            MessageImpl<?> nextMsg = null;\n            long createdAt = System.nanoTime();\n\n            @Override\n            public CompletableFuture<MessageId> getFuture() {\n                return future;\n            }\n\n            @Override\n            public SendCallback getNextSendCallback() {\n                return nextCallback;\n            }\n\n            @Override\n            public MessageImpl<?> getNextMessage() {\n                return nextMsg;\n            }\n\n            @Override\n            public void sendComplete(Exception e) {\n                try {\n                    if (e != null) {\n                        stats.incrementSendFailed();\n                        onSendAcknowledgement(interceptorMessage, null, e);\n                        future.completeExceptionally(e);\n                    } else {\n                        onSendAcknowledgement(interceptorMessage, interceptorMessage.getMessageId(), null);\n                        future.complete(interceptorMessage.getMessageId());\n                        stats.incrementNumAcksReceived(System.nanoTime() - createdAt);\n                    }\n                } finally {\n                    interceptorMessage.getDataBuffer().release();\n                }\n\n                while (nextCallback != null) {\n                    SendCallback sendCallback = nextCallback;\n                    MessageImpl<?> msg = nextMsg;\n                    // Retain the buffer used by interceptors callback to get message. Buffer will release after\n                    // complete interceptors.\n                    try {\n                        msg.getDataBuffer().retain();\n                        if (e != null) {\n                            stats.incrementSendFailed();\n                            onSendAcknowledgement(msg, null, e);\n                            sendCallback.getFuture().completeExceptionally(e);\n                        } else {\n                            onSendAcknowledgement(msg, msg.getMessageId(), null);\n                            sendCallback.getFuture().complete(msg.getMessageId());\n                            stats.incrementNumAcksReceived(System.nanoTime() - createdAt);\n                        }\n                        nextMsg = nextCallback.getNextMessage();\n                        nextCallback = nextCallback.getNextSendCallback();\n                    } finally {\n                        msg.getDataBuffer().release();\n                    }\n                }\n            }\n\n            @Override\n            public void addCallback(MessageImpl<?> msg, SendCallback scb) {\n                nextMsg = msg;\n                nextCallback = scb;\n            }\n        });\n        return future;\n    }\n\n    @Override\n    CompletableFuture<MessageId> internalSendWithTxnAsync(Message<?> message, Transaction txn) {\n        if (txn == null) {\n            return internalSendAsync(message);\n        } else {\n            CompletableFuture<MessageId> completableFuture = new CompletableFuture<>();\n            if (!((TransactionImpl) txn).checkIfOpen(completableFuture)) {\n               return completableFuture;\n            }\n            return ((TransactionImpl) txn).registerProducedTopic(topic)\n                        .thenCompose(ignored -> internalSendAsync(message));\n        }\n    }\n\n    /**\n     * Compress the payload if compression is configured.\n     * @param payload\n     * @return a new payload\n     */\n    private ByteBuf applyCompression(ByteBuf payload) {\n        ByteBuf compressedPayload = compressor.encode(payload);\n        payload.release();\n        return compressedPayload;\n    }\n\n    public void sendAsync(Message<?> message, SendCallback callback) {\n        checkArgument(message instanceof MessageImpl);\n\n        if (!isValidProducerState(callback, message.getSequenceId())) {\n            return;\n        }\n\n        MessageImpl<?> msg = (MessageImpl<?>) message;\n        MessageMetadata msgMetadata = msg.getMessageBuilder();\n        ByteBuf payload = msg.getDataBuffer();\n        final int uncompressedSize = payload.readableBytes();\n\n        if (!canEnqueueRequest(callback, message.getSequenceId(), uncompressedSize)) {\n            return;\n        }\n\n        // If compression is enabled, we are compressing, otherwise it will simply use the same buffer\n        ByteBuf compressedPayload = payload;\n        boolean compressed = false;\n        // Batch will be compressed when closed\n        // If a message has a delayed delivery time, we'll always send it individually\n        if (!isBatchMessagingEnabled() || msgMetadata.hasDeliverAtTime()) {\n            compressedPayload = applyCompression(payload);\n            compressed = true;\n\n            // validate msg-size (For batching this will be check at the batch completion size)\n            int compressedSize = compressedPayload.readableBytes();\n            if (compressedSize > ClientCnx.getMaxMessageSize() && !this.conf.isChunkingEnabled()) {\n                compressedPayload.release();\n                String compressedStr = conf.getCompressionType() != CompressionType.NONE ? \"Compressed\" : \"\";\n                PulsarClientException.InvalidMessageException invalidMessageException =\n                        new PulsarClientException.InvalidMessageException(\n                                format(\"The producer %s of the topic %s sends a %s message with %d bytes that exceeds\"\n                                                + \" %d bytes\",\n                        producerName, topic, compressedStr, compressedSize, ClientCnx.getMaxMessageSize()));\n                completeCallbackAndReleaseSemaphore(uncompressedSize, callback, invalidMessageException);\n                return;\n            }\n        }\n\n        if (!msg.isReplicated() && msgMetadata.hasProducerName()) {\n            PulsarClientException.InvalidMessageException invalidMessageException =\n                new PulsarClientException.InvalidMessageException(\n                    format(\"The producer %s of the topic %s can not reuse the same message\", producerName, topic),\n                        msg.getSequenceId());\n            completeCallbackAndReleaseSemaphore(uncompressedSize, callback, invalidMessageException);\n            compressedPayload.release();\n            return;\n        }\n\n        if (!populateMessageSchema(msg, callback)) {\n            compressedPayload.release();\n            return;\n        }\n\n        // Update the message metadata before computing the payload chunk size to avoid a large message cannot be split\n        // into chunks.\n        updateMessageMetadata(msgMetadata, uncompressedSize);\n\n        // send in chunks\n        int totalChunks;\n        int payloadChunkSize;\n        if (canAddToBatch(msg) || !conf.isChunkingEnabled()) {\n            totalChunks = 1;\n            payloadChunkSize = ClientCnx.getMaxMessageSize();\n        } else {\n            // Reserve current metadata size for chunk size to avoid message size overflow.\n            // NOTE: this is not strictly bounded, as metadata will be updated after chunking.\n            // So there is a small chance that the final message size is larger than ClientCnx.getMaxMessageSize().\n            // But it won't cause produce failure as broker have 10 KB padding space for these cases.\n            payloadChunkSize = ClientCnx.getMaxMessageSize() - msgMetadata.getSerializedSize();\n            if (payloadChunkSize <= 0) {\n                PulsarClientException.InvalidMessageException invalidMessageException =\n                        new PulsarClientException.InvalidMessageException(\n                                format(\"The producer %s of the topic %s sends a message with %d bytes metadata that \"\n                                                + \"exceeds %d bytes\", producerName, topic,\n                                        msgMetadata.getSerializedSize(), ClientCnx.getMaxMessageSize()));\n                completeCallbackAndReleaseSemaphore(uncompressedSize, callback, invalidMessageException);\n                compressedPayload.release();\n                return;\n            }\n            payloadChunkSize = Math.min(chunkMaxMessageSize, payloadChunkSize);\n            totalChunks = MathUtils.ceilDiv(Math.max(1, compressedPayload.readableBytes()), payloadChunkSize);\n        }\n\n        // chunked message also sent individually so, try to acquire send-permits\n        for (int i = 0; i < (totalChunks - 1); i++) {\n            if (!conf.isBlockIfQueueFull() && !canEnqueueRequest(callback, message.getSequenceId(),\n                    0 /* The memory was already reserved */)) {\n                client.getMemoryLimitController().releaseMemory(uncompressedSize);\n                semaphoreRelease(i + 1);\n                return;\n            }\n        }\n\n        try {\n            int readStartIndex = 0;\n            ChunkedMessageCtx chunkedMessageCtx = totalChunks > 1 ? ChunkedMessageCtx.get(totalChunks) : null;\n            byte[] schemaVersion = totalChunks > 1 && msg.getMessageBuilder().hasSchemaVersion()\n                    ? msg.getMessageBuilder().getSchemaVersion() : null;\n            byte[] orderingKey = totalChunks > 1 && msg.getMessageBuilder().hasOrderingKey()\n                    ? msg.getMessageBuilder().getOrderingKey() : null;\n            // msg.messageId will be reset if previous message chunk is sent successfully.\n            final MessageId messageId = msg.getMessageId();\n            for (int chunkId = 0; chunkId < totalChunks; chunkId++) {\n                // Need to reset the schemaVersion, because the schemaVersion is based on a ByteBuf object in\n                // `MessageMetadata`, if we want to re-serialize the `SEND` command using a same `MessageMetadata`,\n                // we need to reset the ByteBuf of the schemaVersion in `MessageMetadata`, I think we need to\n                // reset `ByteBuf` objects in `MessageMetadata` after call the method `MessageMetadata#writeTo()`.\n                if (chunkId > 0) {\n                    if (schemaVersion != null) {\n                        msg.getMessageBuilder().setSchemaVersion(schemaVersion);\n                    }\n                    if (orderingKey != null) {\n                        msg.getMessageBuilder().setOrderingKey(orderingKey);\n                    }\n                }\n                if (chunkId > 0 && conf.isBlockIfQueueFull() && !canEnqueueRequest(callback,\n                        message.getSequenceId(), 0 /* The memory was already reserved */)) {\n                    client.getMemoryLimitController().releaseMemory(uncompressedSize - readStartIndex);\n                    semaphoreRelease(totalChunks - chunkId);\n                    return;\n                }\n                synchronized (this) {\n                    // Update the message metadata before computing the payload chunk size\n                    // to avoid a large message cannot be split into chunks.\n                    final long sequenceId = updateMessageMetadataSequenceId(msgMetadata);\n                    String uuid = totalChunks > 1 ? String.format(\"%s-%d\", producerName, sequenceId) : null;\n\n                    serializeAndSendMessage(msg, payload, sequenceId, uuid, chunkId, totalChunks,\n                            readStartIndex, payloadChunkSize, compressedPayload, compressed,\n                            compressedPayload.readableBytes(), callback, chunkedMessageCtx, messageId);\n                    readStartIndex = ((chunkId + 1) * payloadChunkSize);\n                }\n            }\n        } catch (PulsarClientException e) {\n            e.setSequenceId(msg.getSequenceId());\n            completeCallbackAndReleaseSemaphore(uncompressedSize, callback, e);\n        } catch (Throwable t) {\n            completeCallbackAndReleaseSemaphore(uncompressedSize, callback,\n                    new PulsarClientException(t, msg.getSequenceId()));\n        }\n    }\n\n    /**\n     * Update the message metadata except those fields that will be updated for chunks later.\n     *\n     * @param msgMetadata\n     * @param uncompressedSize\n     * @return the sequence id\n     */\n    private void updateMessageMetadata(final MessageMetadata msgMetadata, final int uncompressedSize) {\n        if (!msgMetadata.hasPublishTime()) {\n            msgMetadata.setPublishTime(client.getClientClock().millis());\n\n            checkArgument(!msgMetadata.hasProducerName());\n\n            msgMetadata.setProducerName(producerName);\n\n            // The field \"uncompressedSize\" is zero means the compression info were not set yet.\n            if (msgMetadata.getUncompressedSize() <= 0) {\n                if (conf.getCompressionType() != CompressionType.NONE) {\n                    msgMetadata\n                            .setCompression(CompressionCodecProvider.convertToWireProtocol(conf.getCompressionType()));\n                }\n                msgMetadata.setUncompressedSize(uncompressedSize);\n            }\n        }\n    }\n\n    private long updateMessageMetadataSequenceId(final MessageMetadata msgMetadata) {\n        final long sequenceId;\n        if (!msgMetadata.hasSequenceId()) {\n            sequenceId = msgIdGenerator++;\n            msgMetadata.setSequenceId(sequenceId);\n        } else {\n            sequenceId = msgMetadata.getSequenceId();\n        }\n        return sequenceId;\n    }\n\n    @Override\n    public int getNumOfPartitions() {\n        return 0;\n    }\n\n    private void serializeAndSendMessage(MessageImpl<?> msg,\n                                         ByteBuf payload,\n                                         long sequenceId,\n                                         String uuid,\n                                         int chunkId,\n                                         int totalChunks,\n                                         int readStartIndex,\n                                         int chunkMaxSizeInBytes,\n                                         ByteBuf compressedPayload,\n                                         boolean compressed,\n                                         int compressedPayloadSize,\n                                         SendCallback callback,\n                                         ChunkedMessageCtx chunkedMessageCtx,\n                                         MessageId messageId) throws IOException {\n        ByteBuf chunkPayload = compressedPayload;\n        MessageMetadata msgMetadata = msg.getMessageBuilder();\n        if (totalChunks > 1 && TopicName.get(topic).isPersistent()) {\n            chunkPayload = compressedPayload.slice(readStartIndex,\n                    Math.min(chunkMaxSizeInBytes, chunkPayload.readableBytes() - readStartIndex));\n            // don't retain last chunk payload and builder as it will be not needed for next chunk-iteration and it will\n            // be released once this chunk-message is sent\n            if (chunkId != totalChunks - 1) {\n                chunkPayload.retain();\n            }\n            if (uuid != null) {\n                msgMetadata.setUuid(uuid);\n            }\n            msgMetadata.setChunkId(chunkId)\n                .setNumChunksFromMsg(totalChunks)\n                .setTotalChunkMsgSize(compressedPayloadSize);\n        }\n\n        if (canAddToBatch(msg) && totalChunks <= 1) {\n            if (canAddToCurrentBatch(msg)) {\n                // should trigger complete the batch message, new message will add to a new batch and new batch\n                // sequence id use the new message, so that broker can handle the message duplication\n                if (sequenceId <= lastSequenceIdPushed) {\n                    isLastSequenceIdPotentialDuplicated = true;\n                    if (sequenceId <= lastSequenceIdPublished) {\n                        log.warn(\"Message with sequence id {} is definitely a duplicate\", sequenceId);\n                    } else {\n                        log.info(\"Message with sequence id {} might be a duplicate but cannot be determined at this\"\n                                + \" time.\", sequenceId);\n                    }\n                    doBatchSendAndAdd(msg, callback, payload);\n                } else {\n                    // Should flush the last potential duplicated since can't combine potential duplicated messages\n                    // and non-duplicated messages into a batch.\n                    if (isLastSequenceIdPotentialDuplicated) {\n                        doBatchSendAndAdd(msg, callback, payload);\n                    } else {\n                        // handle boundary cases where message being added would exceed\n                        // batch size and/or max message size\n                        boolean isBatchFull = batchMessageContainer.add(msg, callback);\n                        lastSendFuture = callback.getFuture();\n                        payload.release();\n                        triggerSendIfFullOrScheduleFlush(isBatchFull);\n                    }\n                    isLastSequenceIdPotentialDuplicated = false;\n                }\n            } else {\n                doBatchSendAndAdd(msg, callback, payload);\n            }\n        } else {\n            // in this case compression has not been applied by the caller\n            // but we have to compress the payload if compression is configured\n            if (!compressed) {\n                chunkPayload = applyCompression(chunkPayload);\n            }\n            ByteBuf encryptedPayload = encryptMessage(msgMetadata, chunkPayload);\n\n            // When publishing during replication, we need to set the correct number of message in batch\n            // This is only used in tracking the publish rate stats\n            int numMessages = msg.getMessageBuilder().hasNumMessagesInBatch()\n                    ? msg.getMessageBuilder().getNumMessagesInBatch()\n                    : 1;\n            final OpSendMsg op;\n            if (msg.getSchemaState() == MessageImpl.SchemaState.Ready) {\n                ByteBufPair cmd = sendMessage(producerId, sequenceId, numMessages, messageId, msgMetadata,\n                        encryptedPayload);\n                op = OpSendMsg.create(msg, cmd, sequenceId, callback);\n            } else {\n                op = OpSendMsg.create(msg, null, sequenceId, callback);\n                final MessageMetadata finalMsgMetadata = msgMetadata;\n                op.rePopulate = () -> {\n                    if (msgMetadata.hasChunkId()) {\n                        // The message metadata is shared between all chunks in a large message\n                        // We need to reset the chunk id for each call of this method\n                        // It's safe to do that because there is only 1 thread to manipulate this message metadata\n                        finalMsgMetadata.setChunkId(chunkId);\n                    }\n                    op.cmd = sendMessage(producerId, sequenceId, numMessages, messageId, finalMsgMetadata,\n                            encryptedPayload);\n                };\n            }\n            op.setNumMessagesInBatch(numMessages);\n            op.setBatchSizeByte(encryptedPayload.readableBytes());\n            if (totalChunks > 1) {\n                op.totalChunks = totalChunks;\n                op.chunkId = chunkId;\n            }\n            op.chunkedMessageCtx = chunkedMessageCtx;\n            lastSendFuture = callback.getFuture();\n            processOpSendMsg(op);\n        }\n    }\n\n    @VisibleForTesting\n    boolean populateMessageSchema(MessageImpl msg, SendCallback callback) {\n        MessageMetadata msgMetadataBuilder = msg.getMessageBuilder();\n        if (msg.getSchemaInternal() == schema) {\n            schemaVersion.ifPresent(v -> msgMetadataBuilder.setSchemaVersion(v));\n            msg.setSchemaState(MessageImpl.SchemaState.Ready);\n            return true;\n        }\n        // If the message is from the replicator and without replicated schema\n        // Which means the message is written with BYTES schema\n        // So we don't need to replicate schema to the remote cluster\n        if (msg.hasReplicateFrom() && msg.getSchemaInfoForReplicator() == null) {\n            msg.setSchemaState(MessageImpl.SchemaState.Ready);\n            return true;\n        }\n\n        if (!isMultiSchemaEnabled(true)) {\n            PulsarClientException.InvalidMessageException e = new PulsarClientException.InvalidMessageException(\n                    format(\"The producer %s of the topic %s is disabled the `MultiSchema`\", producerName, topic)\n                    , msg.getSequenceId());\n            completeCallbackAndReleaseSemaphore(msg.getUncompressedSize(), callback, e);\n            return false;\n        }\n\n        byte[] schemaVersion = schemaCache.get(msg.getSchemaHash());\n        if (schemaVersion != null) {\n            if (schemaVersion != SchemaVersion.Empty.bytes()) {\n                msgMetadataBuilder.setSchemaVersion(schemaVersion);\n            }\n\n            msg.setSchemaState(MessageImpl.SchemaState.Ready);\n        }\n\n        return true;\n    }\n\n    private boolean rePopulateMessageSchema(MessageImpl msg) {\n        byte[] schemaVersion = schemaCache.get(msg.getSchemaHash());\n        if (schemaVersion == null) {\n            return false;\n        }\n\n        if (schemaVersion != SchemaVersion.Empty.bytes()) {\n            msg.getMessageBuilder().setSchemaVersion(schemaVersion);\n        }\n\n        msg.setSchemaState(MessageImpl.SchemaState.Ready);\n        return true;\n    }\n\n    private void tryRegisterSchema(ClientCnx cnx, MessageImpl msg, SendCallback callback, long expectedCnxEpoch) {\n        if (!changeToRegisteringSchemaState()) {\n            return;\n        }\n        SchemaInfo schemaInfo = msg.hasReplicateFrom() ? msg.getSchemaInfoForReplicator() : msg.getSchemaInfo();\n        schemaInfo = Optional.ofNullable(schemaInfo)\n                                        .filter(si -> si.getType().getValue() > 0)\n                                        .orElse(Schema.BYTES.getSchemaInfo());\n        getOrCreateSchemaAsync(cnx, schemaInfo).handle((v, ex) -> {\n            if (ex != null) {\n                Throwable t = FutureUtil.unwrapCompletionException(ex);\n                log.warn(\"[{}] [{}] GetOrCreateSchema error\", topic, producerName, t);\n                if (t instanceof PulsarClientException.IncompatibleSchemaException) {\n                    msg.setSchemaState(MessageImpl.SchemaState.Broken);\n                    callback.sendComplete((PulsarClientException.IncompatibleSchemaException) t);\n                }\n            } else {\n                log.info(\"[{}] [{}] GetOrCreateSchema succeed\", topic, producerName);\n                // In broker, if schema version is an empty byte array, it means the topic doesn't have schema.\n                // In this case, we cache the schema version to `SchemaVersion.Empty.bytes()`.\n                // When we need to set the schema version of the message metadata,\n                // we should check if the cached schema version is `SchemaVersion.Empty.bytes()`\n                if (v.length != 0) {\n                    schemaCache.putIfAbsent(msg.getSchemaHash(), v);\n                    msg.getMessageBuilder().setSchemaVersion(v);\n                } else {\n                    schemaCache.putIfAbsent(msg.getSchemaHash(), SchemaVersion.Empty.bytes());\n                }\n                msg.setSchemaState(MessageImpl.SchemaState.Ready);\n            }\n            cnx.ctx().channel().eventLoop().execute(() -> {\n                synchronized (ProducerImpl.this) {\n                    recoverProcessOpSendMsgFrom(cnx, msg, expectedCnxEpoch);\n                }\n            });\n            return null;\n        });\n    }\n\n    private CompletableFuture<byte[]> getOrCreateSchemaAsync(ClientCnx cnx, SchemaInfo schemaInfo) {\n        if (!Commands.peerSupportsGetOrCreateSchema(cnx.getRemoteEndpointProtocolVersion())) {\n            return FutureUtil.failedFuture(\n                new PulsarClientException.NotSupportedException(\n                    format(\"The command `GetOrCreateSchema` is not supported for the protocol version %d. \"\n                            + \"The producer is %s, topic is %s\",\n                            cnx.getRemoteEndpointProtocolVersion(), producerName, topic)));\n        }\n        long requestId = client.newRequestId();\n        ByteBuf request = Commands.newGetOrCreateSchema(requestId, topic, schemaInfo);\n        log.info(\"[{}] [{}] GetOrCreateSchema request\", topic, producerName);\n        return cnx.sendGetOrCreateSchema(request, requestId);\n    }\n\n    protected ByteBuf encryptMessage(MessageMetadata msgMetadata, ByteBuf compressedPayload)\n            throws PulsarClientException {\n\n        if (!conf.isEncryptionEnabled() || msgCrypto == null) {\n            return compressedPayload;\n        }\n\n        try {\n            int maxSize = msgCrypto.getMaxOutputSize(compressedPayload.readableBytes());\n            ByteBuf encryptedPayload = PulsarByteBufAllocator.DEFAULT.buffer(maxSize);\n            ByteBuffer targetBuffer = encryptedPayload.nioBuffer(0, maxSize);\n\n            msgCrypto.encrypt(conf.getEncryptionKeys(), conf.getCryptoKeyReader(), () -> msgMetadata,\n                    compressedPayload.nioBuffer(), targetBuffer);\n\n            encryptedPayload.writerIndex(targetBuffer.remaining());\n            compressedPayload.release();\n            return encryptedPayload;\n        } catch (PulsarClientException e) {\n            // Unless config is set to explicitly publish un-encrypted message upon failure, fail the request\n            if (conf.getCryptoFailureAction() == ProducerCryptoFailureAction.SEND) {\n                log.warn(\"[{}] [{}] Failed to encrypt message {}. Proceeding with publishing unencrypted message\",\n                        topic, producerName, e.getMessage());\n                return compressedPayload;\n            }\n            throw e;\n        }\n    }\n\n    protected ByteBufPair sendMessage(long producerId, long sequenceId, int numMessages,\n                                      MessageId messageId, MessageMetadata msgMetadata,\n                                      ByteBuf compressedPayload) {\n        if (messageId instanceof MessageIdImpl) {\n            return Commands.newSend(producerId, sequenceId, numMessages, getChecksumType(),\n                    ((MessageIdImpl) messageId).getLedgerId(), ((MessageIdImpl) messageId).getEntryId(),\n                    msgMetadata, compressedPayload);\n        } else {\n            return Commands.newSend(producerId, sequenceId, numMessages, getChecksumType(), -1, -1, msgMetadata,\n                    compressedPayload);\n        }\n    }\n\n    protected ByteBufPair sendMessage(long producerId, long lowestSequenceId, long highestSequenceId, int numMessages,\n                                      MessageMetadata msgMetadata, ByteBuf compressedPayload) {\n        return Commands.newSend(producerId, lowestSequenceId, highestSequenceId, numMessages, getChecksumType(),\n                msgMetadata, compressedPayload);\n    }\n\n    protected ChecksumType getChecksumType() {\n        if (connectionHandler.cnx() == null\n                || connectionHandler.cnx().getRemoteEndpointProtocolVersion() >= brokerChecksumSupportedVersion()) {\n            return ChecksumType.Crc32c;\n        } else {\n            return ChecksumType.None;\n        }\n    }\n\n    private boolean canAddToBatch(MessageImpl<?> msg) {\n        return msg.getSchemaState() == MessageImpl.SchemaState.Ready\n                && isBatchMessagingEnabled() && !msg.getMessageBuilder().hasDeliverAtTime();\n    }\n\n    private boolean canAddToCurrentBatch(MessageImpl<?> msg) {\n        return batchMessageContainer.haveEnoughSpace(msg)\n               && (!isMultiSchemaEnabled(false) || batchMessageContainer.hasSameSchema(msg))\n                && batchMessageContainer.hasSameTxn(msg);\n    }\n\n    private void triggerSendIfFullOrScheduleFlush(boolean isBatchFull) {\n        if (isBatchFull) {\n            batchMessageAndSend(false);\n        } else {\n            maybeScheduleBatchFlushTask();\n        }\n    }\n\n    private void doBatchSendAndAdd(MessageImpl<?> msg, SendCallback callback, ByteBuf payload) {\n        if (log.isDebugEnabled()) {\n            log.debug(\"[{}] [{}] Closing out batch to accommodate large message with size {}\", topic, producerName,\n                    msg.getUncompressedSize());\n        }\n        try {\n            batchMessageAndSend(false);\n            boolean isBatchFull = batchMessageContainer.add(msg, callback);\n            triggerSendIfFullOrScheduleFlush(isBatchFull);\n            lastSendFuture = callback.getFuture();\n        } finally {\n            payload.release();\n        }\n    }\n\n    private boolean isValidProducerState(SendCallback callback, long sequenceId) {\n        switch (getState()) {\n        case Ready:\n            // OK\n        case Connecting:\n            // We are OK to queue the messages on the client, it will be sent to the broker once we get the connection\n        case RegisteringSchema:\n            // registering schema\n            return true;\n        case Closing:\n        case Closed:\n            callback.sendComplete(\n                    new PulsarClientException.AlreadyClosedException(\"Producer already closed\", sequenceId));\n            return false;\n        case ProducerFenced:\n            callback.sendComplete(new PulsarClientException.ProducerFencedException(\"Producer was fenced\"));\n            return false;\n        case Terminated:\n            callback.sendComplete(\n                    new PulsarClientException.TopicTerminatedException(\"Topic was terminated\", sequenceId));\n            return false;\n        case Failed:\n        case Uninitialized:\n        default:\n            callback.sendComplete(new PulsarClientException.NotConnectedException(sequenceId));\n            return false;\n        }\n    }\n\n    private boolean canEnqueueRequest(SendCallback callback, long sequenceId, int payloadSize) {\n        try {\n            if (conf.isBlockIfQueueFull()) {\n                if (semaphore.isPresent()) {\n                    semaphore.get().acquire();\n                }\n                client.getMemoryLimitController().reserveMemory(payloadSize);\n            } else {\n                if (!semaphore.map(Semaphore::tryAcquire).orElse(true)) {\n                    callback.sendComplete(new PulsarClientException.ProducerQueueIsFullError(\n                            \"Producer send queue is full\", sequenceId));\n                    return false;\n                }\n\n                if (!client.getMemoryLimitController().tryReserveMemory(payloadSize)) {\n                    semaphore.ifPresent(Semaphore::release);\n                    callback.sendComplete(new PulsarClientException.MemoryBufferIsFullError(\n                            \"Client memory buffer is full\", sequenceId));\n                    return false;\n                }\n            }\n        } catch (InterruptedException e) {\n            Thread.currentThread().interrupt();\n            callback.sendComplete(new PulsarClientException(e, sequenceId));\n            return false;\n        }\n\n        return true;\n    }\n\n    private static final class WriteInEventLoopCallback implements Runnable {\n        private ProducerImpl<?> producer;\n        private ByteBufPair cmd;\n        private long sequenceId;\n        private ClientCnx cnx;\n        private OpSendMsg op;\n\n        static WriteInEventLoopCallback create(ProducerImpl<?> producer, ClientCnx cnx, OpSendMsg op) {\n            WriteInEventLoopCallback c = RECYCLER.get();\n            c.producer = producer;\n            c.cnx = cnx;\n            c.sequenceId = op.sequenceId;\n            c.cmd = op.cmd;\n            c.op = op;\n            return c;\n        }\n\n        @Override\n        public void run() {\n            if (log.isDebugEnabled()) {\n                log.debug(\"[{}] [{}] Sending message cnx {}, sequenceId {}\", producer.topic, producer.producerName, cnx,\n                        sequenceId);\n            }\n\n            try {\n                cnx.ctx().writeAndFlush(cmd, cnx.ctx().voidPromise());\n                op.updateSentTimestamp();\n            } finally {\n                recycle();\n            }\n        }\n\n        private void recycle() {\n            producer = null;\n            cnx = null;\n            cmd = null;\n            sequenceId = -1;\n            op = null;\n            recyclerHandle.recycle(this);\n        }\n\n        private final Handle<WriteInEventLoopCallback> recyclerHandle;\n\n        private WriteInEventLoopCallback(Handle<WriteInEventLoopCallback> recyclerHandle) {\n            this.recyclerHandle = recyclerHandle;\n        }\n\n        private static final Recycler<WriteInEventLoopCallback> RECYCLER = new Recycler<WriteInEventLoopCallback>() {\n            @Override\n            protected WriteInEventLoopCallback newObject(Handle<WriteInEventLoopCallback> handle) {\n                return new WriteInEventLoopCallback(handle);\n            }\n        };\n    }\n\n    private static final class LastSendFutureWrapper {\n        private final CompletableFuture<MessageId> lastSendFuture;\n        private static final int FALSE = 0;\n        private static final int TRUE = 1;\n        private static final AtomicIntegerFieldUpdater<LastSendFutureWrapper> THROW_ONCE_UPDATER =\n                AtomicIntegerFieldUpdater.newUpdater(LastSendFutureWrapper.class, \"throwOnce\");\n        private volatile int throwOnce = FALSE;\n\n        private LastSendFutureWrapper(CompletableFuture<MessageId> lastSendFuture) {\n            this.lastSendFuture = lastSendFuture;\n        }\n        static LastSendFutureWrapper create(CompletableFuture<MessageId> lastSendFuture) {\n            return new LastSendFutureWrapper(lastSendFuture);\n        }\n        public CompletableFuture<Void> handleOnce() {\n            return lastSendFuture.handle((ignore, t) -> {\n                if (t != null && THROW_ONCE_UPDATER.compareAndSet(this, FALSE, TRUE)) {\n                    throw FutureUtil.wrapToCompletionException(t);\n                }\n                return null;\n            });\n        }\n    }\n\n\n    @Override\n    public CompletableFuture<Void> closeAsync() {\n        final State currentState = getAndUpdateState(state -> {\n            if (state == State.Closed) {\n                return state;\n            }\n            return State.Closing;\n        });\n\n        if (currentState == State.Closed || currentState == State.Closing) {\n            return CompletableFuture.completedFuture(null);\n        }\n\n        closeProducerTasks();\n\n        ClientCnx cnx = cnx();\n        if (cnx == null || currentState != State.Ready) {\n            log.info(\"[{}] [{}] Closed Producer (not connected)\", topic, producerName);\n            closeAndClearPendingMessages();\n            return CompletableFuture.completedFuture(null);\n        }\n\n        long requestId = client.newRequestId();\n        ByteBuf cmd = Commands.newCloseProducer(producerId, requestId);\n\n        CompletableFuture<Void> closeFuture = new CompletableFuture<>();\n        cnx.sendRequestWithId(cmd, requestId).handle((v, exception) -> {\n            cnx.removeProducer(producerId);\n            if (exception == null || !cnx.ctx().channel().isActive()) {\n                // Either we've received the success response for the close producer command from the broker, or the\n                // connection did break in the meantime. In any case, the producer is gone.\n                log.info(\"[{}] [{}] Closed Producer\", topic, producerName);\n                closeAndClearPendingMessages();\n                closeFuture.complete(null);\n            } else {\n                closeFuture.completeExceptionally(exception);\n            }\n\n            return null;\n        });\n\n        return closeFuture;\n    }\n\n    private synchronized void closeAndClearPendingMessages() {\n        setState(State.Closed);\n        client.cleanupProducer(this);\n        PulsarClientException ex = new PulsarClientException.AlreadyClosedException(\n                format(\"The producer %s of the topic %s was already closed when closing the producers\",\n                        producerName, topic));\n        // Use null for cnx to ensure that the pending messages are failed immediately\n        failPendingMessages(null, ex);\n    }\n\n    @Override\n    public boolean isConnected() {\n        return getCnxIfReady() != null;\n    }\n\n    /**\n     * Hook method for testing. By returning null, it's possible to prevent messages\n     * being delivered to the broker.\n     *\n     * @return cnx if OpSend messages should be written to open connection. Caller must\n     * verify that the returned cnx is not null before using reference.\n     */\n    protected ClientCnx getCnxIfReady() {\n        if (getState() == State.Ready) {\n            return connectionHandler.cnx();\n        } else {\n            return null;\n        }\n    }\n\n    @Override\n    public long getLastDisconnectedTimestamp() {\n        return connectionHandler.lastConnectionClosedTimestamp;\n    }\n\n    public boolean isWritable() {\n        ClientCnx cnx = connectionHandler.cnx();\n        return cnx != null && cnx.channel().isWritable();\n    }\n\n    public void terminated(ClientCnx cnx) {\n        State previousState = getAndUpdateState(state -> (state == State.Closed ? State.Closed : State.Terminated));\n        if (previousState != State.Terminated && previousState != State.Closed) {\n            log.info(\"[{}] [{}] The topic has been terminated\", topic, producerName);\n            setClientCnx(null);\n            synchronized (this) {\n                failPendingMessages(cnx,\n                        new PulsarClientException.TopicTerminatedException(\n                                format(\"The topic %s that the producer %s produces to has been terminated\",\n                                        topic, producerName)));\n            }\n        }\n    }\n\n    void ackReceived(ClientCnx cnx, long sequenceId, long highestSequenceId, long ledgerId, long entryId) {\n        OpSendMsg op = null;\n        synchronized (this) {\n            op = pendingMessages.peek();\n            if (op == null) {\n                if (log.isDebugEnabled()) {\n                    log.debug(\"[{}] [{}] Got ack for timed out msg {} - {}\",\n                            topic, producerName, sequenceId, highestSequenceId);\n                }\n                return;\n            }\n\n            if (sequenceId > op.sequenceId) {\n                log.warn(\"[{}] [{}] Got ack for msg. expecting: {} - {} - got: {} - {} - queue-size: {}\",\n                        topic, producerName, op.sequenceId, op.highestSequenceId, sequenceId, highestSequenceId,\n                        pendingMessages.messagesCount());\n                // Force connection closing so that messages can be re-transmitted in a new connection\n                cnx.channel().close();\n                return;\n            } else if (sequenceId < op.sequenceId) {\n                // Ignoring the ack since it's referring to a message that has already timed out.\n                if (log.isDebugEnabled()) {\n                    log.debug(\"[{}] [{}] Got ack for timed out msg. expecting: {} - {} - got: {} - {}\",\n                            topic, producerName, op.sequenceId, op.highestSequenceId, sequenceId, highestSequenceId);\n                }\n                return;\n            } else {\n                // Add check `sequenceId >= highestSequenceId` for backward compatibility.\n                if (sequenceId >= highestSequenceId || highestSequenceId == op.highestSequenceId) {\n                    // Message was persisted correctly\n                    if (log.isDebugEnabled()) {\n                        log.debug(\"[{}] [{}] Received ack for msg {} \", topic, producerName, sequenceId);\n                    }\n                    pendingMessages.remove();\n                    releaseSemaphoreForSendOp(op);\n                } else {\n                    log.warn(\"[{}] [{}] Got ack for batch msg error. expecting: {} - {} - got: {} - {} - queue-size: {}\"\n                                    + \"\",\n                            topic, producerName, op.sequenceId, op.highestSequenceId, sequenceId, highestSequenceId,\n                            pendingMessages.messagesCount());\n                    // Force connection closing so that messages can be re-transmitted in a new connection\n                    cnx.channel().close();\n                    return;\n                }\n            }\n        }\n\n        OpSendMsg finalOp = op;\n        LAST_SEQ_ID_PUBLISHED_UPDATER.getAndUpdate(this, last -> Math.max(last, getHighestSequenceId(finalOp)));\n        op.setMessageId(ledgerId, entryId, partitionIndex);\n        if (op.totalChunks > 1) {\n            if (op.chunkId == 0) {\n                op.chunkedMessageCtx.firstChunkMessageId = new MessageIdImpl(ledgerId, entryId, partitionIndex);\n            } else if (op.chunkId == op.totalChunks - 1) {\n                op.chunkedMessageCtx.lastChunkMessageId = new MessageIdImpl(ledgerId, entryId, partitionIndex);\n                op.setMessageId(op.chunkedMessageCtx.getChunkMessageId());\n            }\n        }\n\n\n        // if message is chunked then call callback only on last chunk\n        if (op.totalChunks <= 1 || (op.chunkId == op.totalChunks - 1)) {\n            try {\n                // Need to protect ourselves from any exception being thrown in the future handler from the\n                // application\n                op.sendComplete(null);\n            } catch (Throwable t) {\n                log.warn(\"[{}] [{}] Got exception while completing the callback for msg {}:\", topic,\n                        producerName, sequenceId, t);\n            }\n        }\n        ReferenceCountUtil.safeRelease(op.cmd);\n        op.recycle();\n    }\n\n    private long getHighestSequenceId(OpSendMsg op) {\n        return Math.max(op.highestSequenceId, op.sequenceId);\n    }\n\n    private void releaseSemaphoreForSendOp(OpSendMsg op) {\n\n        semaphoreRelease(isBatchMessagingEnabled() ? op.numMessagesInBatch : 1);\n\n        client.getMemoryLimitController().releaseMemory(op.uncompressedSize);\n    }\n\n    private void completeCallbackAndReleaseSemaphore(long payloadSize, SendCallback callback, Exception exception) {\n        semaphore.ifPresent(Semaphore::release);\n        client.getMemoryLimitController().releaseMemory(payloadSize);\n        callback.sendComplete(exception);\n    }\n\n    /**\n     * Checks message checksum to retry if message was corrupted while sending to broker. Recomputes checksum of the\n     * message header-payload again.\n     * <ul>\n     * <li><b>if matches with existing checksum</b>: it means message was corrupt while sending to broker. So, resend\n     * message</li>\n     * <li><b>if doesn't match with existing checksum</b>: it means message is already corrupt and can't retry again.\n     * So, fail send-message by failing callback</li>\n     * </ul>\n     *\n     * @param cnx\n     * @param sequenceId\n     */\n    protected synchronized void recoverChecksumError(ClientCnx cnx, long sequenceId) {\n        OpSendMsg op = pendingMessages.peek();\n        if (op == null) {\n            if (log.isDebugEnabled()) {\n                log.debug(\"[{}] [{}] Got send failure for timed out msg {}\", topic, producerName, sequenceId);\n            }\n        } else {\n            long expectedSequenceId = getHighestSequenceId(op);\n            if (sequenceId == expectedSequenceId) {\n                boolean corrupted = !verifyLocalBufferIsNotCorrupted(op);\n                if (corrupted) {\n                    // remove message from pendingMessages queue and fail callback\n                    pendingMessages.remove();\n                    releaseSemaphoreForSendOp(op);\n                    try {\n                        op.sendComplete(\n                            new PulsarClientException.ChecksumException(\n                                format(\"The checksum of the message which is produced by producer %s to the topic \"\n                                        + \"%s is corrupted\", producerName, topic)));\n                    } catch (Throwable t) {\n                        log.warn(\"[{}] [{}] Got exception while completing the callback for msg {}:\", topic,\n                                producerName, sequenceId, t);\n                    }\n                    ReferenceCountUtil.safeRelease(op.cmd);\n                    op.recycle();\n                    return;\n                } else {\n                    if (log.isDebugEnabled()) {\n                        log.debug(\"[{}] [{}] Message is not corrupted, retry send-message with sequenceId {}\", topic,\n                                producerName, sequenceId);\n                    }\n                }\n\n            } else {\n                if (log.isDebugEnabled()) {\n                    log.debug(\"[{}] [{}] Corrupt message is already timed out {}\", topic, producerName, sequenceId);\n                }\n            }\n        }\n        // as msg is not corrupted : let producer resend pending-messages again including checksum failed message\n        resendMessages(cnx, this.connectionHandler.getEpoch());\n    }\n\n    protected synchronized void recoverNotAllowedError(long sequenceId, String errorMsg) {\n        OpSendMsg op = pendingMessages.peek();\n        if (op != null && sequenceId == getHighestSequenceId(op)) {\n            pendingMessages.remove();\n            releaseSemaphoreForSendOp(op);\n            try {\n                op.sendComplete(\n                        new PulsarClientException.NotAllowedException(errorMsg));\n            } catch (Throwable t) {\n                log.warn(\"[{}] [{}] Got exception while completing the callback for msg {}:\", topic,\n                        producerName, sequenceId, t);\n            }\n            ReferenceCountUtil.safeRelease(op.cmd);\n            op.recycle();\n        }\n    }\n\n    /**\n     * Computes checksum again and verifies it against existing checksum. If checksum doesn't match it means that\n     * message is corrupt.\n     *\n     * @param op\n     * @return returns true only if message is not modified and computed-checksum is same as previous checksum else\n     *         return false that means that message is corrupted. Returns true if checksum is not present.\n     */\n    protected boolean verifyLocalBufferIsNotCorrupted(OpSendMsg op) {\n        ByteBufPair msg = op.cmd;\n\n        if (msg != null) {\n            ByteBuf headerFrame = msg.getFirst();\n            headerFrame.markReaderIndex();\n            try {\n                // skip bytes up to checksum index\n                headerFrame.skipBytes(4); // skip [total-size]\n                int cmdSize = (int) headerFrame.readUnsignedInt();\n                headerFrame.skipBytes(cmdSize);\n                // verify if checksum present\n                if (hasChecksum(headerFrame)) {\n                    int checksum = readChecksum(headerFrame);\n                    // msg.readerIndex is already at header-payload index, Recompute checksum for headers-payload\n                    int metadataChecksum = computeChecksum(headerFrame);\n                    long computedChecksum = resumeChecksum(metadataChecksum, msg.getSecond());\n                    return checksum == computedChecksum;\n                } else {\n                    log.warn(\"[{}] [{}] checksum is not present into message with id {}\", topic, producerName,\n                            op.sequenceId);\n                }\n            } finally {\n                headerFrame.resetReaderIndex();\n            }\n            return true;\n        } else {\n            log.warn(\"[{}] Failed while casting empty ByteBufPair, \", producerName);\n            return false;\n        }\n    }\n\n    static class ChunkedMessageCtx extends AbstractReferenceCounted {\n        protected MessageIdImpl firstChunkMessageId;\n        protected MessageIdImpl lastChunkMessageId;\n\n        public ChunkMessageIdImpl getChunkMessageId() {\n            return new ChunkMessageIdImpl(firstChunkMessageId, lastChunkMessageId);\n        }\n\n        private static final Recycler<ProducerImpl.ChunkedMessageCtx> RECYCLER =\n                new Recycler<ProducerImpl.ChunkedMessageCtx>() {\n                    protected ProducerImpl.ChunkedMessageCtx newObject(\n                            Recycler.Handle<ProducerImpl.ChunkedMessageCtx> handle) {\n                        return new ProducerImpl.ChunkedMessageCtx(handle);\n                    }\n                };\n\n        public static ChunkedMessageCtx get(int totalChunks) {\n            ChunkedMessageCtx chunkedMessageCtx = RECYCLER.get();\n            chunkedMessageCtx.setRefCnt(totalChunks);\n            return chunkedMessageCtx;\n        }\n\n        private final Handle<ProducerImpl.ChunkedMessageCtx> recyclerHandle;\n\n        private ChunkedMessageCtx(Handle<ChunkedMessageCtx> recyclerHandle) {\n            this.recyclerHandle = recyclerHandle;\n        }\n\n        @Override\n        protected void deallocate() {\n            this.firstChunkMessageId = null;\n            this.lastChunkMessageId = null;\n            recyclerHandle.recycle(this);\n        }\n\n        @Override\n        public ReferenceCounted touch(Object hint) {\n            return this;\n        }\n    }\n\n    protected static final class OpSendMsg {\n        MessageImpl<?> msg;\n        List<MessageImpl<?>> msgs;\n        ByteBufPair cmd;\n        SendCallback callback;\n        Runnable rePopulate;\n        ChunkedMessageCtx chunkedMessageCtx;\n        long uncompressedSize;\n        long sequenceId;\n        long createdAt;\n        long firstSentAt;\n        long lastSentAt;\n        int retryCount;\n        long batchSizeByte = 0;\n        int numMessagesInBatch = 1;\n        long highestSequenceId;\n        int totalChunks = 0;\n        int chunkId = -1;\n\n        void initialize() {\n            msg = null;\n            msgs = null;\n            cmd = null;\n            callback = null;\n            rePopulate = null;\n            sequenceId = -1L;\n            createdAt = -1L;\n            firstSentAt = -1L;\n            lastSentAt = -1L;\n            highestSequenceId = -1L;\n            totalChunks = 0;\n            chunkId = -1;\n            uncompressedSize = 0;\n            retryCount = 0;\n            batchSizeByte = 0;\n            numMessagesInBatch = 1;\n            chunkedMessageCtx = null;\n        }\n\n        static OpSendMsg create(MessageImpl<?> msg, ByteBufPair cmd, long sequenceId, SendCallback callback) {\n            OpSendMsg op = RECYCLER.get();\n            op.initialize();\n            op.msg = msg;\n            op.cmd = cmd;\n            op.callback = callback;\n            op.sequenceId = sequenceId;\n            op.createdAt = System.nanoTime();\n            op.uncompressedSize = msg.getUncompressedSize();\n            return op;\n        }\n\n        static OpSendMsg create(List<MessageImpl<?>> msgs, ByteBufPair cmd, long sequenceId, SendCallback callback,\n                                int batchAllocatedSize) {\n            OpSendMsg op = RECYCLER.get();\n            op.initialize();\n            op.msgs = msgs;\n            op.cmd = cmd;\n            op.callback = callback;\n            op.sequenceId = sequenceId;\n            op.createdAt = System.nanoTime();\n            op.uncompressedSize = 0;\n            for (int i = 0; i < msgs.size(); i++) {\n                op.uncompressedSize += msgs.get(i).getUncompressedSize();\n            }\n            op.uncompressedSize += batchAllocatedSize;\n            return op;\n        }\n\n        static OpSendMsg create(List<MessageImpl<?>> msgs, ByteBufPair cmd, long lowestSequenceId,\n                                long highestSequenceId,  SendCallback callback, int batchAllocatedSize) {\n            OpSendMsg op = RECYCLER.get();\n            op.initialize();\n            op.msgs = msgs;\n            op.cmd = cmd;\n            op.callback = callback;\n            op.sequenceId = lowestSequenceId;\n            op.highestSequenceId = highestSequenceId;\n            op.createdAt = System.nanoTime();\n            op.uncompressedSize = 0;\n            for (int i = 0; i < msgs.size(); i++) {\n                op.uncompressedSize += msgs.get(i).getUncompressedSize();\n            }\n            op.uncompressedSize += batchAllocatedSize;\n            return op;\n        }\n\n        void updateSentTimestamp() {\n            this.lastSentAt = System.nanoTime();\n            if (this.firstSentAt == -1L) {\n                this.firstSentAt = this.lastSentAt;\n            }\n            ++this.retryCount;\n        }\n\n        void sendComplete(final Exception e) {\n            SendCallback callback = this.callback;\n            if (null != callback) {\n                Exception finalEx = e;\n                if (finalEx instanceof TimeoutException) {\n                    TimeoutException te = (TimeoutException) e;\n                    long sequenceId = te.getSequenceId();\n                    long ns = System.nanoTime();\n                    //firstSentAt and lastSentAt maybe -1, it means that the message didn't flush to channel.\n                    String errMsg = String.format(\n                        \"%s : createdAt %s seconds ago, firstSentAt %s seconds ago, lastSentAt %s seconds ago, \"\n                                + \"retryCount %s\",\n                        te.getMessage(),\n                        RelativeTimeUtil.nsToSeconds(ns - this.createdAt),\n                        RelativeTimeUtil.nsToSeconds(this.firstSentAt <= 0\n                                ? this.firstSentAt\n                                : ns - this.firstSentAt),\n                        RelativeTimeUtil.nsToSeconds(this.lastSentAt <= 0\n                                ? this.lastSentAt\n                                : ns - this.lastSentAt),\n                        retryCount\n                    );\n\n                    finalEx = new TimeoutException(errMsg, sequenceId);\n                }\n\n                callback.sendComplete(finalEx);\n            }\n        }\n\n        void recycle() {\n            ReferenceCountUtil.safeRelease(chunkedMessageCtx);\n            initialize();\n            recyclerHandle.recycle(this);\n        }\n\n        void setNumMessagesInBatch(int numMessagesInBatch) {\n            this.numMessagesInBatch = numMessagesInBatch;\n        }\n\n        void setBatchSizeByte(long batchSizeByte) {\n            this.batchSizeByte = batchSizeByte;\n        }\n\n        void setMessageId(long ledgerId, long entryId, int partitionIndex) {\n            if (msg != null) {\n                msg.setMessageId(new MessageIdImpl(ledgerId, entryId, partitionIndex));\n            } else if (msgs.size() == 1) {\n                // If there is only one message in batch, the producer will publish messages like non-batch\n                msgs.get(0).setMessageId(new MessageIdImpl(ledgerId, entryId, partitionIndex));\n            } else {\n                for (int batchIndex = 0; batchIndex < msgs.size(); batchIndex++) {\n                    msgs.get(batchIndex)\n                            .setMessageId(new BatchMessageIdImpl(ledgerId, entryId, partitionIndex, batchIndex));\n                }\n            }\n        }\n\n        void setMessageId(ChunkMessageIdImpl chunkMessageId) {\n            if (msg != null) {\n                msg.setMessageId(chunkMessageId);\n            }\n        }\n\n        public int getMessageHeaderAndPayloadSize() {\n            if (cmd == null) {\n                return 0;\n            }\n            ByteBuf cmdHeader = cmd.getFirst();\n            cmdHeader.markReaderIndex();\n            int totalSize = cmdHeader.readInt();\n            int cmdSize = cmdHeader.readInt();\n            // The totalSize includes:\n            // | cmdLength | cmdSize | magic and checksum | msgMetadataLength | msgMetadata |\n            // | --------- | ------- | ------------------ | ----------------- | ----------- |\n            // | 4         |         | 6                  | 4                 |             |\n            int msgHeadersAndPayloadSize = totalSize - 4 - cmdSize - 6 - 4;\n            cmdHeader.resetReaderIndex();\n            return msgHeadersAndPayloadSize;\n        }\n\n        private OpSendMsg(Handle<OpSendMsg> recyclerHandle) {\n            this.recyclerHandle = recyclerHandle;\n        }\n\n        private final Handle<OpSendMsg> recyclerHandle;\n        private static final Recycler<OpSendMsg> RECYCLER = new Recycler<OpSendMsg>() {\n            @Override\n            protected OpSendMsg newObject(Handle<OpSendMsg> handle) {\n                return new OpSendMsg(handle);\n            }\n        };\n    }\n\n\n    /**\n     * Queue implementation that is used as the pending messages queue.\n     *\n     * This implementation postpones adding of new OpSendMsg entries that happen\n     * while the forEach call is in progress. This is needed for preventing\n     * ConcurrentModificationExceptions that would occur when the forEach action\n     * calls the add method via a callback in user code.\n     *\n     * This queue is not thread safe.\n     */\n    protected static class OpSendMsgQueue implements Iterable<OpSendMsg> {\n        private final Queue<OpSendMsg> delegate = new ArrayDeque<>();\n        private int forEachDepth = 0;\n        private List<OpSendMsg> postponedOpSendMgs;\n        private final AtomicInteger messagesCount = new AtomicInteger(0);\n\n        @Override\n        public void forEach(Consumer<? super OpSendMsg> action) {\n            try {\n                // track any forEach call that is in progress in the current call stack\n                // so that adding a new item while iterating doesn't cause ConcurrentModificationException\n                forEachDepth++;\n                delegate.forEach(action);\n            } finally {\n                forEachDepth--;\n                // if this is the top-most forEach call and there are postponed items, add them\n                if (forEachDepth == 0 && postponedOpSendMgs != null && !postponedOpSendMgs.isEmpty()) {\n                    delegate.addAll(postponedOpSendMgs);\n                    postponedOpSendMgs.clear();\n                }\n            }\n        }\n\n        public boolean add(OpSendMsg o) {\n            // postpone adding to the queue while forEach iteration is in progress\n            messagesCount.addAndGet(o.numMessagesInBatch);\n            if (forEachDepth > 0) {\n                if (postponedOpSendMgs == null) {\n                    postponedOpSendMgs = new ArrayList<>();\n                }\n                return postponedOpSendMgs.add(o);\n            } else {\n                return delegate.add(o);\n            }\n        }\n\n        public void clear() {\n            delegate.clear();\n            messagesCount.set(0);\n        }\n\n        public void remove() {\n            OpSendMsg op = delegate.remove();\n            if (op != null) {\n                messagesCount.addAndGet(-op.numMessagesInBatch);\n            }\n        }\n\n        public OpSendMsg peek() {\n            return delegate.peek();\n        }\n\n        public int messagesCount() {\n            return messagesCount.get();\n        }\n\n        @Override\n        public Iterator<OpSendMsg> iterator() {\n            return delegate.iterator();\n        }\n    }\n\n\n    @Override\n    public CompletableFuture<Void> connectionOpened(final ClientCnx cnx) {\n        previousExceptions.clear();\n        chunkMaxMessageSize = Math.min(chunkMaxMessageSize, ClientCnx.getMaxMessageSize());\n\n        final long epoch;\n        synchronized (this) {\n            // Because the state could have been updated while retrieving the connection, we set it back to connecting,\n            // as long as the change from current state to connecting is a valid state change.\n            if (!changeToConnecting()) {\n                return CompletableFuture.completedFuture(null);\n            }\n            // We set the cnx reference before registering the producer on the cnx, so if the cnx breaks before creating\n            // the producer, it will try to grab a new cnx. We also increment and get the epoch value for the producer.\n            epoch = connectionHandler.switchClientCnx(cnx);\n        }\n        cnx.registerProducer(producerId, this);\n\n        log.info(\"[{}] [{}] Creating producer on cnx {}\", topic, producerName, cnx.ctx().channel());\n\n        long requestId = client.newRequestId();\n\n        PRODUCER_DEADLINE_UPDATER\n            .compareAndSet(this, 0, System.currentTimeMillis() + client.getConfiguration().getOperationTimeoutMs());\n\n        SchemaInfo schemaInfo = null;\n        if (schema != null) {\n            if (schema.getSchemaInfo() != null) {\n                if (schema.getSchemaInfo().getType() == SchemaType.JSON) {\n                    // for backwards compatibility purposes\n                    // JSONSchema originally generated a schema for pojo based of of the JSON schema standard\n                    // but now we have standardized on every schema to generate an Avro based schema\n                    if (Commands.peerSupportJsonSchemaAvroFormat(cnx.getRemoteEndpointProtocolVersion())) {\n                        schemaInfo = schema.getSchemaInfo();\n                    } else if (schema instanceof JSONSchema){\n                        JSONSchema jsonSchema = (JSONSchema) schema;\n                        schemaInfo = jsonSchema.getBackwardsCompatibleJsonSchemaInfo();\n                    } else {\n                        schemaInfo = schema.getSchemaInfo();\n                    }\n                } else if (schema.getSchemaInfo().getType() == SchemaType.BYTES\n                        || schema.getSchemaInfo().getType() == SchemaType.NONE) {\n                    // don't set schema info for Schema.BYTES\n                    schemaInfo = null;\n                } else {\n                    schemaInfo = schema.getSchemaInfo();\n                }\n            }\n        }\n\n        final CompletableFuture<Void> future = new CompletableFuture<>();\n        cnx.sendRequestWithId(\n                Commands.newProducer(topic, producerId, requestId, producerName, conf.isEncryptionEnabled(), metadata,\n                        schemaInfo, epoch, userProvidedProducerName,\n                        conf.getAccessMode(), topicEpoch, client.conf.isEnableTransaction(),\n                        conf.getInitialSubscriptionName()),\n                requestId).thenAccept(response -> {\n                    String producerName = response.getProducerName();\n                    long lastSequenceId = response.getLastSequenceId();\n                    schemaVersion = Optional.ofNullable(response.getSchemaVersion());\n                    schemaVersion.ifPresent(v -> schemaCache.put(SchemaHash.of(schema), v));\n\n                    // We are now reconnected to broker and clear to send messages. Re-send all pending messages and\n                    // set the cnx pointer so that new messages will be sent immediately\n                    synchronized (ProducerImpl.this) {\n                        State state = getState();\n                        if (state == State.Closing || state == State.Closed) {\n                            // Producer was closed while reconnecting, close the connection to make sure the broker\n                            // drops the producer on its side\n                            cnx.removeProducer(producerId);\n                            cnx.channel().close();\n                            future.complete(null);\n                            return;\n                        }\n                        resetBackoff();\n\n                        log.info(\"[{}] [{}] Created producer on cnx {}\", topic, producerName, cnx.ctx().channel());\n                        connectionId = cnx.ctx().channel().toString();\n                        connectedSince = DateFormatter.now();\n                        if (conf.getAccessMode() != ProducerAccessMode.Shared && !topicEpoch.isPresent()) {\n                            log.info(\"[{}] [{}] Producer epoch is {}\", topic, producerName, response.getTopicEpoch());\n                        }\n                        topicEpoch = response.getTopicEpoch();\n\n                        if (this.producerName == null) {\n                            this.producerName = producerName;\n                        }\n\n                        if (this.msgIdGenerator == 0 && conf.getInitialSequenceId() == null) {\n                            // Only update sequence id generator if it wasn't already modified. That means we only want\n                            // to update the id generator the first time the producer gets established, and ignore the\n                            // sequence id sent by broker in subsequent producer reconnects\n                            this.lastSequenceIdPublished = lastSequenceId;\n                            this.msgIdGenerator = lastSequenceId + 1;\n                        }\n\n                        resendMessages(cnx, epoch);\n                    }\n                    future.complete(null);\n                }).exceptionally((e) -> {\n                    Throwable cause = e.getCause();\n                    cnx.removeProducer(producerId);\n                    State state = getState();\n                    if (state == State.Closing || state == State.Closed) {\n                        // Producer was closed while reconnecting, close the connection to make sure the broker\n                        // drops the producer on its side\n                        cnx.channel().close();\n                        future.complete(null);\n                        return null;\n                    }\n\n                    if (cause instanceof TimeoutException) {\n                        // Creating the producer has timed out. We need to ensure the broker closes the producer\n                        // in case it was indeed created, otherwise it might prevent new create producer operation,\n                        // since we are not necessarily closing the connection.\n                        long closeRequestId = client.newRequestId();\n                        ByteBuf cmd = Commands.newCloseProducer(producerId, closeRequestId);\n                        cnx.sendRequestWithId(cmd, closeRequestId);\n                    }\n\n                    if (cause instanceof PulsarClientException.ProducerFencedException) {\n                        if (log.isDebugEnabled()) {\n                            log.debug(\"[{}] [{}] Failed to create producer: {}\",\n                                    topic, producerName, cause.getMessage());\n                        }\n                    } else {\n                        log.error(\"[{}] [{}] Failed to create producer: {}\", topic, producerName, cause.getMessage());\n                    }\n                    // Close the producer since topic does not exist.\n                    if (cause instanceof PulsarClientException.TopicDoesNotExistException) {\n                        closeAsync().whenComplete((v, ex) -> {\n                            if (ex != null) {\n                                log.error(\"Failed to close producer on TopicDoesNotExistException.\", ex);\n                            }\n                            producerCreatedFuture.completeExceptionally(cause);\n                        });\n                        future.complete(null);\n                        return null;\n                    }\n                    if (cause instanceof PulsarClientException.ProducerBlockedQuotaExceededException) {\n                        synchronized (this) {\n                            log.warn(\"[{}] [{}] Topic backlog quota exceeded. Throwing Exception on producer.\", topic,\n                                    producerName);\n\n                            if (log.isDebugEnabled()) {\n                                log.debug(\"[{}] [{}] Pending messages: {}\", topic, producerName,\n                                        pendingMessages.messagesCount());\n                            }\n\n                            PulsarClientException bqe = new PulsarClientException.ProducerBlockedQuotaExceededException(\n                                format(\"The backlog quota of the topic %s that the producer %s produces to is exceeded\",\n                                    topic, producerName));\n                            failPendingMessages(cnx(), bqe);\n                        }\n                    } else if (cause instanceof PulsarClientException.ProducerBlockedQuotaExceededError) {\n                        log.warn(\"[{}] [{}] Producer is blocked on creation because backlog exceeded on topic.\",\n                                producerName, topic);\n                    }\n\n                    if (cause instanceof PulsarClientException.TopicTerminatedException) {\n                        setState(State.Terminated);\n                        synchronized (this) {\n                            failPendingMessages(cnx(), (PulsarClientException) cause);\n                        }\n                        producerCreatedFuture.completeExceptionally(cause);\n                        closeProducerTasks();\n                        client.cleanupProducer(this);\n                    } else if (cause instanceof PulsarClientException.ProducerFencedException) {\n                        setState(State.ProducerFenced);\n                        synchronized (this) {\n                            failPendingMessages(cnx(), (PulsarClientException) cause);\n                        }\n                        producerCreatedFuture.completeExceptionally(cause);\n                        closeProducerTasks();\n                        client.cleanupProducer(this);\n                    } else if (producerCreatedFuture.isDone() || //\n                               (cause instanceof PulsarClientException && PulsarClientException.isRetriableError(cause)\n                                && System.currentTimeMillis() < PRODUCER_DEADLINE_UPDATER.get(ProducerImpl.this))) {\n                        // Either we had already created the producer once (producerCreatedFuture.isDone()) or we are\n                        // still within the initial timeout budget and we are dealing with a retriable error\n                        future.completeExceptionally(cause);\n                    } else {\n                        setState(State.Failed);\n                        producerCreatedFuture.completeExceptionally(cause);\n                        closeProducerTasks();\n                        client.cleanupProducer(this);\n                        Timeout timeout = sendTimeout;\n                        if (timeout != null) {\n                            timeout.cancel();\n                            sendTimeout = null;\n                        }\n                    }\n                    if (!future.isDone()) {\n                        future.complete(null);\n                    }\n                    return null;\n                });\n        return future;\n    }\n\n    @Override\n    public void connectionFailed(PulsarClientException exception) {\n        boolean nonRetriableError = !PulsarClientException.isRetriableError(exception);\n        boolean timeout = System.currentTimeMillis() > lookupDeadline;\n        if (nonRetriableError || timeout) {\n            exception.setPreviousExceptions(previousExceptions);\n            if (producerCreatedFuture.completeExceptionally(exception)) {\n                if (nonRetriableError) {\n                    log.info(\"[{}] Producer creation failed for producer {} with unretriableError = {}\",\n                            topic, producerId, exception.getMessage());\n                } else {\n                    log.info(\"[{}] Producer creation failed for producer {} after producerTimeout\", topic, producerId);\n                }\n                closeProducerTasks();\n                setState(State.Failed);\n                client.cleanupProducer(this);\n            }\n        } else {\n            previousExceptions.add(exception);\n        }\n    }\n\n    private void closeProducerTasks() {\n        Timeout timeout = sendTimeout;\n        if (timeout != null) {\n            timeout.cancel();\n            sendTimeout = null;\n        }\n\n        if (keyGeneratorTask != null && !keyGeneratorTask.isCancelled()) {\n            keyGeneratorTask.cancel(false);\n        }\n\n        stats.cancelStatsTimeout();\n    }\n\n    private void resendMessages(ClientCnx cnx, long expectedEpoch) {\n        cnx.ctx().channel().eventLoop().execute(() -> {\n            synchronized (this) {\n                if (getState() == State.Closing || getState() == State.Closed) {\n                    // Producer was closed while reconnecting, close the connection to make sure the broker\n                    // drops the producer on its side\n                    cnx.channel().close();\n                    return;\n                }\n\n                int messagesToResend = pendingMessages.messagesCount();\n                if (messagesToResend == 0) {\n                    if (log.isDebugEnabled()) {\n                        log.debug(\"[{}] [{}] No pending messages to resend {}\", topic, producerName, messagesToResend);\n                    }\n                    if (changeToReadyState()) {\n                        producerCreatedFuture.complete(ProducerImpl.this);\n                        scheduleBatchFlushTask(0);\n                        return;\n                    } else {\n                        // Producer was closed while reconnecting, close the connection to make sure the broker\n                        // drops the producer on its side\n                        cnx.channel().close();\n                        return;\n                    }\n\n                }\n\n                log.info(\"[{}] [{}] Re-Sending {} messages to server\", topic, producerName, messagesToResend);\n                recoverProcessOpSendMsgFrom(cnx, null, expectedEpoch);\n            }\n        });\n    }\n\n    /**\n     * Strips checksum from {@link OpSendMsg} command if present else ignore it.\n     *\n     * @param op\n     */\n    private void stripChecksum(OpSendMsg op) {\n        ByteBufPair msg = op.cmd;\n        if (msg != null) {\n            int totalMsgBufSize = msg.readableBytes();\n            ByteBuf headerFrame = msg.getFirst();\n            headerFrame.markReaderIndex();\n            try {\n                headerFrame.skipBytes(4); // skip [total-size]\n                int cmdSize = (int) headerFrame.readUnsignedInt();\n\n                // verify if checksum present\n                headerFrame.skipBytes(cmdSize);\n\n                if (!hasChecksum(headerFrame)) {\n                    return;\n                }\n\n                int headerSize = 4 + 4 + cmdSize; // [total-size] [cmd-length] [cmd-size]\n                int checksumSize = 4 + 2; // [magic-number] [checksum-size]\n                int checksumMark = (headerSize + checksumSize); // [header-size] [checksum-size]\n                int metaPayloadSize = (totalMsgBufSize - checksumMark); // metadataPayload = totalSize - checksumMark\n                int newTotalFrameSizeLength = 4 + cmdSize + metaPayloadSize; // new total-size without checksum\n                headerFrame.resetReaderIndex();\n                int headerFrameSize = headerFrame.readableBytes();\n\n                headerFrame.setInt(0, newTotalFrameSizeLength); // rewrite new [total-size]\n                ByteBuf metadata = headerFrame.slice(checksumMark, headerFrameSize - checksumMark); // sliced only\n                                                                                                    // metadata\n                headerFrame.writerIndex(headerSize); // set headerFrame write-index to overwrite metadata over checksum\n                metadata.readBytes(headerFrame, metadata.readableBytes());\n                headerFrame.capacity(headerFrameSize - checksumSize); // reduce capacity by removed checksum bytes\n            } finally {\n                headerFrame.resetReaderIndex();\n            }\n        } else {\n            log.warn(\"[{}] Failed while casting null into ByteBufPair\", producerName);\n        }\n    }\n\n    public int brokerChecksumSupportedVersion() {\n        return ProtocolVersion.v6.getValue();\n    }\n\n    @Override\n    String getHandlerName() {\n        return producerName;\n    }\n\n    /**\n     * Process sendTimeout events.\n     */\n    @Override\n    public void run(Timeout timeout) throws Exception {\n        if (timeout.isCancelled()) {\n            return;\n        }\n\n        long timeToWaitMs;\n\n        synchronized (this) {\n            // If it's closing/closed we need to ignore this timeout and not schedule next timeout.\n            if (getState() == State.Closing || getState() == State.Closed) {\n                return;\n            }\n\n            OpSendMsg firstMsg = pendingMessages.peek();\n            if (firstMsg == null && (batchMessageContainer == null || batchMessageContainer.isEmpty())) {\n                // If there are no pending messages, reset the timeout to the configured value.\n                timeToWaitMs = conf.getSendTimeoutMs();\n            } else {\n                long createdAt;\n                if (firstMsg != null) {\n                    createdAt = firstMsg.createdAt;\n                } else {\n                    // Because we don't flush batch messages while disconnected, we consider them \"createdAt\" when\n                    // they would have otherwise been flushed.\n                    createdAt = lastBatchSendNanoTime\n                            + TimeUnit.MICROSECONDS.toNanos(conf.getBatchingMaxPublishDelayMicros());\n                }\n                // If there is at least one message, calculate the diff between the message timeout and the elapsed\n                // time since first message was created.\n                long diff = conf.getSendTimeoutMs()\n                        - TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - createdAt);\n                if (diff <= 0) {\n                    // The diff is less than or equal to zero, meaning that the message has been timed out.\n                    // Set the callback to timeout on every message, then clear the pending queue.\n                    log.info(\"[{}] [{}] Message send timed out. Failing {} messages\", topic, producerName,\n                            getPendingQueueSize());\n                    String msg = format(\"The producer %s can not send message to the topic %s within given timeout\",\n                            producerName, topic);\n                    if (firstMsg != null) {\n                        PulsarClientException te = new PulsarClientException.TimeoutException(msg, firstMsg.sequenceId);\n                        failPendingMessages(cnx(), te);\n                    } else {\n                        failPendingBatchMessages(new PulsarClientException.TimeoutException(msg));\n                    }\n\n                    // Since the pending queue is cleared now, set timer to expire after configured value.\n                    timeToWaitMs = conf.getSendTimeoutMs();\n                } else {\n                    // The diff is greater than zero, set the timeout to the diff value\n                    timeToWaitMs = diff;\n                }\n            }\n\n            sendTimeout = client.timer().newTimeout(this, timeToWaitMs, TimeUnit.MILLISECONDS);\n        }\n    }\n\n    /**\n     * This fails and clears the pending messages with the given exception. This method should be called from within the\n     * ProducerImpl object mutex.\n     */\n    private void failPendingMessages(ClientCnx cnx, PulsarClientException ex) {\n        if (cnx == null) {\n            final AtomicInteger releaseCount = new AtomicInteger();\n            final boolean batchMessagingEnabled = isBatchMessagingEnabled();\n            pendingMessages.forEach(op -> {\n                releaseCount.addAndGet(batchMessagingEnabled ? op.numMessagesInBatch : 1);\n                try {\n                    // Need to protect ourselves from any exception being thrown in the future handler from the\n                    // application\n                    ex.setSequenceId(op.sequenceId);\n                    // if message is chunked then call callback only on last chunk\n                    if (op.totalChunks <= 1 || (op.chunkId == op.totalChunks - 1)) {\n                        // Need to protect ourselves from any exception being thrown in the future handler from the\n                        // application\n                        op.sendComplete(ex);\n                    }\n                } catch (Throwable t) {\n                    log.warn(\"[{}] [{}] Got exception while completing the callback for msg {}:\", topic, producerName,\n                            op.sequenceId, t);\n                }\n                client.getMemoryLimitController().releaseMemory(op.uncompressedSize);\n                ReferenceCountUtil.safeRelease(op.cmd);\n                op.recycle();\n            });\n\n            pendingMessages.clear();\n            semaphoreRelease(releaseCount.get());\n            if (batchMessagingEnabled) {\n                failPendingBatchMessages(ex);\n            }\n\n        } else {\n            // If we have a connection, we schedule the callback and recycle on the event loop thread to avoid any\n            // race condition since we also write the message on the socket from this thread\n            cnx.ctx().channel().eventLoop().execute(() -> {\n                synchronized (ProducerImpl.this) {\n                    failPendingMessages(null, ex);\n                }\n            });\n        }\n    }\n\n    /**\n     * fail any pending batch messages that were enqueued, however batch was not closed out.\n     *\n     */\n    private void failPendingBatchMessages(PulsarClientException ex) {\n        if (batchMessageContainer.isEmpty()) {\n            return;\n        }\n        final int numMessagesInBatch = batchMessageContainer.getNumMessagesInBatch();\n        final long currentBatchSize = batchMessageContainer.getCurrentBatchSize();\n        final int batchAllocatedSizeBytes = batchMessageContainer.getBatchAllocatedSizeBytes();\n        semaphoreRelease(numMessagesInBatch);\n        client.getMemoryLimitController().releaseMemory(currentBatchSize + batchAllocatedSizeBytes);\n        batchMessageContainer.discard(ex);\n    }\n\n    @Override\n    public CompletableFuture<Void> flushAsync() {\n        synchronized (ProducerImpl.this) {\n            if (isBatchMessagingEnabled()) {\n                batchMessageAndSend(false);\n            }\n            CompletableFuture<MessageId>  lastSendFuture = this.lastSendFuture;\n            if (!(lastSendFuture == this.lastSendFutureWrapper.lastSendFuture)) {\n                this.lastSendFutureWrapper = LastSendFutureWrapper.create(lastSendFuture);\n            }\n        }\n\n        return this.lastSendFutureWrapper.handleOnce();\n    }\n\n    @Override\n    protected void triggerFlush() {\n        if (isBatchMessagingEnabled()) {\n            synchronized (ProducerImpl.this) {\n                batchMessageAndSend(false);\n            }\n        }\n    }\n\n    // must acquire semaphore before calling\n    private void maybeScheduleBatchFlushTask() {\n        if (this.batchFlushTask != null || getState() != State.Ready) {\n            return;\n        }\n        scheduleBatchFlushTask(conf.getBatchingMaxPublishDelayMicros());\n    }\n\n    // must acquire semaphore before calling\n    private void scheduleBatchFlushTask(long batchingDelayMicros) {\n        ClientCnx cnx = cnx();\n        if (cnx != null && isBatchMessagingEnabled()) {\n            this.batchFlushTask = cnx.ctx().executor().schedule(catchingAndLoggingThrowables(this::batchFlushTask),\n                    batchingDelayMicros, TimeUnit.MICROSECONDS);\n        }\n    }\n\n    private synchronized void batchFlushTask() {\n        if (log.isTraceEnabled()) {\n            log.trace(\"[{}] [{}] Batching the messages from the batch container from flush thread\",\n                    topic, producerName);\n        }\n        this.batchFlushTask = null;\n        // If we're not ready, don't schedule another flush and don't try to send.\n        if (getState() != State.Ready) {\n            return;\n        }\n        // If a batch was sent more recently than the BatchingMaxPublishDelayMicros, schedule another flush to run just\n        // at BatchingMaxPublishDelayMicros after the last send.\n        long microsSinceLastSend = TimeUnit.NANOSECONDS.toMicros(System.nanoTime() - lastBatchSendNanoTime);\n        if (microsSinceLastSend < conf.getBatchingMaxPublishDelayMicros()) {\n            scheduleBatchFlushTask(conf.getBatchingMaxPublishDelayMicros() - microsSinceLastSend);\n        } else if (lastBatchSendNanoTime == 0) {\n            // The first time a producer sends a message, the lastBatchSendNanoTime is 0.\n            lastBatchSendNanoTime = System.nanoTime();\n            scheduleBatchFlushTask(conf.getBatchingMaxPublishDelayMicros());\n        } else {\n            batchMessageAndSend(true);\n        }\n    }\n\n    // must acquire semaphore before enqueuing\n    private void batchMessageAndSend(boolean shouldScheduleNextBatchFlush) {\n        if (log.isTraceEnabled()) {\n            log.trace(\"[{}] [{}] Batching the messages from the batch container with {} messages\", topic, producerName,\n                    batchMessageContainer.getNumMessagesInBatch());\n        }\n        if (!batchMessageContainer.isEmpty()) {\n            try {\n                lastBatchSendNanoTime = System.nanoTime();\n                List<OpSendMsg> opSendMsgs;\n                if (batchMessageContainer.isMultiBatches()) {\n                    opSendMsgs = batchMessageContainer.createOpSendMsgs();\n                } else {\n                    opSendMsgs = Collections.singletonList(batchMessageContainer.createOpSendMsg());\n                }\n                batchMessageContainer.clear();\n                for (OpSendMsg opSendMsg : opSendMsgs) {\n                    processOpSendMsg(opSendMsg);\n                }\n            } catch (Throwable t) {\n                log.warn(\"[{}] [{}] error while create opSendMsg by batch message container\", topic, producerName, t);\n            } finally {\n                if (shouldScheduleNextBatchFlush) {\n                    maybeScheduleBatchFlushTask();\n                }\n            }\n        }\n    }\n\n    protected void processOpSendMsg(OpSendMsg op) {\n        if (op == null) {\n            return;\n        }\n        try {\n            if (op.msg != null && isBatchMessagingEnabled()) {\n                batchMessageAndSend(false);\n            }\n            if (isMessageSizeExceeded(op)) {\n                return;\n            }\n            pendingMessages.add(op);\n            if (op.msg != null) {\n                LAST_SEQ_ID_PUSHED_UPDATER.getAndUpdate(this,\n                        last -> Math.max(last, getHighestSequenceId(op)));\n            }\n\n            final ClientCnx cnx = getCnxIfReady();\n            if (cnx != null) {\n                if (op.msg != null && op.msg.getSchemaState() == None) {\n                    tryRegisterSchema(cnx, op.msg, op.callback, this.connectionHandler.getEpoch());\n                    return;\n                }\n                // If we do have a connection, the message is sent immediately, otherwise we'll try again once a new\n                // connection is established\n                op.cmd.retain();\n                cnx.ctx().channel().eventLoop().execute(WriteInEventLoopCallback.create(this, cnx, op));\n                stats.updateNumMsgsSent(op.numMessagesInBatch, op.batchSizeByte);\n            } else {\n                if (log.isDebugEnabled()) {\n                    log.debug(\"[{}] [{}] Connection is not ready -- sequenceId {}\", topic, producerName,\n                        op.sequenceId);\n                }\n            }\n        } catch (Throwable t) {\n            releaseSemaphoreForSendOp(op);\n            log.warn(\"[{}] [{}] error while closing out batch -- {}\", topic, producerName, t);\n            op.sendComplete(new PulsarClientException(t, op.sequenceId));\n        }\n    }\n\n    // Must acquire a lock on ProducerImpl.this before calling method.\n    private void recoverProcessOpSendMsgFrom(ClientCnx cnx, MessageImpl from, long expectedEpoch) {\n        if (expectedEpoch != this.connectionHandler.getEpoch() || cnx() == null) {\n            // In this case, the cnx passed to this method is no longer the active connection. This method will get\n            // called again once the new connection registers the producer with the broker.\n            log.info(\"[{}][{}] Producer epoch mismatch or the current connection is null. Skip re-sending the \"\n                    + \" {} pending messages since they will deliver using another connection.\", topic, producerName,\n                    pendingMessages.messagesCount());\n            return;\n        }\n        final boolean stripChecksum = cnx.getRemoteEndpointProtocolVersion() < brokerChecksumSupportedVersion();\n        Iterator<OpSendMsg> msgIterator = pendingMessages.iterator();\n        OpSendMsg pendingRegisteringOp = null;\n        while (msgIterator.hasNext()) {\n            OpSendMsg op = msgIterator.next();\n            if (from != null) {\n                if (op.msg == from) {\n                    from = null;\n                } else {\n                    continue;\n                }\n            }\n            if (op.msg != null) {\n                if (op.msg.getSchemaState() == None) {\n                    if (!rePopulateMessageSchema(op.msg)) {\n                        pendingRegisteringOp = op;\n                        break;\n                    }\n                } else if (op.msg.getSchemaState() == Broken) {\n                    op.recycle();\n                    msgIterator.remove();\n                    continue;\n                }\n            }\n            if (op.cmd == null) {\n                checkState(op.rePopulate != null);\n                op.rePopulate.run();\n                if (isMessageSizeExceeded(op)) {\n                    continue;\n                }\n            }\n            if (stripChecksum) {\n                stripChecksum(op);\n            }\n            op.cmd.retain();\n            if (log.isDebugEnabled()) {\n                log.debug(\"[{}] [{}] Re-Sending message in cnx {}, sequenceId {}\", topic, producerName,\n                          cnx.channel(), op.sequenceId);\n            }\n            cnx.ctx().write(op.cmd, cnx.ctx().voidPromise());\n            op.updateSentTimestamp();\n            stats.updateNumMsgsSent(op.numMessagesInBatch, op.batchSizeByte);\n        }\n        cnx.ctx().flush();\n        if (!changeToReadyState()) {\n            // Producer was closed while reconnecting, close the connection to make sure the broker\n            // drops the producer on its side\n            cnx.channel().close();\n            return;\n        }\n        // If any messages were enqueued while the producer was not Ready, we would have skipped\n        // scheduling the batch flush task. Schedule it now, if there are messages in the batch container.\n        if (isBatchMessagingEnabled() && !batchMessageContainer.isEmpty()) {\n            maybeScheduleBatchFlushTask();\n        }\n        if (pendingRegisteringOp != null) {\n            tryRegisterSchema(cnx, pendingRegisteringOp.msg, pendingRegisteringOp.callback, expectedEpoch);\n        }\n    }\n\n    /**\n     *  Check if final message size for non-batch and non-chunked messages is larger than max message size.\n     */\n    private boolean isMessageSizeExceeded(OpSendMsg op) {\n        if (op.msg != null && !conf.isChunkingEnabled()) {\n            int messageSize = op.getMessageHeaderAndPayloadSize();\n            if (messageSize > ClientCnx.getMaxMessageSize()) {\n                releaseSemaphoreForSendOp(op);\n                op.sendComplete(new PulsarClientException.InvalidMessageException(\n                        format(\"The producer %s of the topic %s sends a message with %d bytes that exceeds %d bytes\",\n                                producerName, topic, messageSize, ClientCnx.getMaxMessageSize()),\n                        op.sequenceId));\n                return true;\n            }\n        }\n        return false;\n    }\n\n    public long getDelayInMillis() {\n        OpSendMsg firstMsg = pendingMessages.peek();\n        if (firstMsg != null) {\n            return TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - firstMsg.createdAt);\n        }\n        return 0L;\n    }\n\n    public String getConnectionId() {\n        return cnx() != null ? connectionId : null;\n    }\n\n    public String getConnectedSince() {\n        return cnx() != null ? connectedSince : null;\n    }\n\n    public int getPendingQueueSize() {\n        if (isBatchMessagingEnabled()) {\n            synchronized (this) {\n                return pendingMessages.messagesCount() + batchMessageContainer.getNumMessagesInBatch();\n            }\n        }\n        return pendingMessages.messagesCount();\n    }\n\n    @Override\n    public ProducerStatsRecorder getStats() {\n        return stats;\n    }\n\n    @Override\n    public String getProducerName() {\n        return producerName;\n    }\n\n    // wrapper for connection methods\n    ClientCnx cnx() {\n        return this.connectionHandler.cnx();\n    }\n\n    void resetBackoff() {\n        this.connectionHandler.resetBackoff();\n    }\n\n    void connectionClosed(ClientCnx cnx) {\n        this.connectionHandler.connectionClosed(cnx);\n    }\n\n    public ClientCnx getClientCnx() {\n        return this.connectionHandler.cnx();\n    }\n\n    void setClientCnx(ClientCnx clientCnx) {\n        this.connectionHandler.setClientCnx(clientCnx);\n    }\n\n    void grabCnx() {\n        this.connectionHandler.grabCnx();\n    }\n\n    @VisibleForTesting\n    Optional<Semaphore> getSemaphore() {\n        return semaphore;\n    }\n\n    @VisibleForTesting\n    boolean isErrorStat() {\n        return errorState;\n    }\n\n    @VisibleForTesting\n    CompletableFuture<Void> getOriginalLastSendFuture() {\n        CompletableFuture<MessageId> lastSendFuture = this.lastSendFuture;\n        return lastSendFuture.thenApply(ignore -> null);\n    }\n\n    private static final Logger log = LoggerFactory.getLogger(ProducerImpl.class);\n}\n"
    }
  ]
}