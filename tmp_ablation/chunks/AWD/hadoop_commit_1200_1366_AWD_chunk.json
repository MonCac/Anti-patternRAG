{
  "antipattern_type": "AWD",
  "project_name": "hadoop",
  "commit_number": "commit_1200",
  "id": "1366",
  "group_id": 1,
  "chunks": [
    {
      "file_path": "D:\\Disaster\\Codefield\\Code_Python\\Anti-patternRAG\\data\\AWD\\apache\\hadoop\\commit_1200\\1366\\before\\hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java",
      "chunk_type": "clientClass",
      "ast_subtree": "/**\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.hadoop.hdfs.server.namenode.snapshot;\n\nimport static org.junit.Assert.assertEquals;\nimport static org.junit.Assert.assertNotEquals;\nimport static org.junit.Assert.assertTrue;\nimport static org.junit.Assert.fail;\n\nimport java.io.IOException;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\nimport java.util.EnumSet;\nimport java.util.HashMap;\nimport java.util.Random;\nimport java.util.List;\nimport java.util.ArrayList;\n\nimport org.apache.commons.collections.list.TreeList;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FSDataOutputStream;\nimport org.apache.hadoop.fs.Options.Rename;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.fs.RemoteIterator;\nimport org.apache.hadoop.hdfs.DFSConfigKeys;\nimport org.apache.hadoop.hdfs.DFSTestUtil;\nimport org.apache.hadoop.hdfs.DFSUtil;\nimport org.apache.hadoop.hdfs.DistributedFileSystem;\nimport org.apache.hadoop.hdfs.MiniDFSCluster;\nimport org.apache.hadoop.hdfs.client.HdfsDataOutputStream;\nimport org.apache.hadoop.hdfs.client.HdfsDataOutputStream.SyncFlag;\nimport org.apache.hadoop.hdfs.client.impl.SnapshotDiffReportGenerator;\nimport org.apache.hadoop.hdfs.protocol.SnapshotDiffReport;\nimport org.apache.hadoop.hdfs.protocol.SnapshotDiffReport.DiffReportEntry;\nimport org.apache.hadoop.hdfs.protocol.SnapshotDiffReport.DiffType;\nimport org.apache.hadoop.hdfs.protocol.SnapshotDiffReportListing;\nimport org.apache.hadoop.hdfs.protocol.SnapshotException;\nimport org.apache.hadoop.hdfs.server.namenode.INodeDirectory;\nimport org.apache.hadoop.hdfs.server.namenode.NameNode;\nimport org.apache.hadoop.hdfs.server.namenode.NameNodeAdapter;\nimport org.apache.hadoop.test.GenericTestUtils;\nimport org.apache.hadoop.util.ChunkedArrayList;\nimport org.apache.hadoop.util.Time;\nimport org.junit.After;\nimport org.junit.Assert;\nimport org.junit.Assume;\nimport org.junit.Before;\nimport org.junit.Test;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * Tests snapshot deletion.\n */\npublic class TestSnapshotDiffReport {\n  private static final Logger LOG =\n      LoggerFactory.getLogger(TestSnapshotDiffReport.class);\n  private static final long SEED = 0;\n  private static final short REPLICATION = 3;\n  private static final short REPLICATION_1 = 2;\n  private static final long BLOCKSIZE = 1024;\n  private static final long BUFFERLEN = BLOCKSIZE / 2;\n  private static final long FILELEN = BLOCKSIZE * 2;\n\n  private final Path dir = new Path(\"/TestSnapshot\");\n  private final Path sub1 = new Path(dir, \"sub1\");\n  \n  protected Configuration conf;\n  protected MiniDFSCluster cluster;\n  protected DistributedFileSystem hdfs;\n  private final HashMap<Path, Integer> snapshotNumberMap = new HashMap<Path, Integer>();\n\n  @Before\n  public void setUp() throws Exception {\n    conf = new Configuration();\n    conf.setBoolean(\n        DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_CAPTURE_OPENFILES, true);\n    conf.setLong(DFSConfigKeys.DFS_NAMENODE_ACCESSTIME_PRECISION_KEY, 1);\n    conf.setBoolean(\n        DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_SKIP_CAPTURE_ACCESSTIME_ONLY_CHANGE,\n        true);\n    conf.setBoolean(\n        DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_DIFF_ALLOW_SNAP_ROOT_DESCENDANT,\n        true);\n    conf.setInt(DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_DIFF_LISTING_LIMIT, 3);\n    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(REPLICATION)\n        .format(true).build();\n    cluster.waitActive();\n    hdfs = cluster.getFileSystem();\n  }\n\n  @After\n  public void tearDown() throws Exception {\n    if (cluster != null) {\n      cluster.shutdown();\n      cluster = null;\n    }\n  }\n\n  protected Path getSnapRootDir() {\n    return sub1;\n  }\n\n  private String genSnapshotName(Path snapshotDir) {\n    int sNum = -1;\n    if (snapshotNumberMap.containsKey(snapshotDir)) {\n      sNum = snapshotNumberMap.get(snapshotDir);\n    }\n    snapshotNumberMap.put(snapshotDir, ++sNum);\n    return \"s\" + sNum;\n  }\n\n  /**\n   * Create/modify/delete files under a given directory, also create snapshots\n   * of directories.\n   */\n  protected void modifyAndCreateSnapshot(Path modifyDir, Path[] snapshotDirs)\n      throws Exception {\n    Path file10 = new Path(modifyDir, \"file10\");\n    Path file11 = new Path(modifyDir, \"file11\");\n    Path file12 = new Path(modifyDir, \"file12\");\n    Path file13 = new Path(modifyDir, \"file13\");\n    Path link13 = new Path(modifyDir, \"link13\");\n    Path file14 = new Path(modifyDir, \"file14\");\n    Path file15 = new Path(modifyDir, \"file15\");\n    DFSTestUtil.createFile(hdfs, file10, BLOCKSIZE, REPLICATION_1, SEED);\n    DFSTestUtil.createFile(hdfs, file11, BLOCKSIZE, REPLICATION_1, SEED);\n    DFSTestUtil.createFile(hdfs, file12, BLOCKSIZE, REPLICATION_1, SEED);\n    DFSTestUtil.createFile(hdfs, file13, BLOCKSIZE, REPLICATION_1, SEED);\n    // create link13\n    hdfs.createSymlink(file13, link13, false);\n    // create snapshot\n    for (Path snapshotDir : snapshotDirs) {\n      hdfs.allowSnapshot(snapshotDir);\n      hdfs.createSnapshot(snapshotDir, genSnapshotName(snapshotDir));\n    }\n\n    // delete file11\n    hdfs.delete(file11, true);\n    // modify file12\n    hdfs.setReplication(file12, REPLICATION);\n    // modify file13\n    hdfs.setReplication(file13, REPLICATION);\n    // delete link13\n    hdfs.delete(link13, false);\n    // create file14\n    DFSTestUtil.createFile(hdfs, file14, BLOCKSIZE, REPLICATION, SEED);\n    // create file15\n    DFSTestUtil.createFile(hdfs, file15, BLOCKSIZE, REPLICATION, SEED);\n\n    // create snapshot\n    for (Path snapshotDir : snapshotDirs) {\n      hdfs.createSnapshot(snapshotDir, genSnapshotName(snapshotDir));\n    }\n\n    // create file11 again\n    DFSTestUtil.createFile(hdfs, file11, BLOCKSIZE, REPLICATION, SEED);\n    // delete file12\n    hdfs.delete(file12, true);\n    // modify file13\n    hdfs.setReplication(file13, (short) (REPLICATION - 2));\n    // create link13 again\n    hdfs.createSymlink(file13, link13, false);\n    // delete file14\n    hdfs.delete(file14, true);\n    // modify file15\n    hdfs.setReplication(file15, (short) (REPLICATION - 1));\n\n    // create snapshot\n    for (Path snapshotDir : snapshotDirs) {\n      hdfs.createSnapshot(snapshotDir, genSnapshotName(snapshotDir));\n    }\n    // modify file10\n    hdfs.setReplication(file10, (short) (REPLICATION + 1));\n  }\n\n  /**\n   * Check the correctness of the diff reports.\n   */\n  private void verifyDiffReport(Path dir, String from, String to,\n      DiffReportEntry... entries) throws IOException {\n    DFSTestUtil.verifySnapshotDiffReport(hdfs, dir, from, to, entries);\n  }\n\n  /**\n   * Test the computation and representation of diff between snapshots.\n   */\n  @Test(timeout = 60000)\n  public void testDiffReport() throws Exception {\n    cluster.getNamesystem().getSnapshotManager().setAllowNestedSnapshots(true);\n\n    Path subsub1 = new Path(sub1, \"subsub1\");\n    Path subsubsub1 = new Path(subsub1, \"subsubsub1\");\n    hdfs.mkdirs(subsubsub1);\n    modifyAndCreateSnapshot(sub1, new Path[]{sub1, subsubsub1});\n    modifyAndCreateSnapshot(subsubsub1, new Path[]{sub1, subsubsub1});\n\n    final String invalidName = \"invalid\";\n    try {\n      hdfs.getSnapshotDiffReport(sub1, invalidName, invalidName);\n      fail(\"Expect exception when providing invalid snapshot name \" +\n          \"for diff report\");\n    } catch (IOException e) {\n      GenericTestUtils.assertExceptionContains(\n          \"Cannot find the snapshot of directory \" + sub1 + \" with name \"\n              + invalidName, e);\n    }\n\n    // diff between the same snapshot\n    SnapshotDiffReport report = hdfs.getSnapshotDiffReport(sub1, \"s0\", \"s0\");\n    LOG.info(report.toString());\n    assertEquals(0, report.getDiffList().size());\n\n    report = hdfs.getSnapshotDiffReport(sub1, \"\", \"\");\n    LOG.info(report.toString());\n    assertEquals(0, report.getDiffList().size());\n\n    try {\n      report = hdfs.getSnapshotDiffReport(subsubsub1, null, \"s2\");\n      fail(\"Expect exception when providing null fromSnapshot \");\n    } catch (IllegalArgumentException e) {\n      GenericTestUtils.assertExceptionContains(\"null fromSnapshot\", e);\n    }\n    report = hdfs.getSnapshotDiffReport(subsubsub1, \"s0\", \"s2\");\n    LOG.info(report.toString());\n    assertEquals(0, report.getDiffList().size());\n\n    // test path with scheme also works\n    report = hdfs.getSnapshotDiffReport(hdfs.makeQualified(subsubsub1),\n        \"s0\", \"s2\");\n    LOG.info(report.toString());\n    assertEquals(0, report.getDiffList().size());\n\n    verifyDiffReport(sub1, \"s0\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"file15\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"file12\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"file13\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"link13\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"link13\")));\n\n    verifyDiffReport(sub1, \"s0\", \"s5\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"file15\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"file12\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"file10\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"file13\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"link13\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"link13\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file10\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/link13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file15\")));\n\n    verifyDiffReport(sub1, \"s2\", \"s5\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"file10\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file10\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/link13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file15\")));\n\n    verifyDiffReport(sub1, \"s3\", \"\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file15\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file12\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file10\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/link13\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/link13\")));\n  }\n\n  @Test(timeout = 60000)\n  public void testSnapRootDescendantDiffReport() throws Exception {\n    Assume.assumeTrue(conf.getBoolean(\n        DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_DIFF_ALLOW_SNAP_ROOT_DESCENDANT,\n        DFSConfigKeys.\n            DFS_NAMENODE_SNAPSHOT_DIFF_ALLOW_SNAP_ROOT_DESCENDANT_DEFAULT));\n    Path subSub = new Path(sub1, \"subsub1\");\n    Path subSubSub = new Path(subSub, \"subsubsub1\");\n    Path nonSnapDir = new Path(dir, \"non_snap\");\n    hdfs.mkdirs(subSubSub);\n    hdfs.mkdirs(nonSnapDir);\n\n    modifyAndCreateSnapshot(sub1, new Path[]{sub1});\n    modifyAndCreateSnapshot(subSub, new Path[]{sub1});\n    modifyAndCreateSnapshot(subSubSub, new Path[]{sub1});\n\n    try {\n      hdfs.getSnapshotDiffReport(subSub, \"s1\", \"s2\");\n      hdfs.getSnapshotDiffReport(subSubSub, \"s1\", \"s2\");\n    } catch (IOException e) {\n      fail(\"Unexpected exception when getting snapshot diff report \" +\n          subSub + \": \" + e);\n    }\n\n    try {\n      hdfs.getSnapshotDiffReport(nonSnapDir, \"s1\", \"s2\");\n      fail(\"Snapshot diff report on a non snapshot directory '\"\n          + nonSnapDir.getName() + \"'should fail!\");\n    } catch (SnapshotException e) {\n      GenericTestUtils.assertExceptionContains(\n          \"The path \" + nonSnapDir +\n              \" is neither snapshottable nor under a snapshot root!\", e);\n    }\n\n    final String invalidName = \"invalid\";\n    try {\n      hdfs.getSnapshotDiffReport(subSub, invalidName, invalidName);\n      fail(\"Expect exception when providing invalid snapshot name \" +\n          \"for diff report\");\n    } catch (IOException e) {\n      GenericTestUtils.assertExceptionContains(\n          \"Cannot find the snapshot of directory \" + sub1 + \" with name \"\n              + invalidName, e);\n    }\n\n    // diff between the same snapshot\n    SnapshotDiffReport report = hdfs.getSnapshotDiffReport(subSub, \"s0\", \"s0\");\n    assertEquals(0, report.getDiffList().size());\n\n    report = hdfs.getSnapshotDiffReport(subSub, \"\", \"\");\n    assertEquals(0, report.getDiffList().size());\n\n    report = hdfs.getSnapshotDiffReport(subSubSub, \"s0\", \"s2\");\n    assertEquals(0, report.getDiffList().size());\n\n    report = hdfs.getSnapshotDiffReport(\n        hdfs.makeQualified(subSubSub), \"s0\", \"s2\");\n    assertEquals(0, report.getDiffList().size());\n\n    verifyDescendantDiffReports(sub1, subSub, subSubSub);\n  }\n\n  private void verifyDescendantDiffReports(final Path snapDir,\n      final Path snapSubDir, final Path snapSubSubDir) throws\n      IOException {\n    verifyDiffReport(snapDir, \"s0\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"file15\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"file12\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"file13\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"link13\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"link13\")));\n    verifyDiffReport(snapSubDir, \"s0\", \"s2\", new DiffReportEntry[]{});\n    verifyDiffReport(snapSubSubDir, \"s0\", \"s2\", new DiffReportEntry[]{});\n\n    verifyDiffReport(snapDir, \"s0\", \"s8\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"file15\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"file12\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"file10\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"file13\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"link13\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"link13\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/file10\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/link13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/file15\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file10\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/link13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file15\")));\n\n    verifyDiffReport(snapSubDir, \"s0\", \"s8\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file10\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"link13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file15\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsubsub1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsubsub1/file10\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsubsub1/file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsubsub1/link13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsubsub1/file15\")));\n\n    verifyDiffReport(snapSubSubDir, \"s0\", \"s8\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file10\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"link13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file15\")));\n\n    verifyDiffReport(snapDir, \"s2\", \"s5\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"file10\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/file10\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/link13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/file15\")));\n\n    verifyDiffReport(snapSubDir, \"s2\", \"s5\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file10\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"link13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file15\")));\n    verifyDiffReport(snapSubSubDir, \"s2\", \"s5\",\n        new DiffReportEntry[]{});\n\n    verifyDiffReport(snapDir, \"s3\", \"\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/file15\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/file12\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/file10\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/file11\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/link13\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/link13\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file10\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/link13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file15\")));\n\n    verifyDiffReport(snapSubDir, \"s3\", \"\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file15\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"file12\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"file10\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"link13\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"link13\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsubsub1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsubsub1/file10\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsubsub1/file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsubsub1/link13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsubsub1/file15\")));\n\n    verifyDiffReport(snapSubSubDir, \"s3\", \"\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file10\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"link13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file15\")));\n  }\n\n  @Test\n  public void testSnapRootDescendantDiffReportWithRename() throws Exception {\n    Assume.assumeTrue(conf.getBoolean(\n        DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_DIFF_ALLOW_SNAP_ROOT_DESCENDANT,\n        DFSConfigKeys.\n            DFS_NAMENODE_SNAPSHOT_DIFF_ALLOW_SNAP_ROOT_DESCENDANT_DEFAULT));\n    Path subSub = new Path(sub1, \"subsub1\");\n    Path subSubSub = new Path(subSub, \"subsubsub1\");\n    Path nonSnapDir = new Path(dir, \"non_snap\");\n    hdfs.mkdirs(subSubSub);\n    hdfs.mkdirs(nonSnapDir);\n\n    hdfs.allowSnapshot(sub1);\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n    Path file20 = new Path(subSubSub, \"file20\");\n    DFSTestUtil.createFile(hdfs, file20, BLOCKSIZE, REPLICATION_1, SEED);\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // Case 1: Move a file away from a descendant dir, but within the snap root.\n    // mv <snaproot>/<subsub>/<subsubsub>/file20 <snaproot>/<subsub>/file20\n    hdfs.rename(file20, new Path(subSub, file20.getName()));\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // The snapshot diff for the snap root detects the change as file rename\n    // as the file move happened within the snap root.\n    verifyDiffReport(sub1, \"s1\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1\")),\n        new DiffReportEntry(DiffType.RENAME,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file20\"),\n            DFSUtil.string2Bytes(\"subsub1/file20\")));\n\n    // The snapshot diff for the descendant dir <subsub> still detects the\n    // change as file rename as the file move happened under the snap root\n    // descendant dir.\n    verifyDiffReport(subSub, \"s1\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsubsub1\")),\n        new DiffReportEntry(DiffType.RENAME,\n            DFSUtil.string2Bytes(\"subsubsub1/file20\"),\n            DFSUtil.string2Bytes(\"file20\")));\n\n    // The snapshot diff for the descendant dir <subsubsub> detects the\n    // change as file delete as the file got moved from its scope.\n    verifyDiffReport(subSubSub, \"s1\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"file20\")));\n\n    // Case 2: Move the file from the snap root descendant dir to any\n    // non snap root dir. mv <snaproot>/<subsub>/file20 <nonsnaproot>/file20.\n    hdfs.rename(new Path(subSub, file20.getName()),\n        new Path(dir, file20.getName()));\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // The snapshot diff for the snap root detects the change as file delete\n    // as the file got moved away from the snap root dir to some non snap\n    // root dir.\n    verifyDiffReport(sub1, \"s2\", \"s3\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/file20\")));\n\n    // The snapshot diff for the snap root descendant <subsub> detects the\n    // change as file delete as the file was previously under its scope and\n    // got moved away from its scope.\n    verifyDiffReport(subSub, \"s2\", \"s3\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"file20\")));\n\n    // The file was already not under the descendant dir <subsubsub> scope.\n    // So, the snapshot diff report for the descendant dir doesn't\n    // show the file rename at all.\n    verifyDiffReport(subSubSub, \"s2\", \"s3\",\n        new DiffReportEntry[]{});\n\n    // Case 3: Move the file from the non-snap root dir to snap root dir\n    // mv <nonsnaproot>/file20 <snaproot>/file20\n    hdfs.rename(new Path(dir, file20.getName()),\n        new Path(sub1, file20.getName()));\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // Snap root directory should show the file moved in as a new file.\n    verifyDiffReport(sub1, \"s3\", \"s4\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file20\")));\n\n    // Snap descendant directories don't have visibility to the moved in file.\n    verifyDiffReport(subSub, \"s3\", \"s4\",\n        new DiffReportEntry[]{});\n    verifyDiffReport(subSubSub, \"s3\", \"s4\",\n        new DiffReportEntry[]{});\n\n    hdfs.rename(new Path(sub1, file20.getName()),\n        new Path(subSub, file20.getName()));\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // Snap root directory now shows the rename as both source and\n    // destination paths are under the snap root.\n    verifyDiffReport(sub1, \"s4\", \"s5\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.RENAME,\n            DFSUtil.string2Bytes(\"file20\"),\n            DFSUtil.string2Bytes(\"subsub1/file20\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")));\n\n    // For the descendant directory under the snap root, the file\n    // moved in shows up as a new file created.\n    verifyDiffReport(subSub, \"s4\", \"s5\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file20\")));\n\n    verifyDiffReport(subSubSub, \"s4\", \"s5\",\n        new DiffReportEntry[]{});\n\n    // Case 4: Snapshot diff for the newly created descendant directory.\n    Path subSubSub2 = new Path(subSub, \"subsubsub2\");\n    hdfs.mkdirs(subSubSub2);\n    Path file30 = new Path(subSubSub2, \"file30\");\n    DFSTestUtil.createFile(hdfs, file30, BLOCKSIZE, REPLICATION_1, SEED);\n    hdfs.createFile(file30);\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    verifyDiffReport(sub1, \"s5\", \"s6\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub2\")));\n\n    verifyDiffReport(subSubSub2, \"s5\", \"s6\",\n        new DiffReportEntry[]{});\n\n    verifyDiffReport(subSubSub2, \"s1\", \"s2\",\n        new DiffReportEntry[]{});\n  }\n\n  @Test\n  public void testSnapshotDiffInfo() throws Exception {\n    Path snapshotRootDirPath = dir;\n    Path snapshotDirDescendantPath = new Path(snapshotRootDirPath, \"desc\");\n    Path snapshotDirNonDescendantPath = new Path(\"/dummy/non/snap/desc\");\n    hdfs.mkdirs(snapshotDirDescendantPath);\n    hdfs.mkdirs(snapshotDirNonDescendantPath);\n\n    hdfs.allowSnapshot(snapshotRootDirPath);\n    hdfs.createSnapshot(snapshotRootDirPath, \"s0\");\n    hdfs.createSnapshot(snapshotRootDirPath, \"s1\");\n\n    INodeDirectory snapshotRootDir = cluster.getNameNode()\n        .getNamesystem().getFSDirectory().getINode(\n            snapshotRootDirPath.toUri().getPath())\n        .asDirectory();\n    INodeDirectory snapshotRootDescendantDir = cluster.getNameNode()\n        .getNamesystem().getFSDirectory().getINode(\n            snapshotDirDescendantPath.toUri().getPath())\n        .asDirectory();\n    INodeDirectory snapshotRootNonDescendantDir = cluster.getNameNode()\n        .getNamesystem().getFSDirectory().getINode(\n            snapshotDirNonDescendantPath.toUri().getPath())\n        .asDirectory();\n    try {\n      SnapshotDiffInfo sdi = new SnapshotDiffInfo(\n          snapshotRootDir,\n          snapshotRootDescendantDir,\n          new Snapshot(0, \"s0\", snapshotRootDescendantDir),\n          new Snapshot(0, \"s1\", snapshotRootDescendantDir));\n      LOG.info(\"SnapshotDiffInfo: \" + sdi.getFrom() + \" - \" + sdi.getTo());\n    } catch (IllegalArgumentException iae){\n      fail(\"Unexpected exception when constructing SnapshotDiffInfo: \" + iae);\n    }\n\n    try {\n      SnapshotDiffInfo sdi = new SnapshotDiffInfo(\n          snapshotRootDir,\n          snapshotRootNonDescendantDir,\n          new Snapshot(0, \"s0\", snapshotRootNonDescendantDir),\n          new Snapshot(0, \"s1\", snapshotRootNonDescendantDir));\n      LOG.info(\"SnapshotDiffInfo: \" + sdi.getFrom() + \" - \" + sdi.getTo());\n      fail(\"SnapshotDiffInfo construction should fail for non snapshot root \" +\n          \"or non snapshot root descendant directories!\");\n    } catch (IllegalArgumentException iae) {\n      // expected exception\n    }\n  }\n\n  /**\n   * Make changes under a sub-directory, then delete the sub-directory. Make\n   * sure the diff report computation correctly retrieve the diff from the\n   * deleted sub-directory.\n   */\n  @Test (timeout=60000)\n  public void testDiffReport2() throws Exception {\n    Path subsub1 = new Path(sub1, \"subsub1\");\n    Path subsubsub1 = new Path(subsub1, \"subsubsub1\");\n    hdfs.mkdirs(subsubsub1);\n    modifyAndCreateSnapshot(subsubsub1, new Path[]{sub1});\n    \n    // delete subsub1\n    hdfs.delete(subsub1, true);\n    // check diff report between s0 and s2\n    verifyDiffReport(sub1, \"s0\", \"s2\", \n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1\")), \n        new DiffReportEntry(DiffType.CREATE, \n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file15\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file12\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/link13\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/link13\")));\n    // check diff report between s0 and the current status\n    verifyDiffReport(sub1, \"s0\", \"\", \n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"subsub1\")));\n  }\n\n  @Test\n  public void testDiffReportWithQuota() throws Exception {\n    final Path testdir = new Path(sub1, \"testdir1\");\n    hdfs.mkdirs(testdir);\n    hdfs.allowSnapshot(testdir);\n    // Set quota BEFORE creating the snapshot\n    hdfs.setQuota(testdir, 10, 10);\n    hdfs.createSnapshot(testdir, \"s0\");\n    final SnapshotDiffReport report =\n        hdfs.getSnapshotDiffReport(testdir, \"s0\", \"\");\n    // The diff should be null. Snapshot dir inode should keep the quota.\n    Assert.assertEquals(0, report.getDiffList().size());\n    // Cleanup\n    hdfs.deleteSnapshot(testdir, \"s0\");\n    hdfs.disallowSnapshot(testdir);\n    hdfs.delete(testdir, true);\n  }\n\n  /**\n   * Rename a directory to its prior descendant, and verify the diff report.\n   */\n  @Test\n  public void testDiffReportWithRename() throws Exception {\n    final Path root = new Path(\"/\");\n    final Path sdir1 = new Path(root, \"dir1\");\n    final Path sdir2 = new Path(root, \"dir2\");\n    final Path foo = new Path(sdir1, \"foo\");\n    final Path bar = new Path(foo, \"bar\");\n    hdfs.mkdirs(bar);\n    hdfs.mkdirs(sdir2);\n\n    // create snapshot on root\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n\n    // /dir1/foo/bar -> /dir2/bar\n    final Path bar2 = new Path(sdir2, \"bar\");\n    hdfs.rename(bar, bar2);\n\n    // /dir1/foo -> /dir2/bar/foo\n    final Path foo2 = new Path(bar2, \"foo\");\n    hdfs.rename(foo, foo2);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s2\");\n    // let's delete /dir2 to make things more complicated\n    hdfs.delete(sdir2, true);\n\n    verifyDiffReport(root, \"s1\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/foo\"),\n            DFSUtil.string2Bytes(\"dir2/bar/foo\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir2\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"dir1/foo/bar\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1/foo\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil\n            .string2Bytes(\"dir1/foo/bar\"), DFSUtil.string2Bytes(\"dir2/bar\")));\n  }\n\n  /**\n   * Rename a file/dir outside of the snapshottable dir should be reported as\n   * deleted. Rename a file/dir from outside should be reported as created.\n   */\n  @Test\n  public void testDiffReportWithRenameOutside() throws Exception {\n    final Path root = new Path(\"/\");\n    final Path dir1 = new Path(root, \"dir1\");\n    final Path dir2 = new Path(root, \"dir2\");\n    final Path foo = new Path(dir1, \"foo\");\n    final Path fileInFoo = new Path(foo, \"file\");\n    final Path bar = new Path(dir2, \"bar\");\n    final Path fileInBar = new Path(bar, \"file\");\n    DFSTestUtil.createFile(hdfs, fileInFoo, BLOCKSIZE, REPLICATION, SEED);\n    DFSTestUtil.createFile(hdfs, fileInBar, BLOCKSIZE, REPLICATION, SEED);\n\n    // create snapshot on /dir1\n    SnapshotTestHelper.createSnapshot(hdfs, dir1, \"s0\");\n\n    // move bar into dir1\n    final Path newBar = new Path(dir1, \"newBar\");\n    hdfs.rename(bar, newBar);\n    // move foo out of dir1 into dir2\n    final Path newFoo = new Path(dir2, \"new\");\n    hdfs.rename(foo, newFoo);\n\n    SnapshotTestHelper.createSnapshot(hdfs, dir1, \"s1\");\n    verifyDiffReport(dir1, \"s0\", \"s1\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(newBar\n            .getName())),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(foo.getName())));\n  }\n\n  /**\n   * Renaming a file/dir then delete the ancestor dir of the rename target\n   * should be reported as deleted.\n   */\n  @Test\n  public void testDiffReportWithRenameAndDelete() throws Exception {\n    final Path root = new Path(\"/\");\n    final Path dir1 = new Path(root, \"dir1\");\n    final Path dir2 = new Path(root, \"dir2\");\n    final Path foo = new Path(dir1, \"foo\");\n    final Path fileInFoo = new Path(foo, \"file\");\n    final Path bar = new Path(dir2, \"bar\");\n    final Path fileInBar = new Path(bar, \"file\");\n    DFSTestUtil.createFile(hdfs, fileInFoo, BLOCKSIZE, REPLICATION, SEED);\n    DFSTestUtil.createFile(hdfs, fileInBar, BLOCKSIZE, REPLICATION, SEED);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    hdfs.rename(fileInFoo, fileInBar, Rename.OVERWRITE);\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n    verifyDiffReport(root, \"s0\", \"s1\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1/foo\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir2/bar\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil\n            .string2Bytes(\"dir2/bar/file\")),\n        new DiffReportEntry(DiffType.RENAME,\n            DFSUtil.string2Bytes(\"dir1/foo/file\"),\n            DFSUtil.string2Bytes(\"dir2/bar/file\")));\n\n    // delete bar\n    hdfs.delete(bar, true);\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s2\");\n    verifyDiffReport(root, \"s0\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1/foo\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir2\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"dir2/bar\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir1/foo/file\")));\n  }\n\n  @Test\n  public void testDiffReportWithRenameToNewDir() throws Exception {\n    final Path root = new Path(\"/\");\n    final Path foo = new Path(root, \"foo\");\n    final Path fileInFoo = new Path(foo, \"file\");\n    DFSTestUtil.createFile(hdfs, fileInFoo, BLOCKSIZE, REPLICATION, SEED);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    final Path bar = new Path(root, \"bar\");\n    hdfs.mkdirs(bar);\n    final Path fileInBar = new Path(bar, \"file\");\n    hdfs.rename(fileInFoo, fileInBar);\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n\n    verifyDiffReport(root, \"s0\", \"s1\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"foo\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"bar\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"foo/file\"),\n            DFSUtil.string2Bytes(\"bar/file\")));\n  }\n\n  /**\n   * Rename a file and then append some data to it\n   */\n  @Test\n  public void testDiffReportWithRenameAndAppend() throws Exception {\n    final Path root = new Path(\"/\");\n    final Path foo = new Path(root, \"foo\");\n    DFSTestUtil.createFile(hdfs, foo, BLOCKSIZE, REPLICATION, SEED);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    final Path bar = new Path(root, \"bar\");\n    hdfs.rename(foo, bar);\n    DFSTestUtil.appendFile(hdfs, bar, 10); // append 10 bytes\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n\n    // we always put modification on the file before rename\n    verifyDiffReport(root, \"s0\", \"s1\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"foo\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"foo\"),\n            DFSUtil.string2Bytes(\"bar\")));\n  }\n\n  /**\n   * Nested renamed dir/file and the withNameList in the WithCount node of the\n   * parental directory is empty due to snapshot deletion. See HDFS-6996 for\n   * details.\n   */\n  @Test\n  public void testDiffReportWithRenameAndSnapshotDeletion() throws Exception {\n    final Path root = new Path(\"/\");\n    final Path foo = new Path(root, \"foo\");\n    final Path bar = new Path(foo, \"bar\");\n    DFSTestUtil.createFile(hdfs, bar, BLOCKSIZE, REPLICATION, SEED);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    // rename /foo to /foo2\n    final Path foo2 = new Path(root, \"foo2\");\n    hdfs.rename(foo, foo2);\n    // now /foo/bar becomes /foo2/bar\n    final Path bar2 = new Path(foo2, \"bar\");\n\n    // delete snapshot s0 so that the withNameList inside of the WithCount node\n    // of foo becomes empty\n    hdfs.deleteSnapshot(root, \"s0\");\n\n    // create snapshot s1 and rename bar again\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n    final Path bar3 = new Path(foo2, \"bar-new\");\n    hdfs.rename(bar2, bar3);\n\n    // we always put modification on the file before rename\n    verifyDiffReport(root, \"s1\", \"\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"foo2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"foo2/bar\"),\n            DFSUtil.string2Bytes(\"foo2/bar-new\")));\n  }\n\n  private void createFile(final Path filePath) throws IOException {\n    DFSTestUtil.createFile(hdfs, filePath, (int) BUFFERLEN,\n        FILELEN, BLOCKSIZE, REPLICATION, SEED);\n  }\n\n  private int writeToStream(final FSDataOutputStream outputStream,\n      byte[] buf) throws IOException {\n    outputStream.write(buf);\n    ((HdfsDataOutputStream)outputStream).hsync(\n        EnumSet.of(SyncFlag.UPDATE_LENGTH));\n    return buf.length;\n  }\n\n  private void restartNameNode() throws Exception {\n    cluster.triggerBlockReports();\n    NameNode nameNode = cluster.getNameNode();\n    NameNodeAdapter.enterSafeMode(nameNode, false);\n    NameNodeAdapter.saveNamespace(nameNode);\n    NameNodeAdapter.leaveSafeMode(nameNode);\n    cluster.restartNameNode(true);\n  }\n\n  /**\n   * Test Snapshot diff report for snapshots with open files captures in them.\n   * Also verify if the diff report remains the same across NameNode restarts.\n   */\n  @Test (timeout = 120000)\n  public void testDiffReportWithOpenFiles() throws Exception {\n    // Construct the directory tree\n    final Path level0A = new Path(\"/level_0_A\");\n    final Path flumeSnapRootDir = level0A;\n    final String flumeFileName = \"flume.log\";\n    final String flumeSnap1Name = \"flume_snap_1\";\n    final String flumeSnap2Name = \"flume_snap_2\";\n\n    // Create files and open a stream\n    final Path flumeFile = new Path(level0A, flumeFileName);\n    createFile(flumeFile);\n    FSDataOutputStream flumeOutputStream = hdfs.append(flumeFile);\n\n    // Create Snapshot S1\n    final Path flumeS1Dir = SnapshotTestHelper.createSnapshot(\n        hdfs, flumeSnapRootDir, flumeSnap1Name);\n    final Path flumeS1Path = new Path(flumeS1Dir, flumeFileName);\n    final long flumeFileLengthAfterS1 = hdfs.getFileStatus(flumeFile).getLen();\n\n    // Verify if Snap S1 file length is same as the the live one\n    Assert.assertEquals(flumeFileLengthAfterS1,\n        hdfs.getFileStatus(flumeS1Path).getLen());\n\n    verifyDiffReport(level0A, flumeSnap1Name, \"\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")));\n\n    long flumeFileWrittenDataLength = flumeFileLengthAfterS1;\n    int newWriteLength = (int) (BLOCKSIZE * 1.5);\n    byte[] buf = new byte[newWriteLength];\n    Random random = new Random();\n    random.nextBytes(buf);\n\n    // Write more data to flume file\n    flumeFileWrittenDataLength += writeToStream(flumeOutputStream, buf);\n\n    // Create Snapshot S2\n    final Path flumeS2Dir = SnapshotTestHelper.createSnapshot(\n        hdfs, flumeSnapRootDir, flumeSnap2Name);\n    final Path flumeS2Path = new Path(flumeS2Dir, flumeFileName);\n\n    // Verify live files length is same as all data written till now\n    final long flumeFileLengthAfterS2 = hdfs.getFileStatus(flumeFile).getLen();\n    Assert.assertEquals(flumeFileWrittenDataLength, flumeFileLengthAfterS2);\n\n    // Verify if Snap S2 file length is same as the live one\n    Assert.assertEquals(flumeFileLengthAfterS2,\n        hdfs.getFileStatus(flumeS2Path).getLen());\n\n    verifyDiffReport(level0A, flumeSnap1Name, \"\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(flumeFileName)));\n\n    verifyDiffReport(level0A, flumeSnap2Name, \"\");\n\n    verifyDiffReport(level0A, flumeSnap1Name, flumeSnap2Name,\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(flumeFileName)));\n\n    // Write more data to flume file\n    flumeFileWrittenDataLength += writeToStream(flumeOutputStream, buf);\n\n    // Verify old flume snapshots have point-in-time / frozen file lengths\n    // even after the live file have moved forward.\n    Assert.assertEquals(flumeFileLengthAfterS1,\n        hdfs.getFileStatus(flumeS1Path).getLen());\n    Assert.assertEquals(flumeFileLengthAfterS2,\n        hdfs.getFileStatus(flumeS2Path).getLen());\n\n    flumeOutputStream.close();\n\n    // Verify if Snap S2 file length is same as the live one\n    Assert.assertEquals(flumeFileWrittenDataLength,\n        hdfs.getFileStatus(flumeFile).getLen());\n\n    // Verify old flume snapshots have point-in-time / frozen file lengths\n    // even after the live file have moved forward.\n    Assert.assertEquals(flumeFileLengthAfterS1,\n        hdfs.getFileStatus(flumeS1Path).getLen());\n    Assert.assertEquals(flumeFileLengthAfterS2,\n        hdfs.getFileStatus(flumeS2Path).getLen());\n\n    verifyDiffReport(level0A, flumeSnap1Name, \"\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(flumeFileName)));\n\n    verifyDiffReport(level0A, flumeSnap2Name, \"\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(flumeFileName)));\n\n    verifyDiffReport(level0A, flumeSnap1Name, flumeSnap2Name,\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(flumeFileName)));\n\n    restartNameNode();\n\n    verifyDiffReport(level0A, flumeSnap1Name, flumeSnap2Name,\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(flumeFileName)));\n\n  }\n\n  private long getAccessTime(Path path) throws IOException {\n    return hdfs.getFileStatus(path).getAccessTime();\n  }\n\n  private String getAccessTimeStr(Path path) throws IOException {\n    SimpleDateFormat timeFmt = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n    return timeFmt.format(new Date(getAccessTime(path)));\n  }\n\n  private Path getSSpath(Path path, Path ssRoot, String ssName) {\n    return new Path(ssRoot, \".snapshot/\" + ssName + \"/\" +\n        path.toString().substring(ssRoot.toString().length()));\n  }\n\n  private void printAtime(Path path, Path ssRoot, String ssName)\n      throws IOException {\n    Path ssPath = getSSpath(path, ssRoot, ssName);\n    LOG.info(\"Access time \"\n        + path + \": \" + getAccessTimeStr(path)\n        + \" \" + ssPath + \": \" + getAccessTimeStr(ssPath));\n  }\n\n  private void assertAtimeEquals(Path path, Path ssRoot,\n      String ssName1, String ssName2)\n      throws IOException {\n    Path ssPath1 = getSSpath(path, ssRoot, ssName1);\n    Path ssPath2 = getSSpath(path, ssRoot, ssName2);\n    assertEquals(getAccessTime(ssPath1), getAccessTime(ssPath2));\n  }\n\n  private void assertAtimeNotEquals(Path path, Path ssRoot,\n      String ssName1, String ssName2)\n      throws IOException {\n    Path ssPath1 = getSSpath(path, ssRoot, ssName1);\n    Path ssPath2 = getSSpath(path, ssRoot, ssName2);\n    assertNotEquals(getAccessTime(ssPath1), getAccessTime(ssPath2));\n  }\n\n  /**\n   * Check to see access time is not captured in snapshot when applicable.\n   * When DFS_NAMENODE_SNAPSHOT_SKIP_CAPTURE_ACCESSTIME_ONLY_CHANGE\n   * is set to true, and if a file's access time changed between two\n   * snapshots but has no other modification, then the access time is not\n   * captured in snapshot.\n   */\n  @Test\n  public void testDontCaptureAccessTimeOnlyChangeReport() throws Exception {\n    final Path froot = new Path(\"/\");\n    final Path root = new Path(froot, \"/testSdiffCalc\");\n\n    // items created pre enabling snapshot\n    final Path filePreSS = new Path(root, \"fParent/filePreSS\");\n    final Path dirPreSS = new Path(root, \"dirPreSS\");\n    final Path dirPreSSChild = new Path(dirPreSS, \"dirPreSSChild\");\n\n    // items created after enabling snapshot\n    final Path filePostSS = new Path(root, \"fParent/filePostSS\");\n    final Path dirPostSS = new Path(root, \"dirPostSS\");\n    final Path dirPostSSChild = new Path(dirPostSS, \"dirPostSSChild\");\n\n    DFSTestUtil.createFile(hdfs, filePreSS, BLOCKSIZE, REPLICATION, SEED);\n    DFSTestUtil.createFile(hdfs, dirPreSSChild, BLOCKSIZE, REPLICATION, SEED);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    printAtime(filePreSS, root, \"s0\");\n    printAtime(dirPreSS, root, \"s0\");\n\n    // items created after creating the first snapshot\n    DFSTestUtil.createFile(hdfs, filePostSS, BLOCKSIZE, REPLICATION, SEED);\n    DFSTestUtil.createFile(hdfs, dirPostSSChild, BLOCKSIZE, REPLICATION, SEED);\n\n    Thread.sleep(3000);\n    long now = Time.now();\n    hdfs.setTimes(filePreSS, -1, now);\n    hdfs.setTimes(filePostSS, -1, now);\n    hdfs.setTimes(dirPreSS, -1, now);\n    hdfs.setTimes(dirPostSS, -1, now);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n    printAtime(filePreSS, root, \"s1\");\n    printAtime(dirPreSS, root, \"s1\");\n    printAtime(filePostSS, root, \"s1\");\n    printAtime(dirPostSS, root, \"s1\");\n\n    Thread.sleep(3000);\n    now = Time.now();\n    hdfs.setTimes(filePreSS, -1, now);\n    hdfs.setTimes(filePostSS, -1, now);\n    hdfs.setTimes(dirPreSS, -1, now);\n    hdfs.setTimes(dirPostSS, -1, now);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s2\");\n    printAtime(filePreSS, root, \"s2\");\n    printAtime(dirPreSS, root, \"s2\");\n    printAtime(filePostSS, root, \"s2\");\n    printAtime(dirPostSS, root, \"s2\");\n\n    Thread.sleep(3000);\n    now = Time.now();\n    // modify filePostSS, and change access time\n    hdfs.setReplication(filePostSS, (short) (REPLICATION - 1));\n    hdfs.setTimes(filePostSS, -1, now);\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s3\");\n\n    LOG.info(\"\\nsnapshotDiff s0 -> s1:\");\n    LOG.info(hdfs.getSnapshotDiffReport(root, \"s0\", \"s1\").toString());\n    LOG.info(\"\\nsnapshotDiff s1 -> s2:\");\n    LOG.info(hdfs.getSnapshotDiffReport(root, \"s1\", \"s2\").toString());\n\n    assertAtimeEquals(filePreSS, root, \"s0\", \"s1\");\n    assertAtimeEquals(dirPreSS, root, \"s0\", \"s1\");\n\n    assertAtimeEquals(filePreSS, root, \"s1\", \"s2\");\n    assertAtimeEquals(dirPreSS, root, \"s1\", \"s2\");\n\n    assertAtimeEquals(filePostSS, root, \"s1\", \"s2\");\n    assertAtimeEquals(dirPostSS, root, \"s1\", \"s2\");\n\n    // access time should be captured in snapshot due to\n    // other modification\n    assertAtimeNotEquals(filePostSS, root, \"s2\", \"s3\");\n\n    // restart NN, and see the access time relationship\n    // still stands (no change caused by edit logs\n    // loading)\n    cluster.restartNameNodes();\n    cluster.waitActive();\n    assertAtimeEquals(filePreSS, root, \"s0\", \"s1\");\n    assertAtimeEquals(dirPreSS, root, \"s0\", \"s1\");\n\n    assertAtimeEquals(filePreSS, root, \"s1\", \"s2\");\n    assertAtimeEquals(dirPreSS, root, \"s1\", \"s2\");\n\n    assertAtimeEquals(filePostSS, root, \"s1\", \"s2\");\n    assertAtimeEquals(dirPostSS, root, \"s1\", \"s2\");\n\n    assertAtimeNotEquals(filePostSS, root, \"s2\", \"s3\");\n  }\n\n  /**\n   * Tests to verfy the diff report with maximum SnapsdiffReportEntries limit\n   * over an rpc being set to 3.\n   * @throws Exception\n   */\n  @Test\n  public void testDiffReportWithRpcLimit() throws Exception {\n    final Path root = new Path(\"/\");\n    hdfs.mkdirs(root);\n    for (int i = 1; i < 4; i++) {\n      final Path path = new Path(root, \"dir\" + i);\n      hdfs.mkdirs(path);\n    }\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    for (int i = 1; i < 4; i++) {\n      final Path path = new Path(root, \"dir\" + i);\n      for (int j = 1; j < 4; j++) {\n        final Path file = new Path(path, \"file\" + j);\n        DFSTestUtil.createFile(hdfs, file, BLOCKSIZE, REPLICATION, SEED);\n      }\n    }\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n    verifyDiffReport(root, \"s0\", \"s1\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir1/file1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir1/file2\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir1/file3\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir2\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir2/file1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir2/file2\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir2/file3\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir3\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir3/file2\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir3/file3\")));\n  }\n\n  @Test\n  public void testDiffReportWithRpcLimit2() throws Exception {\n    final Path root = new Path(\"/\");\n    hdfs.mkdirs(root);\n    for (int i = 1; i <=3; i++) {\n      final Path path = new Path(root, \"dir\" + i);\n      hdfs.mkdirs(path);\n    }\n    for (int i = 1; i <= 3; i++) {\n      final Path path = new Path(root, \"dir\" + i);\n      for (int j = 1; j < 4; j++) {\n        final Path file = new Path(path, \"file\" + j);\n        DFSTestUtil.createFile(hdfs, file, BLOCKSIZE, REPLICATION, SEED);\n      }\n    }\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    Path targetDir = new Path(root, \"dir4\");\n    //create directory dir4\n    hdfs.mkdirs(targetDir);\n    //moves files from dir1 to dir4\n    Path path = new Path(root, \"dir1\");\n    for (int j = 1; j < 4; j++) {\n      final Path srcPath = new Path(path, \"file\" + j);\n      final Path targetPath = new Path(targetDir, \"file\" + j);\n      hdfs.rename(srcPath, targetPath);\n    }\n    targetDir = new Path(root, \"dir3\");\n    //overwrite existing files in dir3 from files in dir1\n    path = new Path(root, \"dir2\");\n    for (int j = 1; j < 4; j++) {\n      final Path srcPath = new Path(path, \"file\" + j);\n      final Path targetPath = new Path(targetDir, \"file\" + j);\n      hdfs.rename(srcPath, targetPath, Rename.OVERWRITE);\n    }\n    final Path pathToRename = new Path(root, \"dir2\");\n    //move dir2 inside dir3\n    hdfs.rename(pathToRename, targetDir);\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n    verifyDiffReport(root, \"s0\", \"s1\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir4\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2\"),\n            DFSUtil.string2Bytes(\"dir3/dir2\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file1\"),\n            DFSUtil.string2Bytes(\"dir4/file1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file2\"),\n            DFSUtil.string2Bytes(\"dir4/file2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file3\"),\n            DFSUtil.string2Bytes(\"dir4/file3\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file1\"),\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file2\"),\n            DFSUtil.string2Bytes(\"dir3/file2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file3\"),\n            DFSUtil.string2Bytes(\"dir3/file3\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir3\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file3\")));\n  }\n\n  /**\n   * Tests to verify the diff report with maximum SnapsdiffReportEntries limit\n   * over an rpc being set to 3.\n   * @throws Exception\n   */\n  @Test\n  public void testDiffReportWithRpcLimit3() throws Exception {\n    final Path root = new Path(\"/\");\n    hdfs.mkdirs(root);\n    Path path = new Path(root, \"dir1\");\n    hdfs.mkdirs(path);\n    for (int j = 1; j <= 4; j++) {\n      final Path file = new Path(path, \"file\" + j);\n      DFSTestUtil.createFile(hdfs, file, BLOCKSIZE, REPLICATION, SEED);\n    }\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    path = new Path(root, \"dir1\");\n    for (int j = 1; j <= 4; j++) {\n      final Path file = new Path(path, \"file\" + j);\n      hdfs.delete(file, false);\n    }\n    for (int j = 5; j <= 10; j++) {\n      final Path file = new Path(path, \"file\" + j);\n      DFSTestUtil.createFile(hdfs, file, BLOCKSIZE, REPLICATION, SEED);\n    }\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n    verifyDiffReport(root, \"s0\", \"s1\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir1/file5\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir1/file6\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir1/file7\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir1/file8\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir1/file9\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir1/file10\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir1/file1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir1/file2\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir1/file3\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir1/file4\")));\n  }\n\n  private void verifyDiffReportForGivenReport(Path dirPath, String from,\n      String to, SnapshotDiffReport report, DiffReportEntry... entries)\n      throws IOException {\n    // reverse the order of from and to\n    SnapshotDiffReport inverseReport =\n        hdfs.getSnapshotDiffReport(dirPath, to, from);\n    LOG.info(report.toString());\n    LOG.info(inverseReport.toString() + \"\\n\");\n\n    assertEquals(entries.length, report.getDiffList().size());\n    assertEquals(entries.length, inverseReport.getDiffList().size());\n\n    for (DiffReportEntry entry : entries) {\n      if (entry.getType() == DiffType.MODIFY) {\n        assertTrue(report.getDiffList().contains(entry));\n        assertTrue(inverseReport.getDiffList().contains(entry));\n      } else if (entry.getType() == DiffType.DELETE) {\n        assertTrue(report.getDiffList().contains(entry));\n        assertTrue(inverseReport.getDiffList().contains(\n            new DiffReportEntry(DiffType.CREATE, entry.getSourcePath())));\n      } else if (entry.getType() == DiffType.CREATE) {\n        assertTrue(report.getDiffList().contains(entry));\n        assertTrue(inverseReport.getDiffList().contains(\n            new DiffReportEntry(DiffType.DELETE, entry.getSourcePath())));\n      }\n    }\n  }\n\n  @Test\n  public void testSnapshotDiffReportRemoteIterator() throws Exception {\n    final Path root = new Path(\"/\");\n    hdfs.mkdirs(root);\n    for (int i = 1; i <= 3; i++) {\n      final Path path = new Path(root, \"dir\" + i);\n      hdfs.mkdirs(path);\n    }\n    for (int i = 1; i <= 3; i++) {\n      final Path path = new Path(root, \"dir\" + i);\n      for (int j = 1; j < 4; j++) {\n        final Path file = new Path(path, \"file\" + j);\n        DFSTestUtil.createFile(hdfs, file, BLOCKSIZE, REPLICATION, SEED);\n      }\n    }\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    Path targetDir = new Path(root, \"dir4\");\n    //create directory dir4\n    hdfs.mkdirs(targetDir);\n    //moves files from dir1 to dir4\n    Path path = new Path(root, \"dir1\");\n    for (int j = 1; j < 4; j++) {\n      final Path srcPath = new Path(path, \"file\" + j);\n      final Path targetPath = new Path(targetDir, \"file\" + j);\n      hdfs.rename(srcPath, targetPath);\n    }\n    targetDir = new Path(root, \"dir3\");\n    //overwrite existing files in dir3 from files in dir1\n    path = new Path(root, \"dir2\");\n    for (int j = 1; j < 4; j++) {\n      final Path srcPath = new Path(path, \"file\" + j);\n      final Path targetPath = new Path(targetDir, \"file\" + j);\n      hdfs.rename(srcPath, targetPath, Rename.OVERWRITE);\n    }\n    final Path pathToRename = new Path(root, \"dir2\");\n    //move dir2 inside dir3\n    hdfs.rename(pathToRename, targetDir);\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n    RemoteIterator<SnapshotDiffReportListing> iterator =\n        hdfs.snapshotDiffReportListingRemoteIterator(root, \"s0\", \"s1\");\n    SnapshotDiffReportGenerator snapshotDiffReport;\n    List<SnapshotDiffReportListing.DiffReportListingEntry> modifiedList =\n        new TreeList();\n    List<SnapshotDiffReportListing.DiffReportListingEntry> createdList =\n        new ChunkedArrayList<>();\n    List<SnapshotDiffReportListing.DiffReportListingEntry> deletedList =\n        new ChunkedArrayList<>();\n    SnapshotDiffReportListing report = null;\n    List<SnapshotDiffReportListing> reportList = new ArrayList<>();\n    while (iterator.hasNext()) {\n      report = iterator.next();\n      reportList.add(report);\n      modifiedList.addAll(report.getModifyList());\n      createdList.addAll(report.getCreateList());\n      deletedList.addAll(report.getDeleteList());\n    }\n    try {\n      iterator.next();\n    } catch (Exception e) {\n      Assert.assertTrue(\n          e.getMessage().contains(\"No more entry in SnapshotDiffReport for /\"));\n    }\n    Assert.assertNotEquals(0, reportList.size());\n    // generate the snapshotDiffReport and Verify\n    snapshotDiffReport = new SnapshotDiffReportGenerator(\"/\", \"s0\", \"s1\",\n        report.getIsFromEarlier(), modifiedList, createdList, deletedList);\n    verifyDiffReportForGivenReport(root, \"s0\", \"s1\",\n        snapshotDiffReport.generateReport(),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"dir4\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2\"),\n            DFSUtil.string2Bytes(\"dir3/dir2\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file1\"),\n            DFSUtil.string2Bytes(\"dir4/file1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file2\"),\n            DFSUtil.string2Bytes(\"dir4/file2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file3\"),\n            DFSUtil.string2Bytes(\"dir4/file3\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file1\"),\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file2\"),\n            DFSUtil.string2Bytes(\"dir3/file2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file3\"),\n            DFSUtil.string2Bytes(\"dir3/file3\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir3\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file3\")));\n  }\n\n  @Test\n  public void testSnapshotDiffReportRemoteIterator2() throws Exception {\n    final Path root = new Path(\"/\");\n    hdfs.mkdirs(root);\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    try {\n      hdfs.snapshotDiffReportListingRemoteIterator(root, \"s0\", \"\");\n    } catch (Exception e) {\n      Assert.assertTrue(e.getMessage().contains(\"Remote Iterator is\"\n          + \"supported for snapshotDiffReport between two snapshots\"));\n    }\n  }\n}\n"
    },
    {
      "file_path": "D:\\Disaster\\Codefield\\Code_Python\\Anti-patternRAG\\data\\AWD\\apache\\hadoop\\commit_1200\\1366\\before\\hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java",
      "chunk_type": "superType",
      "ast_subtree": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.hadoop.fs;\n\nimport javax.annotation.Nonnull;\nimport java.io.Closeable;\nimport java.io.FileNotFoundException;\nimport java.io.IOException;\nimport java.lang.ref.WeakReference;\nimport java.lang.ref.ReferenceQueue;\nimport java.net.URI;\nimport java.net.URISyntaxException;\nimport java.security.PrivilegedExceptionAction;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.EnumSet;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.IdentityHashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.NoSuchElementException;\nimport java.util.Optional;\nimport java.util.ServiceConfigurationError;\nimport java.util.ServiceLoader;\nimport java.util.Set;\nimport java.util.Stack;\nimport java.util.TreeSet;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.Semaphore;\nimport java.util.concurrent.atomic.AtomicLong;\n\nimport org.apache.hadoop.classification.InterfaceAudience;\nimport org.apache.hadoop.classification.InterfaceStability;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.GlobalStorageStatistics.StorageStatisticsProvider;\nimport org.apache.hadoop.fs.Options.ChecksumOpt;\nimport org.apache.hadoop.fs.Options.HandleOpt;\nimport org.apache.hadoop.fs.Options.Rename;\nimport org.apache.hadoop.fs.impl.AbstractFSBuilderImpl;\nimport org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl;\nimport org.apache.hadoop.fs.impl.OpenFileParameters;\nimport org.apache.hadoop.fs.permission.AclEntry;\nimport org.apache.hadoop.fs.permission.AclStatus;\nimport org.apache.hadoop.fs.permission.FsAction;\nimport org.apache.hadoop.fs.permission.FsCreateModes;\nimport org.apache.hadoop.fs.permission.FsPermission;\nimport org.apache.hadoop.io.IOUtils;\nimport org.apache.hadoop.io.MultipleIOException;\nimport org.apache.hadoop.net.NetUtils;\nimport org.apache.hadoop.security.AccessControlException;\nimport org.apache.hadoop.security.Credentials;\nimport org.apache.hadoop.security.SecurityUtil;\nimport org.apache.hadoop.security.UserGroupInformation;\nimport org.apache.hadoop.security.token.Token;\nimport org.apache.hadoop.security.token.DelegationTokenIssuer;\nimport org.apache.hadoop.util.ClassUtil;\nimport org.apache.hadoop.util.DataChecksum;\nimport org.apache.hadoop.util.DurationInfo;\nimport org.apache.hadoop.util.LambdaUtils;\nimport org.apache.hadoop.util.Progressable;\nimport org.apache.hadoop.util.ReflectionUtils;\nimport org.apache.hadoop.util.ShutdownHookManager;\nimport org.apache.hadoop.util.StringUtils;\nimport org.apache.hadoop.tracing.Tracer;\nimport org.apache.hadoop.tracing.TraceScope;\nimport org.apache.hadoop.util.Preconditions;\nimport org.apache.hadoop.classification.VisibleForTesting;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport static org.apache.hadoop.fs.Options.OpenFileOptions.FS_OPTION_OPENFILE_BUFFER_SIZE;\nimport static org.apache.hadoop.util.Preconditions.checkArgument;\nimport static org.apache.hadoop.fs.CommonConfigurationKeysPublic.*;\nimport static org.apache.hadoop.fs.impl.PathCapabilitiesSupport.validatePathCapabilityArgs;\n\n/****************************************************************\n * An abstract base class for a fairly generic filesystem.  It\n * may be implemented as a distributed filesystem, or as a \"local\"\n * one that reflects the locally-connected disk.  The local version\n * exists for small Hadoop instances and for testing.\n *\n * <p>\n *\n * All user code that may potentially use the Hadoop Distributed\n * File System should be written to use a FileSystem object or its\n * successor, {@link FileContext}.\n * </p>\n * <p>\n * The local implementation is {@link LocalFileSystem} and distributed\n * implementation is DistributedFileSystem. There are other implementations\n * for object stores and (outside the Apache Hadoop codebase),\n * third party filesystems.\n * </p>\n * Notes\n * <ol>\n * <li>The behaviour of the filesystem is\n * <a href=\"https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/filesystem/filesystem.html\">\n * specified in the Hadoop documentation. </a>\n * However, the normative specification of the behavior of this class is\n * actually HDFS: if HDFS does not behave the way these Javadocs or\n * the specification in the Hadoop documentations define, assume that\n * the documentation is incorrect.\n * </li>\n * <li>The term {@code FileSystem} refers to an instance of this class.</li>\n * <li>The acronym \"FS\" is used as an abbreviation of FileSystem.</li>\n * <li>The term {@code filesystem} refers to the distributed/local filesystem\n * itself, rather than the class used to interact with it.</li>\n * <li>The term \"file\" refers to a file in the remote filesystem,\n * rather than instances of {@code java.io.File}.</li>\n * </ol>\n *\n * This is a carefully evolving class.\n * New methods may be marked as Unstable or Evolving for their initial release,\n * as a warning that they are new and may change based on the\n * experience of use in applications.\n * <p>\n * <b>Important note for developers</b>\n * </p>\n * If you are making changes here to the public API or protected methods,\n * you must review the following subclasses and make sure that\n * they are filtering/passing through new methods as appropriate.\n *\n * {@link FilterFileSystem}: methods are passed through. If not,\n * then {@code TestFilterFileSystem.MustNotImplement} must be\n * updated with the unsupported interface.\n * Furthermore, if the new API's support is probed for via\n * {@link #hasPathCapability(Path, String)} then\n * {@link FilterFileSystem#hasPathCapability(Path, String)}\n * must return false, always.\n * <p>\n * {@link ChecksumFileSystem}: checksums are created and\n * verified.\n * </p>\n * {@code TestHarFileSystem} will need its {@code MustNotImplement}\n * interface updated.\n *\n * <p>\n * There are some external places your changes will break things.\n * Do co-ordinate changes here.\n * </p>\n *\n * HBase: HBoss\n * <p>\n * Hive: HiveShim23\n * </p>\n * {@code shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java}\n *\n *****************************************************************/\n@SuppressWarnings(\"DeprecatedIsStillUsed\")\n@InterfaceAudience.Public\n@InterfaceStability.Stable\npublic abstract class FileSystem extends Configured\n    implements Closeable, DelegationTokenIssuer, PathCapabilities {\n  public static final String FS_DEFAULT_NAME_KEY =\n                   CommonConfigurationKeys.FS_DEFAULT_NAME_KEY;\n  public static final String DEFAULT_FS =\n                   CommonConfigurationKeys.FS_DEFAULT_NAME_DEFAULT;\n\n  /**\n   * This log is widely used in the org.apache.hadoop.fs code and tests,\n   * so must be considered something to only be changed with care.\n   */\n  @InterfaceAudience.Private\n  public static final Logger LOG = LoggerFactory.getLogger(FileSystem.class);\n\n  /**\n   * The SLF4J logger to use in logging within the FileSystem class itself.\n   */\n  private static final Logger LOGGER =\n      LoggerFactory.getLogger(FileSystem.class);\n\n  /**\n   * Priority of the FileSystem shutdown hook: {@value}.\n   */\n  public static final int SHUTDOWN_HOOK_PRIORITY = 10;\n\n  /**\n   * Prefix for trash directory: {@value}.\n   */\n  public static final String TRASH_PREFIX = \".Trash\";\n  public static final String USER_HOME_PREFIX = \"/user\";\n\n  /** FileSystem cache. */\n  static final Cache CACHE = new Cache(new Configuration());\n\n  /** The key this instance is stored under in the cache. */\n  private Cache.Key key;\n\n  /** Recording statistics per a FileSystem class. */\n  private static final Map<Class<? extends FileSystem>, Statistics>\n      statisticsTable = new IdentityHashMap<>();\n\n  /**\n   * The statistics for this file system.\n   */\n  protected Statistics statistics;\n\n  /**\n   * A cache of files that should be deleted when the FileSystem is closed\n   * or the JVM is exited.\n   */\n  private final Set<Path> deleteOnExit = new TreeSet<>();\n\n  /**\n   * Should symbolic links be resolved by {@link FileSystemLinkResolver}.\n   * Set to the value of\n   * {@link CommonConfigurationKeysPublic#FS_CLIENT_RESOLVE_REMOTE_SYMLINKS_KEY}\n   */\n  boolean resolveSymlinks;\n\n  /**\n   * This method adds a FileSystem instance to the cache so that it can\n   * be retrieved later. It is only for testing.\n   * @param uri the uri to store it under\n   * @param conf the configuration to store it under\n   * @param fs the FileSystem to store\n   * @throws IOException if the current user cannot be determined.\n   */\n  @VisibleForTesting\n  static void addFileSystemForTesting(URI uri, Configuration conf,\n      FileSystem fs) throws IOException {\n    CACHE.map.put(new Cache.Key(uri, conf), fs);\n  }\n\n  @VisibleForTesting\n  static void removeFileSystemForTesting(URI uri, Configuration conf,\n      FileSystem fs) throws IOException {\n    CACHE.map.remove(new Cache.Key(uri, conf), fs);\n  }\n\n  @VisibleForTesting\n  static int cacheSize() {\n    return CACHE.map.size();\n  }\n\n  /**\n   * Get a FileSystem instance based on the uri, the passed in\n   * configuration and the user.\n   * @param uri of the filesystem\n   * @param conf the configuration to use\n   * @param user to perform the get as\n   * @return the filesystem instance\n   * @throws IOException failure to load\n   * @throws InterruptedException If the {@code UGI.doAs()} call was\n   * somehow interrupted.\n   */\n  public static FileSystem get(final URI uri, final Configuration conf,\n        final String user) throws IOException, InterruptedException {\n    String ticketCachePath =\n      conf.get(CommonConfigurationKeys.KERBEROS_TICKET_CACHE_PATH);\n    UserGroupInformation ugi =\n        UserGroupInformation.getBestUGI(ticketCachePath, user);\n    return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {\n      @Override\n      public FileSystem run() throws IOException {\n        return get(uri, conf);\n      }\n    });\n  }\n\n  /**\n   * Returns the configured FileSystem implementation.\n   * @param conf the configuration to use\n   * @return FileSystem.\n   * @throws IOException If an I/O error occurred.\n   */\n  public static FileSystem get(Configuration conf) throws IOException {\n    return get(getDefaultUri(conf), conf);\n  }\n\n  /**\n   * Get the default FileSystem URI from a configuration.\n   * @param conf the configuration to use\n   * @return the uri of the default filesystem\n   */\n  public static URI getDefaultUri(Configuration conf) {\n    URI uri =\n        URI.create(fixName(conf.getTrimmed(FS_DEFAULT_NAME_KEY, DEFAULT_FS)));\n    if (uri.getScheme() == null) {\n      throw new IllegalArgumentException(\"No scheme in default FS: \" + uri);\n    }\n    return uri;\n  }\n\n  /**\n   * Set the default FileSystem URI in a configuration.\n   * @param conf the configuration to alter\n   * @param uri the new default filesystem uri\n   */\n  public static void setDefaultUri(Configuration conf, URI uri) {\n    conf.set(FS_DEFAULT_NAME_KEY, uri.toString());\n  }\n\n  /** Set the default FileSystem URI in a configuration.\n   * @param conf the configuration to alter\n   * @param uri the new default filesystem uri\n   */\n  public static void setDefaultUri(Configuration conf, String uri) {\n    setDefaultUri(conf, URI.create(fixName(uri)));\n  }\n\n  /**\n   * Initialize a FileSystem.\n   *\n   * Called after the new FileSystem instance is constructed, and before it\n   * is ready for use.\n   *\n   * FileSystem implementations overriding this method MUST forward it to\n   * their superclass, though the order in which it is done, and whether\n   * to alter the configuration before the invocation are options of the\n   * subclass.\n   * @param name a URI whose authority section names the host, port, etc.\n   *   for this FileSystem\n   * @param conf the configuration\n   * @throws IOException on any failure to initialize this instance.\n   * @throws IllegalArgumentException if the URI is considered invalid.\n   */\n  public void initialize(URI name, Configuration conf) throws IOException {\n    final String scheme;\n    if (name.getScheme() == null || name.getScheme().isEmpty()) {\n      scheme = getDefaultUri(conf).getScheme();\n    } else {\n      scheme = name.getScheme();\n    }\n    statistics = getStatistics(scheme, getClass());\n    resolveSymlinks = conf.getBoolean(\n        CommonConfigurationKeysPublic.FS_CLIENT_RESOLVE_REMOTE_SYMLINKS_KEY,\n        CommonConfigurationKeysPublic.FS_CLIENT_RESOLVE_REMOTE_SYMLINKS_DEFAULT);\n  }\n\n  /**\n   * Return the protocol scheme for this FileSystem.\n   * <p>\n   * This implementation throws an <code>UnsupportedOperationException</code>.\n   *\n   * @return the protocol scheme for this FileSystem.\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default).\n   */\n  public String getScheme() {\n    throw new UnsupportedOperationException(\"Not implemented by the \"\n        + getClass().getSimpleName() + \" FileSystem implementation\");\n  }\n\n  /**\n   * Returns a URI which identifies this FileSystem.\n   *\n   * @return the URI of this filesystem.\n   */\n  public abstract URI getUri();\n\n  /**\n   * Return a canonicalized form of this FileSystem's URI.\n   *\n   * The default implementation simply calls {@link #canonicalizeUri(URI)}\n   * on the filesystem's own URI, so subclasses typically only need to\n   * implement that method.\n   *\n   * @see #canonicalizeUri(URI)\n   * @return the URI of this filesystem.\n   */\n  protected URI getCanonicalUri() {\n    return canonicalizeUri(getUri());\n  }\n\n  /**\n   * Canonicalize the given URI.\n   *\n   * This is implementation-dependent, and may for example consist of\n   * canonicalizing the hostname using DNS and adding the default\n   * port if not specified.\n   *\n   * The default implementation simply fills in the default port if\n   * not specified and if {@link #getDefaultPort()} returns a\n   * default port.\n   *\n   * @param uri url.\n   * @return URI\n   * @see NetUtils#getCanonicalUri(URI, int)\n   */\n  protected URI canonicalizeUri(URI uri) {\n    if (uri.getPort() == -1 && getDefaultPort() > 0) {\n      // reconstruct the uri with the default port set\n      try {\n        uri = new URI(uri.getScheme(), uri.getUserInfo(),\n            uri.getHost(), getDefaultPort(),\n            uri.getPath(), uri.getQuery(), uri.getFragment());\n      } catch (URISyntaxException e) {\n        // Should never happen!\n        throw new AssertionError(\"Valid URI became unparseable: \" +\n            uri);\n      }\n    }\n\n    return uri;\n  }\n\n  /**\n   * Get the default port for this FileSystem.\n   * @return the default port or 0 if there isn't one\n   */\n  protected int getDefaultPort() {\n    return 0;\n  }\n\n  protected static FileSystem getFSofPath(final Path absOrFqPath,\n      final Configuration conf)\n      throws UnsupportedFileSystemException, IOException {\n    absOrFqPath.checkNotSchemeWithRelative();\n    absOrFqPath.checkNotRelative();\n\n    // Uses the default FileSystem if not fully qualified\n    return get(absOrFqPath.toUri(), conf);\n  }\n\n  /**\n   * Get a canonical service name for this FileSystem.\n   * The token cache is the only user of the canonical service name,\n   * and uses it to lookup this FileSystem's service tokens.\n   * If the file system provides a token of its own then it must have a\n   * canonical name, otherwise the canonical name can be null.\n   *\n   * Default implementation: If the FileSystem has child file systems\n   * (such as an embedded file system) then it is assumed that the FS has no\n   * tokens of its own and hence returns a null name; otherwise a service\n   * name is built using Uri and port.\n   *\n   * @return a service string that uniquely identifies this file system, null\n   *         if the filesystem does not implement tokens\n   * @see SecurityUtil#buildDTServiceName(URI, int)\n   */\n  @InterfaceAudience.Public\n  @InterfaceStability.Evolving\n  @Override\n  public String getCanonicalServiceName() {\n    return (getChildFileSystems() == null)\n      ? SecurityUtil.buildDTServiceName(getUri(), getDefaultPort())\n      : null;\n  }\n\n  /**\n   * @return uri to string.\n   * @deprecated call {@link #getUri()} instead.\n   */\n  @Deprecated\n  public String getName() { return getUri().toString(); }\n\n  /**\n   * @deprecated call {@link #get(URI, Configuration)} instead.\n   *\n   * @param name name.\n   * @param conf configuration.\n   * @return file system.\n   * @throws IOException If an I/O error occurred.\n   */\n  @Deprecated\n  public static FileSystem getNamed(String name, Configuration conf)\n    throws IOException {\n    return get(URI.create(fixName(name)), conf);\n  }\n\n  /** Update old-format filesystem names, for back-compatibility.  This should\n   * eventually be replaced with a checkName() method that throws an exception\n   * for old-format names.\n   */\n  private static String fixName(String name) {\n    // convert old-format name to new-format name\n    if (name.equals(\"local\")) {         // \"local\" is now \"file:///\".\n      LOGGER.warn(\"\\\"local\\\" is a deprecated filesystem name.\"\n               +\" Use \\\"file:///\\\" instead.\");\n      name = \"file:///\";\n    } else if (name.indexOf('/')==-1) {   // unqualified is \"hdfs://\"\n      LOGGER.warn(\"\\\"\"+name+\"\\\" is a deprecated filesystem name.\"\n               +\" Use \\\"hdfs://\"+name+\"/\\\" instead.\");\n      name = \"hdfs://\"+name;\n    }\n    return name;\n  }\n\n  /**\n   * Get the local FileSystem.\n   * @param conf the configuration to configure the FileSystem with\n   * if it is newly instantiated.\n   * @return a LocalFileSystem\n   * @throws IOException if somehow the local FS cannot be instantiated.\n   */\n  public static LocalFileSystem getLocal(Configuration conf)\n    throws IOException {\n    return (LocalFileSystem)get(LocalFileSystem.NAME, conf);\n  }\n\n  /**\n   * Get a FileSystem for this URI's scheme and authority.\n   * <ol>\n   * <li>\n   *   If the configuration has the property\n   *   {@code \"fs.$SCHEME.impl.disable.cache\"} set to true,\n   *   a new instance will be created, initialized with the supplied URI and\n   *   configuration, then returned without being cached.\n   * </li>\n   * <li>\n   *   If the there is a cached FS instance matching the same URI, it will\n   *   be returned.\n   * </li>\n   * <li>\n   *   Otherwise: a new FS instance will be created, initialized with the\n   *   configuration and URI, cached and returned to the caller.\n   * </li>\n   * </ol>\n   * @param uri uri of the filesystem.\n   * @param conf configrution.\n   * @return filesystem instance.\n   * @throws IOException if the FileSystem cannot be instantiated.\n   */\n  public static FileSystem get(URI uri, Configuration conf) throws IOException {\n    String scheme = uri.getScheme();\n    String authority = uri.getAuthority();\n\n    if (scheme == null && authority == null) {     // use default FS\n      return get(conf);\n    }\n\n    if (scheme != null && authority == null) {     // no authority\n      URI defaultUri = getDefaultUri(conf);\n      if (scheme.equals(defaultUri.getScheme())    // if scheme matches default\n          && defaultUri.getAuthority() != null) {  // & default has authority\n        return get(defaultUri, conf);              // return default\n      }\n    }\n    String disableCacheName = String.format(\"fs.%s.impl.disable.cache\", scheme);\n    if (conf.getBoolean(disableCacheName, false)) {\n      LOGGER.debug(\"Bypassing cache to create filesystem {}\", uri);\n      return createFileSystem(uri, conf);\n    }\n\n    return CACHE.get(uri, conf);\n  }\n\n  /**\n   * Returns the FileSystem for this URI's scheme and authority and the\n   * given user. Internally invokes {@link #newInstance(URI, Configuration)}\n   * @param uri uri of the filesystem.\n   * @param conf the configuration to use\n   * @param user to perform the get as\n   * @return filesystem instance\n   * @throws IOException if the FileSystem cannot be instantiated.\n   * @throws InterruptedException If the {@code UGI.doAs()} call was\n   *         somehow interrupted.\n   */\n  public static FileSystem newInstance(final URI uri, final Configuration conf,\n      final String user) throws IOException, InterruptedException {\n    String ticketCachePath =\n      conf.get(CommonConfigurationKeys.KERBEROS_TICKET_CACHE_PATH);\n    UserGroupInformation ugi =\n        UserGroupInformation.getBestUGI(ticketCachePath, user);\n    return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {\n      @Override\n      public FileSystem run() throws IOException {\n        return newInstance(uri, conf);\n      }\n    });\n  }\n\n  /**\n   * Returns the FileSystem for this URI's scheme and authority.\n   * The entire URI is passed to the FileSystem instance's initialize method.\n   * This always returns a new FileSystem object.\n   * @param uri FS URI\n   * @param config configuration to use\n   * @return the new FS instance\n   * @throws IOException FS creation or initialization failure.\n   */\n  public static FileSystem newInstance(URI uri, Configuration config)\n      throws IOException {\n    String scheme = uri.getScheme();\n    String authority = uri.getAuthority();\n\n    if (scheme == null) {                       // no scheme: use default FS\n      return newInstance(config);\n    }\n\n    if (authority == null) {                       // no authority\n      URI defaultUri = getDefaultUri(config);\n      if (scheme.equals(defaultUri.getScheme())    // if scheme matches default\n          && defaultUri.getAuthority() != null) {  // & default has authority\n        return newInstance(defaultUri, config);              // return default\n      }\n    }\n    return CACHE.getUnique(uri, config);\n  }\n\n  /**\n   * Returns a unique configured FileSystem implementation for the default\n   * filesystem of the supplied configuration.\n   * This always returns a new FileSystem object.\n   * @param conf the configuration to use\n   * @return the new FS instance\n   * @throws IOException FS creation or initialization failure.\n   */\n  public static FileSystem newInstance(Configuration conf) throws IOException {\n    return newInstance(getDefaultUri(conf), conf);\n  }\n\n  /**\n   * Get a unique local FileSystem object.\n   * @param conf the configuration to configure the FileSystem with\n   * @return a new LocalFileSystem object.\n   * @throws IOException FS creation or initialization failure.\n   */\n  public static LocalFileSystem newInstanceLocal(Configuration conf)\n    throws IOException {\n    return (LocalFileSystem)newInstance(LocalFileSystem.NAME, conf);\n  }\n\n  /**\n   * Close all cached FileSystem instances. After this operation, they\n   * may not be used in any operations.\n   *\n   * @throws IOException a problem arose closing one or more filesystem.\n   */\n  public static void closeAll() throws IOException {\n    debugLogFileSystemClose(\"closeAll\", \"\");\n    CACHE.closeAll();\n  }\n\n  /**\n   * Close all cached FileSystem instances for a given UGI.\n   * Be sure those filesystems are not used anymore.\n   * @param ugi user group info to close\n   * @throws IOException a problem arose closing one or more filesystem.\n   */\n  public static void closeAllForUGI(UserGroupInformation ugi)\n      throws IOException {\n    debugLogFileSystemClose(\"closeAllForUGI\", \"UGI: \" + ugi);\n    CACHE.closeAll(ugi);\n  }\n\n  private static void debugLogFileSystemClose(String methodName,\n      String additionalInfo) {\n    if (LOGGER.isDebugEnabled()) {\n      Throwable throwable = new Throwable().fillInStackTrace();\n      LOGGER.debug(\"FileSystem.{}() by method: {}); {}\", methodName,\n          throwable.getStackTrace()[2], additionalInfo);\n      if (LOGGER.isTraceEnabled()) {\n        LOGGER.trace(\"FileSystem.{}() full stack trace:\", methodName,\n            throwable);\n      }\n    }\n  }\n\n  /**\n   * Qualify a path to one which uses this FileSystem and, if relative,\n   * made absolute.\n   * @param path to qualify.\n   * @return this path if it contains a scheme and authority and is absolute, or\n   * a new path that includes a path and authority and is fully qualified\n   * @see Path#makeQualified(URI, Path)\n   * @throws IllegalArgumentException if the path has a schema/URI different\n   * from this FileSystem.\n   */\n  public Path makeQualified(Path path) {\n    checkPath(path);\n    return path.makeQualified(this.getUri(), this.getWorkingDirectory());\n  }\n\n  /**\n   * Get a new delegation token for this FileSystem.\n   * This is an internal method that should have been declared protected\n   * but wasn't historically.\n   * Callers should use {@link #addDelegationTokens(String, Credentials)}\n   *\n   * @param renewer the account name that is allowed to renew the token.\n   * @return a new delegation token or null if the FS does not support tokens.\n   * @throws IOException on any problem obtaining a token\n   */\n  @InterfaceAudience.Private()\n  @Override\n  public Token<?> getDelegationToken(String renewer) throws IOException {\n    return null;\n  }\n\n  /**\n   * Get all the immediate child FileSystems embedded in this FileSystem.\n   * It does not recurse and get grand children.  If a FileSystem\n   * has multiple child FileSystems, then it must return a unique list\n   * of those FileSystems.  Default is to return null to signify no children.\n   *\n   * @return FileSystems that are direct children of this FileSystem,\n   *         or null for \"no children\"\n   */\n  @InterfaceAudience.LimitedPrivate({ \"HDFS\" })\n  @VisibleForTesting\n  public FileSystem[] getChildFileSystems() {\n    return null;\n  }\n\n  @InterfaceAudience.Private\n  @Override\n  public DelegationTokenIssuer[] getAdditionalTokenIssuers()\n      throws IOException {\n    return getChildFileSystems();\n  }\n\n  /**\n   * Create a file with the provided permission.\n   *\n   * The permission of the file is set to be the provided permission as in\n   * setPermission, not permission{@literal &~}umask\n   *\n   * The HDFS implementation is implemented using two RPCs.\n   * It is understood that it is inefficient,\n   * but the implementation is thread-safe. The other option is to change the\n   * value of umask in configuration to be 0, but it is not thread-safe.\n   *\n   * @param fs FileSystem\n   * @param file the name of the file to be created\n   * @param permission the permission of the file\n   * @return an output stream\n   * @throws IOException IO failure\n   */\n  public static FSDataOutputStream create(FileSystem fs,\n      Path file, FsPermission permission) throws IOException {\n    // create the file with default permission\n    FSDataOutputStream out = fs.create(file);\n    // set its permission to the supplied one\n    fs.setPermission(file, permission);\n    return out;\n  }\n\n  /**\n   * Create a directory with the provided permission.\n   * The permission of the directory is set to be the provided permission as in\n   * setPermission, not permission{@literal &~}umask\n   *\n   * @see #create(FileSystem, Path, FsPermission)\n   *\n   * @param fs FileSystem handle\n   * @param dir the name of the directory to be created\n   * @param permission the permission of the directory\n   * @return true if the directory creation succeeds; false otherwise\n   * @throws IOException A problem creating the directories.\n   */\n  public static boolean mkdirs(FileSystem fs, Path dir, FsPermission permission)\n      throws IOException {\n    // create the directory using the default permission\n    boolean result = fs.mkdirs(dir);\n    // set its permission to be the supplied one\n    fs.setPermission(dir, permission);\n    return result;\n  }\n\n  ///////////////////////////////////////////////////////////////\n  // FileSystem\n  ///////////////////////////////////////////////////////////////\n\n  protected FileSystem() {\n    super(null);\n  }\n\n  /**\n   * Check that a Path belongs to this FileSystem.\n   *\n   * The base implementation performs case insensitive equality checks\n   * of the URIs' schemes and authorities. Subclasses may implement slightly\n   * different checks.\n   * @param path to check\n   * @throws IllegalArgumentException if the path is not considered to be\n   * part of this FileSystem.\n   *\n   */\n  protected void checkPath(Path path) {\n    Preconditions.checkArgument(path != null, \"null path\");\n    URI uri = path.toUri();\n    String thatScheme = uri.getScheme();\n    if (thatScheme == null)                // fs is relative\n      return;\n    URI thisUri = getCanonicalUri();\n    String thisScheme = thisUri.getScheme();\n    //authority and scheme are not case sensitive\n    if (thisScheme.equalsIgnoreCase(thatScheme)) {// schemes match\n      String thisAuthority = thisUri.getAuthority();\n      String thatAuthority = uri.getAuthority();\n      if (thatAuthority == null &&                // path's authority is null\n          thisAuthority != null) {                // fs has an authority\n        URI defaultUri = getDefaultUri(getConf());\n        if (thisScheme.equalsIgnoreCase(defaultUri.getScheme())) {\n          uri = defaultUri; // schemes match, so use this uri instead\n        } else {\n          uri = null; // can't determine auth of the path\n        }\n      }\n      if (uri != null) {\n        // canonicalize uri before comparing with this fs\n        uri = canonicalizeUri(uri);\n        thatAuthority = uri.getAuthority();\n        if (thisAuthority == thatAuthority ||       // authorities match\n            (thisAuthority != null &&\n             thisAuthority.equalsIgnoreCase(thatAuthority)))\n          return;\n      }\n    }\n    throw new IllegalArgumentException(\"Wrong FS: \" + path +\n                                       \", expected: \" + this.getUri());\n  }\n\n  /**\n   * Return an array containing hostnames, offset and size of\n   * portions of the given file.  For nonexistent\n   * file or regions, {@code null} is returned.\n   *\n   * <pre>\n   *   if f == null :\n   *     result = null\n   *   elif f.getLen() {@literal <=} start:\n   *     result = []\n   *   else result = [ locations(FS, b) for b in blocks(FS, p, s, s+l)]\n   * </pre>\n   * This call is most helpful with and distributed filesystem\n   * where the hostnames of machines that contain blocks of the given file\n   * can be determined.\n   *\n   * The default implementation returns an array containing one element:\n   * <pre>\n   * BlockLocation( { \"localhost:9866\" },  { \"localhost\" }, 0, file.getLen())\n   * </pre>\n   *\n   * In HDFS, if file is three-replicated, the returned array contains\n   * elements like:\n   * <pre>\n   * BlockLocation(offset: 0, length: BLOCK_SIZE,\n   *   hosts: {\"host1:9866\", \"host2:9866, host3:9866\"})\n   * BlockLocation(offset: BLOCK_SIZE, length: BLOCK_SIZE,\n   *   hosts: {\"host2:9866\", \"host3:9866, host4:9866\"})\n   * </pre>\n   *\n   * And if a file is erasure-coded, the returned BlockLocation are logical\n   * block groups.\n   *\n   * Suppose we have a RS_3_2 coded file (3 data units and 2 parity units).\n   * 1. If the file size is less than one stripe size, say 2 * CELL_SIZE, then\n   * there will be one BlockLocation returned, with 0 offset, actual file size\n   * and 4 hosts (2 data blocks and 2 parity blocks) hosting the actual blocks.\n   * 3. If the file size is less than one group size but greater than one\n   * stripe size, then there will be one BlockLocation returned, with 0 offset,\n   * actual file size with 5 hosts (3 data blocks and 2 parity blocks) hosting\n   * the actual blocks.\n   * 4. If the file size is greater than one group size, 3 * BLOCK_SIZE + 123\n   * for example, then the result will be like:\n   * <pre>\n   * BlockLocation(offset: 0, length: 3 * BLOCK_SIZE, hosts: {\"host1:9866\",\n   *   \"host2:9866\",\"host3:9866\",\"host4:9866\",\"host5:9866\"})\n   * BlockLocation(offset: 3 * BLOCK_SIZE, length: 123, hosts: {\"host1:9866\",\n   *   \"host4:9866\", \"host5:9866\"})\n   * </pre>\n   *\n   * @param file FilesStatus to get data from\n   * @param start offset into the given file\n   * @param len length for which to get locations for\n   * @throws IOException IO failure\n   * @return block location array.\n   */\n  public BlockLocation[] getFileBlockLocations(FileStatus file,\n      long start, long len) throws IOException {\n    if (file == null) {\n      return null;\n    }\n\n    if (start < 0 || len < 0) {\n      throw new IllegalArgumentException(\"Invalid start or len parameter\");\n    }\n\n    if (file.getLen() <= start) {\n      return new BlockLocation[0];\n\n    }\n    String[] name = {\"localhost:9866\"};\n    String[] host = {\"localhost\"};\n    return new BlockLocation[] {\n      new BlockLocation(name, host, 0, file.getLen()) };\n  }\n\n  /**\n   * Return an array containing hostnames, offset and size of\n   * portions of the given file.  For a nonexistent\n   * file or regions, {@code null} is returned.\n   *\n   * This call is most helpful with location-aware distributed\n   * filesystems, where it returns hostnames of machines that\n   * contain the given file.\n   *\n   * A FileSystem will normally return the equivalent result\n   * of passing the {@code FileStatus} of the path to\n   * {@link #getFileBlockLocations(FileStatus, long, long)}\n   *\n   * @param p path is used to identify an FS since an FS could have\n   *          another FS that it could be delegating the call to\n   * @param start offset into the given file\n   * @param len length for which to get locations for\n   * @throws FileNotFoundException when the path does not exist\n   * @throws IOException IO failure\n   * @return block location array.\n   */\n  public BlockLocation[] getFileBlockLocations(Path p,\n      long start, long len) throws IOException {\n    if (p == null) {\n      throw new NullPointerException();\n    }\n    FileStatus file = getFileStatus(p);\n    return getFileBlockLocations(file, start, len);\n  }\n\n  /**\n   * Return a set of server default configuration values.\n   * @return server default configuration values\n   * @throws IOException IO failure\n   * @deprecated use {@link #getServerDefaults(Path)} instead\n   */\n  @Deprecated\n  public FsServerDefaults getServerDefaults() throws IOException {\n    Configuration config = getConf();\n    // CRC32 is chosen as default as it is available in all\n    // releases that support checksum.\n    // The client trash configuration is ignored.\n    return new FsServerDefaults(getDefaultBlockSize(),\n        config.getInt(\"io.bytes.per.checksum\", 512),\n        64 * 1024,\n        getDefaultReplication(),\n        config.getInt(IO_FILE_BUFFER_SIZE_KEY, IO_FILE_BUFFER_SIZE_DEFAULT),\n        false,\n        FS_TRASH_INTERVAL_DEFAULT,\n        DataChecksum.Type.CRC32,\n        \"\");\n  }\n\n  /**\n   * Return a set of server default configuration values.\n   * @param p path is used to identify an FS since an FS could have\n   *          another FS that it could be delegating the call to\n   * @return server default configuration values\n   * @throws IOException IO failure\n   */\n  public FsServerDefaults getServerDefaults(Path p) throws IOException {\n    return getServerDefaults();\n  }\n\n  /**\n   * Return the fully-qualified path of path, resolving the path\n   * through any symlinks or mount point.\n   * @param p path to be resolved\n   * @return fully qualified path\n   * @throws FileNotFoundException if the path is not present\n   * @throws IOException for any other error\n   */\n   public Path resolvePath(final Path p) throws IOException {\n     checkPath(p);\n     return getFileStatus(p).getPath();\n   }\n\n  /**\n   * Opens an FSDataInputStream at the indicated Path.\n   * @param f the file name to open\n   * @param bufferSize the size of the buffer to be used.\n   * @throws IOException IO failure\n   * @return input stream.\n   */\n  public abstract FSDataInputStream open(Path f, int bufferSize)\n    throws IOException;\n\n  /**\n   * Opens an FSDataInputStream at the indicated Path.\n   * @param f the file to open\n   * @throws IOException IO failure\n   * @return input stream.\n   */\n  public FSDataInputStream open(Path f) throws IOException {\n    return open(f, getConf().getInt(IO_FILE_BUFFER_SIZE_KEY,\n        IO_FILE_BUFFER_SIZE_DEFAULT));\n  }\n\n  /**\n   * Open an FSDataInputStream matching the PathHandle instance. The\n   * implementation may encode metadata in PathHandle to address the\n   * resource directly and verify that the resource referenced\n   * satisfies constraints specified at its construciton.\n   * @param fd PathHandle object returned by the FS authority.\n   * @throws InvalidPathHandleException If {@link PathHandle} constraints are\n   *                                    not satisfied\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException If {@link #open(PathHandle, int)}\n   *                                       not overridden by subclass\n   * @return input stream.\n   */\n  public FSDataInputStream open(PathHandle fd) throws IOException {\n    return open(fd, getConf().getInt(IO_FILE_BUFFER_SIZE_KEY,\n        IO_FILE_BUFFER_SIZE_DEFAULT));\n  }\n\n  /**\n   * Open an FSDataInputStream matching the PathHandle instance. The\n   * implementation may encode metadata in PathHandle to address the\n   * resource directly and verify that the resource referenced\n   * satisfies constraints specified at its construciton.\n   * @param fd PathHandle object returned by the FS authority.\n   * @param bufferSize the size of the buffer to use\n   * @throws InvalidPathHandleException If {@link PathHandle} constraints are\n   *                                    not satisfied\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException If not overridden by subclass\n   * @return input stream.\n   */\n  public FSDataInputStream open(PathHandle fd, int bufferSize)\n      throws IOException {\n    throw new UnsupportedOperationException();\n  }\n\n  /**\n   * Create a durable, serializable handle to the referent of the given\n   * entity.\n   * @param stat Referent in the target FileSystem\n   * @param opt If absent, assume {@link HandleOpt#path()}.\n   * @throws IllegalArgumentException If the FileStatus does not belong to\n   *         this FileSystem\n   * @throws UnsupportedOperationException If {@link #createPathHandle}\n   *         not overridden by subclass.\n   * @throws UnsupportedOperationException If this FileSystem cannot enforce\n   *         the specified constraints.\n   * @return path handle.\n   */\n  public final PathHandle getPathHandle(FileStatus stat, HandleOpt... opt) {\n    // method is final with a default so clients calling getPathHandle(stat)\n    // get the same semantics for all FileSystem implementations\n    if (null == opt || 0 == opt.length) {\n      return createPathHandle(stat, HandleOpt.path());\n    }\n    return createPathHandle(stat, opt);\n  }\n\n  /**\n   * Hook to implement support for {@link PathHandle} operations.\n   * @param stat Referent in the target FileSystem\n   * @param opt Constraints that determine the validity of the\n   *            {@link PathHandle} reference.\n   * @return path handle.\n   */\n  protected PathHandle createPathHandle(FileStatus stat, HandleOpt... opt) {\n    throw new UnsupportedOperationException();\n  }\n\n  /**\n   * Create an FSDataOutputStream at the indicated Path.\n   * Files are overwritten by default.\n   * @param f the file to create\n   * @throws IOException IO failure\n   * @return output stream.\n   */\n  public FSDataOutputStream create(Path f) throws IOException {\n    return create(f, true);\n  }\n\n  /**\n   * Create an FSDataOutputStream at the indicated Path.\n   * @param f the file to create\n   * @param overwrite if a file with this name already exists, then if true,\n   *   the file will be overwritten, and if false an exception will be thrown.\n   * @throws IOException IO failure\n   * @return output stream.\n   */\n  public FSDataOutputStream create(Path f, boolean overwrite)\n      throws IOException {\n    return create(f, overwrite,\n                  getConf().getInt(IO_FILE_BUFFER_SIZE_KEY,\n                      IO_FILE_BUFFER_SIZE_DEFAULT),\n                  getDefaultReplication(f),\n                  getDefaultBlockSize(f));\n  }\n\n  /**\n   * Create an FSDataOutputStream at the indicated Path with write-progress\n   * reporting.\n   * Files are overwritten by default.\n   * @param f the file to create\n   * @param progress to report progress\n   * @throws IOException IO failure\n   * @return output stream.\n   */\n  public FSDataOutputStream create(Path f, Progressable progress)\n      throws IOException {\n    return create(f, true,\n                  getConf().getInt(IO_FILE_BUFFER_SIZE_KEY,\n                      IO_FILE_BUFFER_SIZE_DEFAULT),\n                  getDefaultReplication(f),\n                  getDefaultBlockSize(f), progress);\n  }\n\n  /**\n   * Create an FSDataOutputStream at the indicated Path.\n   * Files are overwritten by default.\n   * @param f the file to create\n   * @param replication the replication factor\n   * @throws IOException IO failure\n   * @return output stream1\n   */\n  public FSDataOutputStream create(Path f, short replication)\n      throws IOException {\n    return create(f, true,\n                  getConf().getInt(IO_FILE_BUFFER_SIZE_KEY,\n                      IO_FILE_BUFFER_SIZE_DEFAULT),\n                  replication,\n                  getDefaultBlockSize(f));\n  }\n\n  /**\n   * Create an FSDataOutputStream at the indicated Path with write-progress\n   * reporting.\n   * Files are overwritten by default.\n   * @param f the file to create\n   * @param replication the replication factor\n   * @param progress to report progress\n   * @throws IOException IO failure\n   * @return output stream.\n   */\n  public FSDataOutputStream create(Path f, short replication,\n      Progressable progress) throws IOException {\n    return create(f, true,\n                  getConf().getInt(IO_FILE_BUFFER_SIZE_KEY,\n                      IO_FILE_BUFFER_SIZE_DEFAULT),\n                  replication, getDefaultBlockSize(f), progress);\n  }\n\n\n  /**\n   * Create an FSDataOutputStream at the indicated Path.\n   * @param f the file to create\n   * @param overwrite if a path with this name already exists, then if true,\n   *   the file will be overwritten, and if false an error will be thrown.\n   * @param bufferSize the size of the buffer to be used.\n   * @throws IOException IO failure\n   * @return output stream.\n   */\n  public FSDataOutputStream create(Path f,\n                                   boolean overwrite,\n                                   int bufferSize\n                                   ) throws IOException {\n    return create(f, overwrite, bufferSize,\n                  getDefaultReplication(f),\n                  getDefaultBlockSize(f));\n  }\n\n  /**\n   * Create an {@link FSDataOutputStream} at the indicated Path\n   * with write-progress reporting.\n   *\n   * The frequency of callbacks is implementation-specific; it may be \"none\".\n   * @param f the path of the file to open\n   * @param overwrite if a file with this name already exists, then if true,\n   *   the file will be overwritten, and if false an error will be thrown.\n   * @param bufferSize the size of the buffer to be used.\n   * @param progress to report progress.\n   * @throws IOException IO failure\n   * @return output stream.\n   */\n  public FSDataOutputStream create(Path f,\n                                   boolean overwrite,\n                                   int bufferSize,\n                                   Progressable progress\n                                   ) throws IOException {\n    return create(f, overwrite, bufferSize,\n                  getDefaultReplication(f),\n                  getDefaultBlockSize(f), progress);\n  }\n\n\n  /**\n   * Create an FSDataOutputStream at the indicated Path.\n   * @param f the file name to open\n   * @param overwrite if a file with this name already exists, then if true,\n   *   the file will be overwritten, and if false an error will be thrown.\n   * @param bufferSize the size of the buffer to be used.\n   * @param replication required block replication for the file.\n   * @param blockSize the size of the buffer to be used.\n   * @throws IOException IO failure\n   * @return output stream.\n   */\n  public FSDataOutputStream create(Path f,\n      boolean overwrite,\n      int bufferSize,\n      short replication,\n      long blockSize) throws IOException {\n    return create(f, overwrite, bufferSize, replication, blockSize, null);\n  }\n\n  /**\n   * Create an FSDataOutputStream at the indicated Path with write-progress\n   * reporting.\n   * @param f the file name to open\n   * @param overwrite if a file with this name already exists, then if true,\n   *   the file will be overwritten, and if false an error will be thrown.\n   * @param bufferSize the size of the buffer to be used.\n   * @param replication required block replication for the file.\n   * @param blockSize the size of the buffer to be used.\n   * @param progress to report progress.\n   * @throws IOException IO failure\n   * @return output stream.\n   */\n  public FSDataOutputStream create(Path f,\n                                            boolean overwrite,\n                                            int bufferSize,\n                                            short replication,\n                                            long blockSize,\n                                            Progressable progress\n                                            ) throws IOException {\n    return this.create(f, FsCreateModes.applyUMask(\n        FsPermission.getFileDefault(), FsPermission.getUMask(getConf())),\n        overwrite, bufferSize, replication, blockSize, progress);\n  }\n\n  /**\n   * Create an FSDataOutputStream at the indicated Path with write-progress\n   * reporting.\n   * @param f the file name to open\n   * @param permission file permission\n   * @param overwrite if a file with this name already exists, then if true,\n   *   the file will be overwritten, and if false an error will be thrown.\n   * @param bufferSize the size of the buffer to be used.\n   * @param replication required block replication for the file.\n   * @param blockSize block size\n   * @param progress the progress reporter\n   * @throws IOException IO failure\n   * @see #setPermission(Path, FsPermission)\n   * @return output stream.\n   */\n  public abstract FSDataOutputStream create(Path f,\n      FsPermission permission,\n      boolean overwrite,\n      int bufferSize,\n      short replication,\n      long blockSize,\n      Progressable progress) throws IOException;\n\n  /**\n   * Create an FSDataOutputStream at the indicated Path with write-progress\n   * reporting.\n   * @param f the file name to open\n   * @param permission file permission\n   * @param flags {@link CreateFlag}s to use for this stream.\n   * @param bufferSize the size of the buffer to be used.\n   * @param replication required block replication for the file.\n   * @param blockSize block size\n   * @param progress the progress reporter\n   * @throws IOException IO failure\n   * @see #setPermission(Path, FsPermission)\n   * @return output stream.\n   */\n  public FSDataOutputStream create(Path f,\n      FsPermission permission,\n      EnumSet<CreateFlag> flags,\n      int bufferSize,\n      short replication,\n      long blockSize,\n      Progressable progress) throws IOException {\n    return create(f, permission, flags, bufferSize, replication,\n        blockSize, progress, null);\n  }\n\n  /**\n   * Create an FSDataOutputStream at the indicated Path with a custom\n   * checksum option.\n   * @param f the file name to open\n   * @param permission file permission\n   * @param flags {@link CreateFlag}s to use for this stream.\n   * @param bufferSize the size of the buffer to be used.\n   * @param replication required block replication for the file.\n   * @param blockSize block size\n   * @param progress the progress reporter\n   * @param checksumOpt checksum parameter. If null, the values\n   *        found in conf will be used.\n   * @throws IOException IO failure\n   * @see #setPermission(Path, FsPermission)\n   * @return output stream.\n   */\n  public FSDataOutputStream create(Path f,\n      FsPermission permission,\n      EnumSet<CreateFlag> flags,\n      int bufferSize,\n      short replication,\n      long blockSize,\n      Progressable progress,\n      ChecksumOpt checksumOpt) throws IOException {\n    // Checksum options are ignored by default. The file systems that\n    // implement checksum need to override this method. The full\n    // support is currently only available in DFS.\n    return create(f, permission, flags.contains(CreateFlag.OVERWRITE),\n        bufferSize, replication, blockSize, progress);\n  }\n\n  /**\n   * This create has been added to support the FileContext that processes\n   * the permission with umask before calling this method.\n   * This a temporary method added to support the transition from FileSystem\n   * to FileContext for user applications.\n   *\n   * @param f path.\n   * @param absolutePermission permission.\n   * @param flag create flag.\n   * @param bufferSize buffer size.\n   * @param replication replication.\n   * @param blockSize block size.\n   * @param progress progress.\n   * @param checksumOpt check sum opt.\n   * @return output stream.\n   * @throws IOException IO failure\n   */\n  @Deprecated\n  protected FSDataOutputStream primitiveCreate(Path f,\n      FsPermission absolutePermission,\n      EnumSet<CreateFlag> flag,\n      int bufferSize,\n      short replication,\n      long blockSize,\n      Progressable progress,\n      ChecksumOpt checksumOpt) throws IOException {\n\n    boolean pathExists = exists(f);\n    CreateFlag.validate(f, pathExists, flag);\n\n    // Default impl  assumes that permissions do not matter and\n    // nor does the bytesPerChecksum  hence\n    // calling the regular create is good enough.\n    // FSs that implement permissions should override this.\n\n    if (pathExists && flag.contains(CreateFlag.APPEND)) {\n      return append(f, bufferSize, progress);\n    }\n\n    return this.create(f, absolutePermission,\n        flag.contains(CreateFlag.OVERWRITE), bufferSize, replication,\n        blockSize, progress);\n  }\n\n  /**\n   * This version of the mkdirs method assumes that the permission is absolute.\n   * It has been added to support the FileContext that processes the permission\n   * with umask before calling this method.\n   * This a temporary method added to support the transition from FileSystem\n   * to FileContext for user applications.\n   * @param f path\n   * @param absolutePermission permissions\n   * @return true if the directory was actually created.\n   * @throws IOException IO failure\n   * @see #mkdirs(Path, FsPermission)\n   */\n  @Deprecated\n  protected boolean primitiveMkdir(Path f, FsPermission absolutePermission)\n    throws IOException {\n   return this.mkdirs(f, absolutePermission);\n  }\n\n\n  /**\n   * This version of the mkdirs method assumes that the permission is absolute.\n   * It has been added to support the FileContext that processes the permission\n   * with umask before calling this method.\n   * This a temporary method added to support the transition from FileSystem\n   * to FileContext for user applications.\n   *\n   * @param f the path.\n   * @param absolutePermission permission.\n   * @param createParent create parent.\n   * @throws IOException IO failure.\n   */\n  @Deprecated\n  protected void primitiveMkdir(Path f, FsPermission absolutePermission,\n                    boolean createParent)\n    throws IOException {\n\n    if (!createParent) { // parent must exist.\n      // since the this.mkdirs makes parent dirs automatically\n      // we must throw exception if parent does not exist.\n      final FileStatus stat = getFileStatus(f.getParent());\n      if (stat == null) {\n        throw new FileNotFoundException(\"Missing parent:\" + f);\n      }\n      if (!stat.isDirectory()) {\n        throw new ParentNotDirectoryException(\"parent is not a dir\");\n      }\n      // parent does exist - go ahead with mkdir of leaf\n    }\n    // Default impl is to assume that permissions do not matter and hence\n    // calling the regular mkdirs is good enough.\n    // FSs that implement permissions should override this.\n    if (!this.mkdirs(f, absolutePermission)) {\n      throw new IOException(\"mkdir of \"+ f + \" failed\");\n    }\n  }\n\n  /**\n   * Opens an FSDataOutputStream at the indicated Path with write-progress\n   * reporting. Same as create(), except fails if parent directory doesn't\n   * already exist.\n   * @param f the file name to open\n   * @param overwrite if a file with this name already exists, then if true,\n   * the file will be overwritten, and if false an error will be thrown.\n   * @param bufferSize the size of the buffer to be used.\n   * @param replication required block replication for the file.\n   * @param blockSize block size\n   * @param progress the progress reporter\n   * @throws IOException IO failure\n   * @see #setPermission(Path, FsPermission)\n   * @return output stream.\n   */\n  public FSDataOutputStream createNonRecursive(Path f,\n      boolean overwrite,\n      int bufferSize, short replication, long blockSize,\n      Progressable progress) throws IOException {\n    return this.createNonRecursive(f, FsPermission.getFileDefault(),\n        overwrite, bufferSize, replication, blockSize, progress);\n  }\n\n  /**\n   * Opens an FSDataOutputStream at the indicated Path with write-progress\n   * reporting. Same as create(), except fails if parent directory doesn't\n   * already exist.\n   * @param f the file name to open\n   * @param permission file permission\n   * @param overwrite if a file with this name already exists, then if true,\n   * the file will be overwritten, and if false an error will be thrown.\n   * @param bufferSize the size of the buffer to be used.\n   * @param replication required block replication for the file.\n   * @param blockSize block size\n   * @param progress the progress reporter\n   * @throws IOException IO failure\n   * @see #setPermission(Path, FsPermission)\n   * @return output stream.\n   */\n   public FSDataOutputStream createNonRecursive(Path f, FsPermission permission,\n       boolean overwrite, int bufferSize, short replication, long blockSize,\n       Progressable progress) throws IOException {\n     return createNonRecursive(f, permission,\n         overwrite ? EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE)\n             : EnumSet.of(CreateFlag.CREATE), bufferSize,\n             replication, blockSize, progress);\n   }\n\n   /**\n    * Opens an FSDataOutputStream at the indicated Path with write-progress\n    * reporting. Same as create(), except fails if parent directory doesn't\n    * already exist.\n    * @param f the file name to open\n    * @param permission file permission\n    * @param flags {@link CreateFlag}s to use for this stream.\n    * @param bufferSize the size of the buffer to be used.\n    * @param replication required block replication for the file.\n    * @param blockSize block size\n    * @param progress the progress reporter\n    * @throws IOException IO failure\n    * @see #setPermission(Path, FsPermission)\n    * @return output stream.\n    */\n    public FSDataOutputStream createNonRecursive(Path f, FsPermission permission,\n        EnumSet<CreateFlag> flags, int bufferSize, short replication, long blockSize,\n        Progressable progress) throws IOException {\n      throw new IOException(\"createNonRecursive unsupported for this filesystem \"\n          + this.getClass());\n    }\n\n  /**\n   * Creates the given Path as a brand-new zero-length file.  If\n   * create fails, or if it already existed, return false.\n   * <i>Important: the default implementation is not atomic</i>\n   * @param f path to use for create\n   * @throws IOException IO failure\n   * @return if create new file success true,not false.\n   */\n  public boolean createNewFile(Path f) throws IOException {\n    if (exists(f)) {\n      return false;\n    } else {\n      create(f, false, getConf().getInt(IO_FILE_BUFFER_SIZE_KEY,\n          IO_FILE_BUFFER_SIZE_DEFAULT)).close();\n      return true;\n    }\n  }\n\n  /**\n   * Append to an existing file (optional operation).\n   * Same as\n   * {@code append(f, getConf().getInt(IO_FILE_BUFFER_SIZE_KEY,\n   *     IO_FILE_BUFFER_SIZE_DEFAULT), null)}\n   * @param f the existing file to be appended.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default).\n   * @return output stream.\n   */\n  public FSDataOutputStream append(Path f) throws IOException {\n    return append(f, getConf().getInt(IO_FILE_BUFFER_SIZE_KEY,\n        IO_FILE_BUFFER_SIZE_DEFAULT), null);\n  }\n\n  /**\n   * Append to an existing file (optional operation).\n   * Same as append(f, bufferSize, null).\n   * @param f the existing file to be appended.\n   * @param bufferSize the size of the buffer to be used.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default).\n   * @return output stream.\n   */\n  public FSDataOutputStream append(Path f, int bufferSize) throws IOException {\n    return append(f, bufferSize, null);\n  }\n\n  /**\n   * Append to an existing file (optional operation).\n   * @param f the existing file to be appended.\n   * @param bufferSize the size of the buffer to be used.\n   * @param progress for reporting progress if it is not null.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default).\n   * @return output stream.\n   */\n  public abstract FSDataOutputStream append(Path f, int bufferSize,\n      Progressable progress) throws IOException;\n\n  /**\n   * Append to an existing file (optional operation).\n   * @param f the existing file to be appended.\n   * @param appendToNewBlock whether to append data to a new block\n   * instead of the end of the last partial block\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default).\n   * @return output stream.\n   */\n  public FSDataOutputStream append(Path f, boolean appendToNewBlock) throws IOException {\n    return append(f, getConf().getInt(IO_FILE_BUFFER_SIZE_KEY,\n        IO_FILE_BUFFER_SIZE_DEFAULT), null, appendToNewBlock);\n  }\n\n  /**\n   * Append to an existing file (optional operation).\n   * This function is used for being overridden by some FileSystem like DistributedFileSystem\n   * @param f the existing file to be appended.\n   * @param bufferSize the size of the buffer to be used.\n   * @param progress for reporting progress if it is not null.\n   * @param appendToNewBlock whether to append data to a new block\n   * instead of the end of the last partial block\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default).\n   * @return output stream.\n   */\n  public FSDataOutputStream append(Path f, int bufferSize,\n      Progressable progress, boolean appendToNewBlock) throws IOException {\n    return append(f, bufferSize, progress);\n  }\n\n  /**\n   * Concat existing files together.\n   * @param trg the path to the target destination.\n   * @param psrcs the paths to the sources to use for the concatenation.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default).\n   */\n  public void concat(final Path trg, final Path [] psrcs) throws IOException {\n    throw new UnsupportedOperationException(\"Not implemented by the \" +\n        getClass().getSimpleName() + \" FileSystem implementation\");\n  }\n\n /**\n   * Get the replication factor.\n   *\n   * @deprecated Use {@link #getFileStatus(Path)} instead\n   * @param src file name\n   * @return file replication\n   * @throws FileNotFoundException if the path does not resolve.\n   * @throws IOException an IO failure\n   */\n  @Deprecated\n  public short getReplication(Path src) throws IOException {\n    return getFileStatus(src).getReplication();\n  }\n\n  /**\n   * Set the replication for an existing file.\n   * If a filesystem does not support replication, it will always\n   * return true: the check for a file existing may be bypassed.\n   * This is the default behavior.\n   * @param src file name\n   * @param replication new replication\n   * @throws IOException an IO failure.\n   * @return true if successful, or the feature in unsupported;\n   *         false if replication is supported but the file does not exist,\n   *         or is a directory\n   */\n  public boolean setReplication(Path src, short replication)\n    throws IOException {\n    return true;\n  }\n\n  /**\n   * Renames Path src to Path dst.\n   * @param src path to be renamed\n   * @param dst new path after rename\n   * @throws IOException on failure\n   * @return true if rename is successful\n   */\n  public abstract boolean rename(Path src, Path dst) throws IOException;\n\n  /**\n   * Renames Path src to Path dst\n   * <ul>\n   *   <li>Fails if src is a file and dst is a directory.</li>\n   *   <li>Fails if src is a directory and dst is a file.</li>\n   *   <li>Fails if the parent of dst does not exist or is a file.</li>\n   * </ul>\n   * <p>\n   * If OVERWRITE option is not passed as an argument, rename fails\n   * if the dst already exists.\n   * </p>\n   * <p>\n   * If OVERWRITE option is passed as an argument, rename overwrites\n   * the dst if it is a file or an empty directory. Rename fails if dst is\n   * a non-empty directory.\n   * </p>\n   * Note that atomicity of rename is dependent on the file system\n   * implementation. Please refer to the file system documentation for\n   * details. This default implementation is non atomic.\n   * <p>\n   * This method is deprecated since it is a temporary method added to\n   * support the transition from FileSystem to FileContext for user\n   * applications.\n   * </p>\n   *\n   * @param src path to be renamed\n   * @param dst new path after rename\n   * @param options rename options.\n   * @throws FileNotFoundException src path does not exist, or the parent\n   * path of dst does not exist.\n   * @throws FileAlreadyExistsException dest path exists and is a file\n   * @throws ParentNotDirectoryException if the parent path of dest is not\n   * a directory\n   * @throws IOException on failure\n   */\n  @Deprecated\n  protected void rename(final Path src, final Path dst,\n      final Rename... options) throws IOException {\n    // Default implementation\n    final FileStatus srcStatus = getFileLinkStatus(src);\n    if (srcStatus == null) {\n      throw new FileNotFoundException(\"rename source \" + src + \" not found.\");\n    }\n\n    boolean overwrite = false;\n    if (null != options) {\n      for (Rename option : options) {\n        if (option == Rename.OVERWRITE) {\n          overwrite = true;\n        }\n      }\n    }\n\n    FileStatus dstStatus;\n    try {\n      dstStatus = getFileLinkStatus(dst);\n    } catch (IOException e) {\n      dstStatus = null;\n    }\n    if (dstStatus != null) {\n      if (srcStatus.isDirectory() != dstStatus.isDirectory()) {\n        throw new IOException(\"Source \" + src + \" Destination \" + dst\n            + \" both should be either file or directory\");\n      }\n      if (!overwrite) {\n        throw new FileAlreadyExistsException(\"rename destination \" + dst\n            + \" already exists.\");\n      }\n      // Delete the destination that is a file or an empty directory\n      if (dstStatus.isDirectory()) {\n        FileStatus[] list = listStatus(dst);\n        if (list != null && list.length != 0) {\n          throw new IOException(\n              \"rename cannot overwrite non empty destination directory \" + dst);\n        }\n      }\n      delete(dst, false);\n    } else {\n      final Path parent = dst.getParent();\n      final FileStatus parentStatus = getFileStatus(parent);\n      if (parentStatus == null) {\n        throw new FileNotFoundException(\"rename destination parent \" + parent\n            + \" not found.\");\n      }\n      if (!parentStatus.isDirectory()) {\n        throw new ParentNotDirectoryException(\"rename destination parent \" + parent\n            + \" is a file.\");\n      }\n    }\n    if (!rename(src, dst)) {\n      throw new IOException(\"rename from \" + src + \" to \" + dst + \" failed.\");\n    }\n  }\n\n  /**\n   * Truncate the file in the indicated path to the indicated size.\n   * <ul>\n   *   <li>Fails if path is a directory.</li>\n   *   <li>Fails if path does not exist.</li>\n   *   <li>Fails if path is not closed.</li>\n   *   <li>Fails if new size is greater than current size.</li>\n   * </ul>\n   * @param f The path to the file to be truncated\n   * @param newLength The size the file is to be truncated to\n   *\n   * @return <code>true</code> if the file has been truncated to the desired\n   * <code>newLength</code> and is immediately available to be reused for\n   * write operations such as <code>append</code>, or\n   * <code>false</code> if a background process of adjusting the length of\n   * the last block has been started, and clients should wait for it to\n   * complete before proceeding with further file updates.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default).\n   */\n  public boolean truncate(Path f, long newLength) throws IOException {\n    throw new UnsupportedOperationException(\"Not implemented by the \" +\n        getClass().getSimpleName() + \" FileSystem implementation\");\n  }\n\n  /**\n   * Delete a file/directory.\n   * @param f the path.\n   * @throws IOException IO failure.\n   * @return if delete success true, not false.\n   * @deprecated Use {@link #delete(Path, boolean)} instead.\n   */\n  @Deprecated\n  public boolean delete(Path f) throws IOException {\n    return delete(f, true);\n  }\n\n  /** Delete a file.\n   *\n   * @param f the path to delete.\n   * @param recursive if path is a directory and set to\n   * true, the directory is deleted else throws an exception. In\n   * case of a file the recursive can be set to either true or false.\n   * @return  true if delete is successful else false.\n   * @throws IOException IO failure\n   */\n  public abstract boolean delete(Path f, boolean recursive) throws IOException;\n\n  /**\n   * Mark a path to be deleted when its FileSystem is closed.\n   * When the JVM shuts down cleanly, all cached FileSystem objects will be\n   * closed automatically. These the marked paths will be deleted as a result.\n   *\n   * If a FileSystem instance is not cached, i.e. has been created with\n   * {@link #createFileSystem(URI, Configuration)}, then the paths will\n   * be deleted in when {@link #close()} is called on that instance.\n   *\n   * The path must exist in the filesystem at the time of the method call;\n   * it does not have to exist at the time of JVM shutdown.\n   *\n   * Notes\n   * <ol>\n   *   <li>Clean shutdown of the JVM cannot be guaranteed.</li>\n   *   <li>The time to shut down a FileSystem will depends on the number of\n   *   files to delete. For filesystems where the cost of checking\n   *   for the existence of a file/directory and the actual delete operation\n   *   (for example: object stores) is high, the time to shutdown the JVM can be\n   *   significantly extended by over-use of this feature.</li>\n   *   <li>Connectivity problems with a remote filesystem may delay shutdown\n   *   further, and may cause the files to not be deleted.</li>\n   * </ol>\n   * @param f the path to delete.\n   * @return  true if deleteOnExit is successful, otherwise false.\n   * @throws IOException IO failure\n   */\n  public boolean deleteOnExit(Path f) throws IOException {\n    if (!exists(f)) {\n      return false;\n    }\n    synchronized (deleteOnExit) {\n      deleteOnExit.add(f);\n    }\n    return true;\n  }\n\n  /**\n   * Cancel the scheduled deletion of the path when the FileSystem is closed.\n   * @param f the path to cancel deletion\n   * @return true if the path was found in the delete-on-exit list.\n   */\n  public boolean cancelDeleteOnExit(Path f) {\n    synchronized (deleteOnExit) {\n      return deleteOnExit.remove(f);\n    }\n  }\n\n  /**\n   * Delete all paths that were marked as delete-on-exit. This recursively\n   * deletes all files and directories in the specified paths.\n   *\n   * The time to process this operation is {@code O(paths)}, with the actual\n   * time dependent on the time for existence and deletion operations to\n   * complete, successfully or not.\n   */\n  protected void processDeleteOnExit() {\n    synchronized (deleteOnExit) {\n      for (Iterator<Path> iter = deleteOnExit.iterator(); iter.hasNext();) {\n        Path path = iter.next();\n        try {\n          if (exists(path)) {\n            delete(path, true);\n          }\n        }\n        catch (IOException e) {\n          LOGGER.info(\"Ignoring failure to deleteOnExit for path {}\", path);\n        }\n        iter.remove();\n      }\n    }\n  }\n\n  /** Check if a path exists.\n   *\n   * It is highly discouraged to call this method back to back with other\n   * {@link #getFileStatus(Path)} calls, as this will involve multiple redundant\n   * RPC calls in HDFS.\n   *\n   * @param f source path\n   * @return true if the path exists\n   * @throws IOException IO failure\n   */\n  public boolean exists(Path f) throws IOException {\n    try {\n      return getFileStatus(f) != null;\n    } catch (FileNotFoundException e) {\n      return false;\n    }\n  }\n\n  /** True iff the named path is a directory.\n   * Note: Avoid using this method. Instead reuse the FileStatus\n   * returned by getFileStatus() or listStatus() methods.\n   *\n   * @param f path to check\n   * @throws IOException IO failure\n   * @deprecated Use {@link #getFileStatus(Path)} instead\n   * @return if f is directory true, not false.\n   */\n  @Deprecated\n  public boolean isDirectory(Path f) throws IOException {\n    try {\n      return getFileStatus(f).isDirectory();\n    } catch (FileNotFoundException e) {\n      return false;               // f does not exist\n    }\n  }\n\n  /** True iff the named path is a regular file.\n   * Note: Avoid using this method. Instead reuse the FileStatus\n   * returned by {@link #getFileStatus(Path)} or listStatus() methods.\n   *\n   * @param f path to check\n   * @throws IOException IO failure\n   * @deprecated Use {@link #getFileStatus(Path)} instead\n   * @return if f is file true, not false.\n   */\n  @Deprecated\n  public boolean isFile(Path f) throws IOException {\n    try {\n      return getFileStatus(f).isFile();\n    } catch (FileNotFoundException e) {\n      return false;               // f does not exist\n    }\n  }\n\n  /**\n   * The number of bytes in a file.\n   * @param f the path.\n   * @return the number of bytes; 0 for a directory\n   * @deprecated Use {@link #getFileStatus(Path)} instead.\n   * @throws FileNotFoundException if the path does not resolve\n   * @throws IOException IO failure\n   */\n  @Deprecated\n  public long getLength(Path f) throws IOException {\n    return getFileStatus(f).getLen();\n  }\n\n  /** Return the {@link ContentSummary} of a given {@link Path}.\n   * @param f path to use\n   * @throws FileNotFoundException if the path does not resolve\n   * @throws IOException IO failure\n   * @return content summary.\n   */\n  public ContentSummary getContentSummary(Path f) throws IOException {\n    FileStatus status = getFileStatus(f);\n    if (status.isFile()) {\n      // f is a file\n      long length = status.getLen();\n      return new ContentSummary.Builder().length(length).\n          fileCount(1).directoryCount(0).spaceConsumed(length).build();\n    }\n    // f is a directory\n    long[] summary = {0, 0, 1};\n    for(FileStatus s : listStatus(f)) {\n      long length = s.getLen();\n      ContentSummary c = s.isDirectory() ? getContentSummary(s.getPath()) :\n          new ContentSummary.Builder().length(length).\n          fileCount(1).directoryCount(0).spaceConsumed(length).build();\n      summary[0] += c.getLength();\n      summary[1] += c.getFileCount();\n      summary[2] += c.getDirectoryCount();\n    }\n    return new ContentSummary.Builder().length(summary[0]).\n        fileCount(summary[1]).directoryCount(summary[2]).\n        spaceConsumed(summary[0]).build();\n  }\n\n  /** Return the {@link QuotaUsage} of a given {@link Path}.\n   * @param f path to use\n   * @return the quota usage\n   * @throws IOException IO failure\n   */\n  public QuotaUsage getQuotaUsage(Path f) throws IOException {\n    return getContentSummary(f);\n  }\n\n  /**\n   * Set quota for the given {@link Path}.\n   *\n   * @param src the target path to set quota for\n   * @param namespaceQuota the namespace quota (i.e., # of files/directories)\n   *                       to set\n   * @param storagespaceQuota the storage space quota to set\n   * @throws IOException IO failure\n   */\n  public void setQuota(Path src, final long namespaceQuota,\n      final long storagespaceQuota) throws IOException {\n    methodNotSupported();\n  }\n\n  /**\n   * Set per storage type quota for the given {@link Path}.\n   *\n   * @param src the target path to set storage type quota for\n   * @param type the storage type to set\n   * @param quota the quota to set for the given storage type\n   * @throws IOException IO failure\n   */\n  public void setQuotaByStorageType(Path src, final StorageType type,\n      final long quota) throws IOException {\n    methodNotSupported();\n  }\n\n  /**\n   * The default filter accepts all paths.\n   */\n  private static final PathFilter DEFAULT_FILTER = new PathFilter() {\n      @Override\n      public boolean accept(Path file) {\n        return true;\n      }\n    };\n\n  /**\n   * List the statuses of the files/directories in the given path if the path is\n   * a directory.\n   * <p>\n   * Does not guarantee to return the List of files/directories status in a\n   * sorted order.\n   * <p>\n   * Will not return null. Expect IOException upon access error.\n   * @param f given path\n   * @return the statuses of the files/directories in the given patch\n   * @throws FileNotFoundException when the path does not exist\n   * @throws IOException see specific implementation\n   */\n  public abstract FileStatus[] listStatus(Path f) throws FileNotFoundException,\n                                                         IOException;\n\n  /**\n   * Represents a batch of directory entries when iteratively listing a\n   * directory. This is a private API not meant for use by end users.\n   * <p>\n   * For internal use by FileSystem subclasses that override\n   * {@link FileSystem#listStatusBatch(Path, byte[])} to implement iterative\n   * listing.\n   */\n  @InterfaceAudience.Private\n  public static class DirectoryEntries {\n    private final FileStatus[] entries;\n    private final byte[] token;\n    private final boolean hasMore;\n\n    public DirectoryEntries(FileStatus[] entries, byte[] token, boolean\n        hasMore) {\n      this.entries = entries;\n      if (token != null) {\n        this.token = token.clone();\n      } else {\n        this.token = null;\n      }\n      this.hasMore = hasMore;\n    }\n\n    public FileStatus[] getEntries() {\n      return entries;\n    }\n\n    public byte[] getToken() {\n      return token;\n    }\n\n    public boolean hasMore() {\n      return hasMore;\n    }\n  }\n\n  /**\n   * Given an opaque iteration token, return the next batch of entries in a\n   * directory. This is a private API not meant for use by end users.\n   * <p>\n   * This method should be overridden by FileSystem subclasses that want to\n   * use the generic {@link FileSystem#listStatusIterator(Path)} implementation.\n   * @param f Path to list\n   * @param token opaque iteration token returned by previous call, or null\n   *              if this is the first call.\n   * @return directory entries.\n   * @throws FileNotFoundException when the path does not exist.\n   * @throws IOException If an I/O error occurred.\n   */\n  @InterfaceAudience.Private\n  protected DirectoryEntries listStatusBatch(Path f, byte[] token) throws\n      FileNotFoundException, IOException {\n    // The default implementation returns the entire listing as a single batch.\n    // Thus, there is never a second batch, and no need to respect the passed\n    // token or set a token in the returned DirectoryEntries.\n    FileStatus[] listing = listStatus(f);\n    return new DirectoryEntries(listing, null, false);\n  }\n\n  /**\n   * Filter files/directories in the given path using the user-supplied path\n   * filter. Results are added to the given array <code>results</code>.\n   * @throws FileNotFoundException when the path does not exist\n   * @throws IOException see specific implementation\n   */\n  private void listStatus(ArrayList<FileStatus> results, Path f,\n      PathFilter filter) throws FileNotFoundException, IOException {\n    FileStatus listing[] = listStatus(f);\n    Preconditions.checkNotNull(listing, \"listStatus should not return NULL\");\n    for (int i = 0; i < listing.length; i++) {\n      if (filter.accept(listing[i].getPath())) {\n        results.add(listing[i]);\n      }\n    }\n  }\n\n  /**\n   * List corrupted file blocks.\n   *\n   * @param path the path.\n   * @return an iterator over the corrupt files under the given path\n   * (may contain duplicates if a file has more than one corrupt block)\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default).\n   * @throws IOException IO failure\n   */\n  public RemoteIterator<Path> listCorruptFileBlocks(Path path)\n    throws IOException {\n    throw new UnsupportedOperationException(getClass().getCanonicalName() +\n        \" does not support listCorruptFileBlocks\");\n  }\n\n  /**\n   * Filter files/directories in the given path using the user-supplied path\n   * filter.\n   * <p>\n   * Does not guarantee to return the List of files/directories status in a\n   * sorted order.\n   *\n   * @param f\n   *          a path name\n   * @param filter\n   *          the user-supplied path filter\n   * @return an array of FileStatus objects for the files under the given path\n   *         after applying the filter\n   * @throws FileNotFoundException when the path does not exist\n   * @throws IOException see specific implementation\n   */\n  public FileStatus[] listStatus(Path f, PathFilter filter)\n                                   throws FileNotFoundException, IOException {\n    ArrayList<FileStatus> results = new ArrayList<>();\n    listStatus(results, f, filter);\n    return results.toArray(new FileStatus[results.size()]);\n  }\n\n  /**\n   * Filter files/directories in the given list of paths using default\n   * path filter.\n   * <p>\n   * Does not guarantee to return the List of files/directories status in a\n   * sorted order.\n   *\n   * @param files\n   *          a list of paths\n   * @return a list of statuses for the files under the given paths after\n   *         applying the filter default Path filter\n   * @throws FileNotFoundException when the path does not exist\n   * @throws IOException see specific implementation\n   */\n  public FileStatus[] listStatus(Path[] files)\n      throws FileNotFoundException, IOException {\n    return listStatus(files, DEFAULT_FILTER);\n  }\n\n  /**\n   * Filter files/directories in the given list of paths using user-supplied\n   * path filter.\n   * <p>\n   * Does not guarantee to return the List of files/directories status in a\n   * sorted order.\n   *\n   * @param files\n   *          a list of paths\n   * @param filter\n   *          the user-supplied path filter\n   * @return a list of statuses for the files under the given paths after\n   *         applying the filter\n   * @throws FileNotFoundException when the path does not exist\n   * @throws IOException see specific implementation\n   */\n  public FileStatus[] listStatus(Path[] files, PathFilter filter)\n      throws FileNotFoundException, IOException {\n    ArrayList<FileStatus> results = new ArrayList<FileStatus>();\n    for (int i = 0; i < files.length; i++) {\n      listStatus(results, files[i], filter);\n    }\n    return results.toArray(new FileStatus[results.size()]);\n  }\n\n  /**\n   * <p>Return all the files that match filePattern and are not checksum\n   * files. Results are sorted by their names.\n   *\n   * <p>\n   * A filename pattern is composed of <i>regular</i> characters and\n   * <i>special pattern matching</i> characters, which are:\n   *\n   * <dl>\n   *  <dd>\n   *   <dl>\n   *    <dt> <tt> ? </tt>\n   *    <dd> Matches any single character.\n   *\n   *    <dt> <tt> * </tt>\n   *    <dd> Matches zero or more characters.\n   *\n   *    <dt> <tt> [<i>abc</i>] </tt>\n   *    <dd> Matches a single character from character set\n   *     <tt>{<i>a,b,c</i>}</tt>.\n   *\n   *    <dt> <tt> [<i>a</i>-<i>b</i>] </tt>\n   *    <dd> Matches a single character from the character range\n   *     <tt>{<i>a...b</i>}</tt>.  Note that character <tt><i>a</i></tt> must be\n   *     lexicographically less than or equal to character <tt><i>b</i></tt>.\n   *\n   *    <dt> <tt> [^<i>a</i>] </tt>\n   *    <dd> Matches a single character that is not from character set or range\n   *     <tt>{<i>a</i>}</tt>.  Note that the <tt>^</tt> character must occur\n   *     immediately to the right of the opening bracket.\n   *\n   *    <dt> <tt> \\<i>c</i> </tt>\n   *    <dd> Removes (escapes) any special meaning of character <i>c</i>.\n   *\n   *    <dt> <tt> {ab,cd} </tt>\n   *    <dd> Matches a string from the string set <tt>{<i>ab, cd</i>} </tt>\n   *\n   *    <dt> <tt> {ab,c{de,fh}} </tt>\n   *    <dd> Matches a string from the string set <tt>{<i>ab, cde, cfh</i>}</tt>\n   *\n   *   </dl>\n   *  </dd>\n   * </dl>\n   *\n   * @param pathPattern a glob specifying a path pattern\n\n   * @return an array of paths that match the path pattern\n   * @throws IOException IO failure\n   */\n  public FileStatus[] globStatus(Path pathPattern) throws IOException {\n    return Globber.createGlobber(this)\n        .withPathPattern(pathPattern)\n        .withPathFiltern(DEFAULT_FILTER)\n        .withResolveSymlinks(true)\n        .build()\n        .glob();\n  }\n\n  /**\n   * Return an array of {@link FileStatus} objects whose path names match\n   * {@code pathPattern} and is accepted by the user-supplied path filter.\n   * Results are sorted by their path names.\n   *\n   * @param pathPattern a glob specifying the path pattern\n   * @param filter a user-supplied path filter\n   * @return null if {@code pathPattern} has no glob and the path does not exist\n   *         an empty array if {@code pathPattern} has a glob and no path\n   *         matches it else an array of {@link FileStatus} objects matching the\n   *         pattern\n   * @throws IOException if any I/O error occurs when fetching file status\n   */\n  public FileStatus[] globStatus(Path pathPattern, PathFilter filter)\n      throws IOException {\n    return new Globber(this, pathPattern, filter).glob();\n  }\n\n  /**\n   * List the statuses of the files/directories in the given path if the path is\n   * a directory.\n   * Return the file's status and block locations If the path is a file.\n   *\n   * If a returned status is a file, it contains the file's block locations.\n   *\n   * @param f is the path\n   *\n   * @return an iterator that traverses statuses of the files/directories\n   *         in the given path\n   *\n   * @throws FileNotFoundException If <code>f</code> does not exist\n   * @throws IOException If an I/O error occurred\n   */\n  public RemoteIterator<LocatedFileStatus> listLocatedStatus(final Path f)\n  throws FileNotFoundException, IOException {\n    return listLocatedStatus(f, DEFAULT_FILTER);\n  }\n\n  /**\n   * List a directory.\n   * The returned results include its block location if it is a file\n   * The results are filtered by the given path filter\n   * @param f a path\n   * @param filter a path filter\n   * @return an iterator that traverses statuses of the files/directories\n   *         in the given path\n   * @throws FileNotFoundException if <code>f</code> does not exist\n   * @throws IOException if any I/O error occurred\n   */\n  protected RemoteIterator<LocatedFileStatus> listLocatedStatus(final Path f,\n      final PathFilter filter)\n  throws FileNotFoundException, IOException {\n    return new RemoteIterator<LocatedFileStatus>() {\n      private final FileStatus[] stats = listStatus(f, filter);\n      private int i = 0;\n\n      @Override\n      public boolean hasNext() {\n        return i<stats.length;\n      }\n\n      @Override\n      public LocatedFileStatus next() throws IOException {\n        if (!hasNext()) {\n          throw new NoSuchElementException(\"No more entries in \" + f);\n        }\n        FileStatus result = stats[i++];\n        // for files, use getBlockLocations(FileStatus, int, int) to avoid\n        // calling getFileStatus(Path) to load the FileStatus again\n        BlockLocation[] locs = result.isFile() ?\n            getFileBlockLocations(result, 0, result.getLen()) :\n            null;\n        return new LocatedFileStatus(result, locs);\n      }\n    };\n  }\n\n  /**\n   * Generic iterator for implementing {@link #listStatusIterator(Path)}.\n   */\n  protected class DirListingIterator<T extends FileStatus> implements\n      RemoteIterator<T> {\n\n    private final Path path;\n    private DirectoryEntries entries;\n    private int i = 0;\n\n    DirListingIterator(Path path) throws IOException {\n      this.path = path;\n      this.entries = listStatusBatch(path, null);\n    }\n\n    @Override\n    public boolean hasNext() throws IOException {\n      return i < entries.getEntries().length ||\n          entries.hasMore();\n    }\n\n    private void fetchMore() throws IOException {\n      byte[] token = entries.getToken();\n      entries = listStatusBatch(path, token);\n      i = 0;\n    }\n\n    @Override\n    @SuppressWarnings(\"unchecked\")\n    public T next() throws IOException {\n      if (!hasNext()) {\n        throw new NoSuchElementException(\"No more items in iterator\");\n      }\n      if (i == entries.getEntries().length) {\n        fetchMore();\n      }\n      return (T)entries.getEntries()[i++];\n    }\n  }\n\n  /**\n   * Returns a remote iterator so that followup calls are made on demand\n   * while consuming the entries. Each FileSystem implementation should\n   * override this method and provide a more efficient implementation, if\n   * possible.\n   *\n   * Does not guarantee to return the iterator that traverses statuses\n   * of the files in a sorted order.\n   *\n   * @param p target path\n   * @return remote iterator\n   * @throws FileNotFoundException if <code>p</code> does not exist\n   * @throws IOException if any I/O error occurred\n   */\n  public RemoteIterator<FileStatus> listStatusIterator(final Path p)\n  throws FileNotFoundException, IOException {\n    return new DirListingIterator<>(p);\n  }\n\n  /**\n   * List the statuses and block locations of the files in the given path.\n   * Does not guarantee to return the iterator that traverses statuses\n   * of the files in a sorted order.\n   *\n   * <pre>\n   * If the path is a directory,\n   *   if recursive is false, returns files in the directory;\n   *   if recursive is true, return files in the subtree rooted at the path.\n   * If the path is a file, return the file's status and block locations.\n   * </pre>\n   * @param f is the path\n   * @param recursive if the subdirectories need to be traversed recursively\n   *\n   * @return an iterator that traverses statuses of the files\n   *\n   * @throws FileNotFoundException when the path does not exist;\n   * @throws IOException see specific implementation\n   */\n  public RemoteIterator<LocatedFileStatus> listFiles(\n      final Path f, final boolean recursive)\n  throws FileNotFoundException, IOException {\n    return new RemoteIterator<LocatedFileStatus>() {\n      private Stack<RemoteIterator<LocatedFileStatus>> itors = new Stack<>();\n      private RemoteIterator<LocatedFileStatus> curItor =\n        listLocatedStatus(f);\n      private LocatedFileStatus curFile;\n\n      @Override\n      public boolean hasNext() throws IOException {\n        while (curFile == null) {\n          if (curItor.hasNext()) {\n            handleFileStat(curItor.next());\n          } else if (!itors.empty()) {\n            curItor = itors.pop();\n          } else {\n            return false;\n          }\n        }\n        return true;\n      }\n\n      /**\n       * Process the input stat.\n       * If it is a file, return the file stat.\n       * If it is a directory, traverse the directory if recursive is true;\n       * ignore it if recursive is false.\n       * @param stat input status\n       * @throws IOException if any IO error occurs\n       */\n      private void handleFileStat(LocatedFileStatus stat) throws IOException {\n        if (stat.isFile()) { // file\n          curFile = stat;\n        } else if (recursive) { // directory\n          try {\n            RemoteIterator<LocatedFileStatus> newDirItor = listLocatedStatus(stat.getPath());\n            itors.push(curItor);\n            curItor = newDirItor;\n          } catch (FileNotFoundException ignored) {\n            LOGGER.debug(\"Directory {} deleted while attempting for recursive listing\",\n                stat.getPath());\n          }\n        }\n      }\n\n      @Override\n      public LocatedFileStatus next() throws IOException {\n        if (hasNext()) {\n          LocatedFileStatus result = curFile;\n          curFile = null;\n          return result;\n        }\n        throw new java.util.NoSuchElementException(\"No more entry in \" + f);\n      }\n    };\n  }\n\n  /** Return the current user's home directory in this FileSystem.\n   * The default implementation returns {@code \"/user/$USER/\"}.\n   * @return the path.\n   */\n  public Path getHomeDirectory() {\n    String username;\n    try {\n      username = UserGroupInformation.getCurrentUser().getShortUserName();\n    } catch(IOException ex) {\n      LOGGER.warn(\"Unable to get user name. Fall back to system property \" +\n          \"user.name\", ex);\n      username = System.getProperty(\"user.name\");\n    }\n    return this.makeQualified(\n        new Path(USER_HOME_PREFIX + \"/\" + username));\n  }\n\n\n  /**\n   * Set the current working directory for the given FileSystem. All relative\n   * paths will be resolved relative to it.\n   *\n   * @param new_dir Path of new working directory\n   */\n  public abstract void setWorkingDirectory(Path new_dir);\n\n  /**\n   * Get the current working directory for the given FileSystem\n   * @return the directory pathname\n   */\n  public abstract Path getWorkingDirectory();\n\n  /**\n   * Note: with the new FileContext class, getWorkingDirectory()\n   * will be removed.\n   * The working directory is implemented in FileContext.\n   *\n   * Some FileSystems like LocalFileSystem have an initial workingDir\n   * that we use as the starting workingDir. For other file systems\n   * like HDFS there is no built in notion of an initial workingDir.\n   *\n   * @return if there is built in notion of workingDir then it\n   * is returned; else a null is returned.\n   */\n  protected Path getInitialWorkingDirectory() {\n    return null;\n  }\n\n  /**\n   * Call {@link #mkdirs(Path, FsPermission)} with default permission.\n   * @param f path\n   * @return true if the directory was created\n   * @throws IOException IO failure\n   */\n  public boolean mkdirs(Path f) throws IOException {\n    return mkdirs(f, FsPermission.getDirDefault());\n  }\n\n  /**\n   * Make the given file and all non-existent parents into\n   * directories. Has roughly the semantics of Unix @{code mkdir -p}.\n   * Existence of the directory hierarchy is not an error.\n   * @param f path to create\n   * @param permission to apply to f\n   * @throws IOException IO failure\n   * @return if mkdir success true, not false.\n   */\n  public abstract boolean mkdirs(Path f, FsPermission permission\n      ) throws IOException;\n\n  /**\n   * The src file is on the local disk.  Add it to filesystem at\n   * the given dst name and the source is kept intact afterwards\n   * @param src path\n   * @param dst path\n   * @throws IOException IO failure\n   */\n  public void copyFromLocalFile(Path src, Path dst)\n    throws IOException {\n    copyFromLocalFile(false, src, dst);\n  }\n\n  /**\n   * The src files is on the local disk.  Add it to filesystem at\n   * the given dst name, removing the source afterwards.\n   * @param srcs source paths\n   * @param dst path\n   * @throws IOException IO failure\n   */\n  public void moveFromLocalFile(Path[] srcs, Path dst)\n    throws IOException {\n    copyFromLocalFile(true, true, srcs, dst);\n  }\n\n  /**\n   * The src file is on the local disk.  Add it to the filesystem at\n   * the given dst name, removing the source afterwards.\n   * @param src local path\n   * @param dst path\n   * @throws IOException IO failure\n   */\n  public void moveFromLocalFile(Path src, Path dst)\n    throws IOException {\n    copyFromLocalFile(true, src, dst);\n  }\n\n  /**\n   * The src file is on the local disk.  Add it to the filesystem at\n   * the given dst name.\n   * delSrc indicates if the source should be removed\n   * @param delSrc whether to delete the src\n   * @param src path\n   * @param dst path\n   * @throws IOException IO failure.\n   */\n  public void copyFromLocalFile(boolean delSrc, Path src, Path dst)\n    throws IOException {\n    copyFromLocalFile(delSrc, true, src, dst);\n  }\n\n  /**\n   * The src files are on the local disk.  Add it to the filesystem at\n   * the given dst name.\n   * delSrc indicates if the source should be removed\n   * @param delSrc whether to delete the src\n   * @param overwrite whether to overwrite an existing file\n   * @param srcs array of paths which are source\n   * @param dst path\n   * @throws IOException IO failure\n   */\n  public void copyFromLocalFile(boolean delSrc, boolean overwrite,\n                                Path[] srcs, Path dst)\n    throws IOException {\n    Configuration conf = getConf();\n    FileUtil.copy(getLocal(conf), srcs, this, dst, delSrc, overwrite, conf);\n  }\n\n  /**\n   * The src file is on the local disk.  Add it to the filesystem at\n   * the given dst name.\n   * delSrc indicates if the source should be removed\n   * @param delSrc whether to delete the src\n   * @param overwrite whether to overwrite an existing file\n   * @param src path\n   * @param dst path\n   * @throws IOException IO failure\n   */\n  public void copyFromLocalFile(boolean delSrc, boolean overwrite,\n                                Path src, Path dst)\n    throws IOException {\n    Configuration conf = getConf();\n    FileUtil.copy(getLocal(conf), src, this, dst, delSrc, overwrite, conf);\n  }\n\n  /**\n   * Copy it a file from the remote filesystem to the local one.\n   * @param src path src file in the remote filesystem\n   * @param dst path local destination\n   * @throws IOException IO failure\n   */\n  public void copyToLocalFile(Path src, Path dst) throws IOException {\n    copyToLocalFile(false, src, dst);\n  }\n\n  /**\n   * Copy a file to the local filesystem, then delete it from the\n   * remote filesystem (if successfully copied).\n   * @param src path src file in the remote filesystem\n   * @param dst path local destination\n   * @throws IOException IO failure\n   */\n  public void moveToLocalFile(Path src, Path dst) throws IOException {\n    copyToLocalFile(true, src, dst);\n  }\n\n  /**\n   * Copy it a file from a remote filesystem to the local one.\n   * delSrc indicates if the src will be removed or not.\n   * @param delSrc whether to delete the src\n   * @param src path src file in the remote filesystem\n   * @param dst path local destination\n   * @throws IOException IO failure\n   */\n  public void copyToLocalFile(boolean delSrc, Path src, Path dst)\n    throws IOException {\n    copyToLocalFile(delSrc, src, dst, false);\n  }\n\n  /**\n   * The src file is under this filesystem, and the dst is on the local disk.\n   * Copy it from the remote filesystem to the local dst name.\n   * delSrc indicates if the src will be removed\n   * or not. useRawLocalFileSystem indicates whether to use RawLocalFileSystem\n   * as the local file system or not. RawLocalFileSystem is non checksumming,\n   * So, It will not create any crc files at local.\n   *\n   * @param delSrc\n   *          whether to delete the src\n   * @param src\n   *          path\n   * @param dst\n   *          path\n   * @param useRawLocalFileSystem\n   *          whether to use RawLocalFileSystem as local file system or not.\n   *\n   * @throws IOException for any IO error\n   */\n  public void copyToLocalFile(boolean delSrc, Path src, Path dst,\n      boolean useRawLocalFileSystem) throws IOException {\n    Configuration conf = getConf();\n    FileSystem local = null;\n    if (useRawLocalFileSystem) {\n      local = getLocal(conf).getRawFileSystem();\n    } else {\n      local = getLocal(conf);\n    }\n    FileUtil.copy(this, src, local, dst, delSrc, conf);\n  }\n\n  /**\n   * Returns a local file that the user can write output to.  The caller\n   * provides both the eventual target name in this FileSystem\n   * and the local working file path.\n   * If this FileSystem is local, we write directly into the target.  If\n   * the FileSystem is not local, we write into the tmp local area.\n   * @param fsOutputFile path of output file\n   * @param tmpLocalFile path of local tmp file\n   * @throws IOException IO failure\n   * @return the path.\n   */\n  public Path startLocalOutput(Path fsOutputFile, Path tmpLocalFile)\n    throws IOException {\n    return tmpLocalFile;\n  }\n\n  /**\n   * Called when we're all done writing to the target.\n   * A local FS will do nothing, because we've written to exactly the\n   * right place.\n   * A remote FS will copy the contents of tmpLocalFile to the correct target at\n   * fsOutputFile.\n   * @param fsOutputFile path of output file\n   * @param tmpLocalFile path to local tmp file\n   * @throws IOException IO failure\n   */\n  public void completeLocalOutput(Path fsOutputFile, Path tmpLocalFile)\n    throws IOException {\n    moveFromLocalFile(tmpLocalFile, fsOutputFile);\n  }\n\n  /**\n   * Close this FileSystem instance.\n   * Will release any held locks, delete all files queued for deletion\n   * through calls to {@link #deleteOnExit(Path)}, and remove this FS instance\n   * from the cache, if cached.\n   *\n   * After this operation, the outcome of any method call on this FileSystem\n   * instance, or any input/output stream created by it is <i>undefined</i>.\n   * @throws IOException IO failure\n   */\n  @Override\n  public void close() throws IOException {\n    debugLogFileSystemClose(\"close\", \"Key: \" + key + \"; URI: \" + getUri()\n        + \"; Object Identity Hash: \"\n        + Integer.toHexString(System.identityHashCode(this)));\n    // delete all files that were marked as delete-on-exit.\n    try {\n      processDeleteOnExit();\n    } finally {\n      CACHE.remove(this.key, this);\n    }\n  }\n\n  /**\n   * Return the total size of all files in the filesystem.\n   * @throws IOException IO failure\n   * @return the number of path used.\n   */\n  public long getUsed() throws IOException {\n    Path path = new Path(\"/\");\n    return getUsed(path);\n  }\n\n  /**\n   * Return the total size of all files from a specified path.\n   * @param path the path.\n   * @throws IOException IO failure\n   * @return the number of path content summary.\n   */\n  public long getUsed(Path path) throws IOException {\n    return getContentSummary(path).getLength();\n  }\n\n  /**\n   * Get the block size for a particular file.\n   * @param f the filename\n   * @return the number of bytes in a block\n   * @deprecated Use {@link #getFileStatus(Path)} instead\n   * @throws FileNotFoundException if the path is not present\n   * @throws IOException IO failure\n   */\n  @Deprecated\n  public long getBlockSize(Path f) throws IOException {\n    return getFileStatus(f).getBlockSize();\n  }\n\n  /**\n   * Return the number of bytes that large input files should be optimally\n   * be split into to minimize I/O time.\n   * @deprecated use {@link #getDefaultBlockSize(Path)} instead\n   * @return default block size.\n   */\n  @Deprecated\n  public long getDefaultBlockSize() {\n    // default to 32MB: large enough to minimize the impact of seeks\n    return getConf().getLong(\"fs.local.block.size\", 32 * 1024 * 1024);\n  }\n\n  /**\n   * Return the number of bytes that large input files should be optimally\n   * be split into to minimize I/O time.  The given path will be used to\n   * locate the actual filesystem.  The full path does not have to exist.\n   * @param f path of file\n   * @return the default block size for the path's filesystem\n   */\n  public long getDefaultBlockSize(Path f) {\n    return getDefaultBlockSize();\n  }\n\n  /**\n   * Get the default replication.\n   * @return the replication; the default value is \"1\".\n   * @deprecated use {@link #getDefaultReplication(Path)} instead\n   */\n  @Deprecated\n  public short getDefaultReplication() { return 1; }\n\n  /**\n   * Get the default replication for a path.\n   * The given path will be used to locate the actual FileSystem to query.\n   * The full path does not have to exist.\n   * @param path of the file\n   * @return default replication for the path's filesystem\n   */\n  public short getDefaultReplication(Path path) {\n    return getDefaultReplication();\n  }\n\n  /**\n   * Return a file status object that represents the path.\n   * @param f The path we want information from\n   * @return a FileStatus object\n   * @throws FileNotFoundException when the path does not exist\n   * @throws IOException see specific implementation\n   */\n  public abstract FileStatus getFileStatus(Path f) throws IOException;\n\n  /**\n   * Synchronize client metadata state.\n   * <p>\n   * In some FileSystem implementations such as HDFS metadata\n   * synchronization is essential to guarantee consistency of read requests\n   * particularly in HA setting.\n   * @throws IOException If an I/O error occurred.\n   * @throws UnsupportedOperationException if the operation is unsupported.\n   */\n  public void msync() throws IOException, UnsupportedOperationException {\n    throw new UnsupportedOperationException(getClass().getCanonicalName() +\n        \" does not support method msync\");\n  }\n\n  /**\n   * Checks if the user can access a path.  The mode specifies which access\n   * checks to perform.  If the requested permissions are granted, then the\n   * method returns normally.  If access is denied, then the method throws an\n   * {@link AccessControlException}.\n   * <p>\n   * The default implementation calls {@link #getFileStatus(Path)}\n   * and checks the returned permissions against the requested permissions.\n   *\n   * Note that the {@link #getFileStatus(Path)} call will be subject to\n   * authorization checks.\n   * Typically, this requires search (execute) permissions on each directory in\n   * the path's prefix, but this is implementation-defined.  Any file system\n   * that provides a richer authorization model (such as ACLs) may override the\n   * default implementation so that it checks against that model instead.\n   * <p>\n   * In general, applications should avoid using this method, due to the risk of\n   * time-of-check/time-of-use race conditions.  The permissions on a file may\n   * change immediately after the access call returns.  Most applications should\n   * prefer running specific file system actions as the desired user represented\n   * by a {@link UserGroupInformation}.\n   *\n   * @param path Path to check\n   * @param mode type of access to check\n   * @throws AccessControlException if access is denied\n   * @throws FileNotFoundException if the path does not exist\n   * @throws IOException see specific implementation\n   */\n  @InterfaceAudience.LimitedPrivate({\"HDFS\", \"Hive\"})\n  public void access(Path path, FsAction mode) throws AccessControlException,\n      FileNotFoundException, IOException {\n    checkAccessPermissions(this.getFileStatus(path), mode);\n  }\n\n  /**\n   * This method provides the default implementation of\n   * {@link #access(Path, FsAction)}.\n   *\n   * @param stat FileStatus to check\n   * @param mode type of access to check\n   * @throws AccessControlException if access is denied\n   * @throws IOException for any error\n   */\n  @InterfaceAudience.Private\n  static void checkAccessPermissions(FileStatus stat, FsAction mode)\n      throws AccessControlException, IOException {\n    FsPermission perm = stat.getPermission();\n    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n    String user = ugi.getShortUserName();\n    if (user.equals(stat.getOwner())) {\n      if (perm.getUserAction().implies(mode)) {\n        return;\n      }\n    } else if (ugi.getGroupsSet().contains(stat.getGroup())) {\n      if (perm.getGroupAction().implies(mode)) {\n        return;\n      }\n    } else {\n      if (perm.getOtherAction().implies(mode)) {\n        return;\n      }\n    }\n    throw new AccessControlException(String.format(\n      \"Permission denied: user=%s, path=\\\"%s\\\":%s:%s:%s%s\", user, stat.getPath(),\n      stat.getOwner(), stat.getGroup(), stat.isDirectory() ? \"d\" : \"-\", perm));\n  }\n\n  /**\n   * See {@link FileContext#fixRelativePart}.\n   * @param p the path.\n   * @return relative part.\n   */\n  protected Path fixRelativePart(Path p) {\n    if (p.isUriPathAbsolute()) {\n      return p;\n    } else {\n      return new Path(getWorkingDirectory(), p);\n    }\n  }\n\n  /**\n   * See {@link FileContext#createSymlink(Path, Path, boolean)}.\n   *\n   * @param target target path.\n   * @param link link.\n   * @param createParent create parent.\n   * @throws AccessControlException if access is denied.\n   * @throws FileAlreadyExistsException when the path does not exist.\n   * @throws FileNotFoundException when the path does not exist.\n   * @throws ParentNotDirectoryException if the parent path of dest is not\n   *                                     a directory.\n   * @throws UnsupportedFileSystemException if there was no known implementation\n   *                                        for the scheme.\n   * @throws IOException raised on errors performing I/O.\n   */\n  public void createSymlink(final Path target, final Path link,\n      final boolean createParent) throws AccessControlException,\n      FileAlreadyExistsException, FileNotFoundException,\n      ParentNotDirectoryException, UnsupportedFileSystemException,\n      IOException {\n    // Supporting filesystems should override this method\n    throw new UnsupportedOperationException(\n        \"Filesystem does not support symlinks!\");\n  }\n\n  /**\n   * See {@link FileContext#getFileLinkStatus(Path)}.\n   *\n   * @param f the path.\n   * @throws AccessControlException if access is denied.\n   * @throws FileNotFoundException when the path does not exist.\n   * @throws IOException raised on errors performing I/O.\n   * @throws UnsupportedFileSystemException if there was no known implementation\n   *                                        for the scheme.\n   * @return file status\n   */\n  public FileStatus getFileLinkStatus(final Path f)\n      throws AccessControlException, FileNotFoundException,\n      UnsupportedFileSystemException, IOException {\n    // Supporting filesystems should override this method\n    return getFileStatus(f);\n  }\n\n  /**\n   * See {@link AbstractFileSystem#supportsSymlinks()}.\n   * @return if support symlinkls true, not false.\n   */\n  public boolean supportsSymlinks() {\n    return false;\n  }\n\n  /**\n   * See {@link FileContext#getLinkTarget(Path)}.\n   * @param f the path.\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).\n   * @throws IOException IO failure.\n   * @return the path.\n   */\n  public Path getLinkTarget(Path f) throws IOException {\n    // Supporting filesystems should override this method\n    throw new UnsupportedOperationException(\n        \"Filesystem does not support symlinks!\");\n  }\n\n  /**\n   * See {@link AbstractFileSystem#getLinkTarget(Path)}.\n   * @param f the path.\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).\n   * @throws IOException IO failure.\n   * @return the path.\n   */\n  protected Path resolveLink(Path f) throws IOException {\n    // Supporting filesystems should override this method\n    throw new UnsupportedOperationException(\n        \"Filesystem does not support symlinks!\");\n  }\n\n  /**\n   * Get the checksum of a file, if the FS supports checksums.\n   *\n   * @param f The file path\n   * @return The file checksum.  The default return value is null,\n   *  which indicates that no checksum algorithm is implemented\n   *  in the corresponding FileSystem.\n   * @throws IOException IO failure\n   */\n  public FileChecksum getFileChecksum(Path f) throws IOException {\n    return getFileChecksum(f, Long.MAX_VALUE);\n  }\n\n  /**\n   * Get the checksum of a file, from the beginning of the file till the\n   * specific length.\n   * @param f The file path\n   * @param length The length of the file range for checksum calculation\n   * @return The file checksum or null if checksums are not supported.\n   * @throws IOException IO failure\n   */\n  public FileChecksum getFileChecksum(Path f, final long length)\n      throws IOException {\n    return null;\n  }\n\n  /**\n   * Set the verify checksum flag. This is only applicable if the\n   * corresponding filesystem supports checksums.\n   * By default doesn't do anything.\n   * @param verifyChecksum Verify checksum flag\n   */\n  public void setVerifyChecksum(boolean verifyChecksum) {\n    //doesn't do anything\n  }\n\n  /**\n   * Set the write checksum flag. This is only applicable if the\n   * corresponding filesystem supports checksums.\n   * By default doesn't do anything.\n   * @param writeChecksum Write checksum flag\n   */\n  public void setWriteChecksum(boolean writeChecksum) {\n    //doesn't do anything\n  }\n\n  /**\n   * Returns a status object describing the use and capacity of the\n   * filesystem. If the filesystem has multiple partitions, the\n   * use and capacity of the root partition is reflected.\n   *\n   * @return a FsStatus object\n   * @throws IOException\n   *           see specific implementation\n   */\n  public FsStatus getStatus() throws IOException {\n    return getStatus(null);\n  }\n\n  /**\n   * Returns a status object describing the use and capacity of the\n   * filesystem. If the filesystem has multiple partitions, the\n   * use and capacity of the partition pointed to by the specified\n   * path is reflected.\n   * @param p Path for which status should be obtained. null means\n   * the default partition.\n   * @return a FsStatus object\n   * @throws IOException\n   *           see specific implementation\n   */\n  public FsStatus getStatus(Path p) throws IOException {\n    return new FsStatus(Long.MAX_VALUE, 0, Long.MAX_VALUE);\n  }\n\n  /**\n   * Set permission of a path.\n   * @param p The path\n   * @param permission permission\n   * @throws IOException IO failure\n   */\n  public void setPermission(Path p, FsPermission permission\n      ) throws IOException {\n  }\n\n  /**\n   * Set owner of a path (i.e. a file or a directory).\n   * The parameters username and groupname cannot both be null.\n   * @param p The path\n   * @param username If it is null, the original username remains unchanged.\n   * @param groupname If it is null, the original groupname remains unchanged.\n   * @throws IOException IO failure\n   */\n  public void setOwner(Path p, String username, String groupname\n      ) throws IOException {\n  }\n\n  /**\n   * Set access time of a file.\n   * @param p The path\n   * @param mtime Set the modification time of this file.\n   *              The number of milliseconds since Jan 1, 1970.\n   *              A value of -1 means that this call should not set modification time.\n   * @param atime Set the access time of this file.\n   *              The number of milliseconds since Jan 1, 1970.\n   *              A value of -1 means that this call should not set access time.\n   * @throws IOException IO failure\n   */\n  public void setTimes(Path p, long mtime, long atime\n      ) throws IOException {\n  }\n\n  /**\n   * Create a snapshot with a default name.\n   * @param path The directory where snapshots will be taken.\n   * @return the snapshot path.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   */\n  public final Path createSnapshot(Path path) throws IOException {\n    return createSnapshot(path, null);\n  }\n\n  /**\n   * Create a snapshot.\n   * @param path The directory where snapshots will be taken.\n   * @param snapshotName The name of the snapshot\n   * @return the snapshot path.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   */\n  public Path createSnapshot(Path path, String snapshotName)\n      throws IOException {\n    throw new UnsupportedOperationException(getClass().getSimpleName()\n        + \" doesn't support createSnapshot\");\n  }\n\n  /**\n   * Rename a snapshot.\n   * @param path The directory path where the snapshot was taken\n   * @param snapshotOldName Old name of the snapshot\n   * @param snapshotNewName New name of the snapshot\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).\n   */\n  public void renameSnapshot(Path path, String snapshotOldName,\n      String snapshotNewName) throws IOException {\n    throw new UnsupportedOperationException(getClass().getSimpleName()\n        + \" doesn't support renameSnapshot\");\n  }\n\n  /**\n   * Delete a snapshot of a directory.\n   * @param path  The directory that the to-be-deleted snapshot belongs to\n   * @param snapshotName The name of the snapshot\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).\n   */\n  public void deleteSnapshot(Path path, String snapshotName)\n      throws IOException {\n    throw new UnsupportedOperationException(getClass().getSimpleName()\n        + \" doesn't support deleteSnapshot\");\n  }\n\n  /**\n   * Modifies ACL entries of files and directories.  This method can add new ACL\n   * entries or modify the permissions on existing ACL entries.  All existing\n   * ACL entries that are not specified in this call are retained without\n   * changes.  (Modifications are merged into the current ACL.)\n   *\n   * @param path Path to modify\n   * @param aclSpec List&lt;AclEntry&gt; describing modifications\n   * @throws IOException if an ACL could not be modified\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).\n   */\n  public void modifyAclEntries(Path path, List<AclEntry> aclSpec)\n      throws IOException {\n    throw new UnsupportedOperationException(getClass().getSimpleName()\n        + \" doesn't support modifyAclEntries\");\n  }\n\n  /**\n   * Removes ACL entries from files and directories.  Other ACL entries are\n   * retained.\n   *\n   * @param path Path to modify\n   * @param aclSpec List describing entries to remove\n   * @throws IOException if an ACL could not be modified\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).\n   */\n  public void removeAclEntries(Path path, List<AclEntry> aclSpec)\n      throws IOException {\n    throw new UnsupportedOperationException(getClass().getSimpleName()\n        + \" doesn't support removeAclEntries\");\n  }\n\n  /**\n   * Removes all default ACL entries from files and directories.\n   *\n   * @param path Path to modify\n   * @throws IOException if an ACL could not be modified\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).\n   */\n  public void removeDefaultAcl(Path path)\n      throws IOException {\n    throw new UnsupportedOperationException(getClass().getSimpleName()\n        + \" doesn't support removeDefaultAcl\");\n  }\n\n  /**\n   * Removes all but the base ACL entries of files and directories.  The entries\n   * for user, group, and others are retained for compatibility with permission\n   * bits.\n   *\n   * @param path Path to modify\n   * @throws IOException if an ACL could not be removed\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).\n   */\n  public void removeAcl(Path path)\n      throws IOException {\n    throw new UnsupportedOperationException(getClass().getSimpleName()\n        + \" doesn't support removeAcl\");\n  }\n\n  /**\n   * Fully replaces ACL of files and directories, discarding all existing\n   * entries.\n   *\n   * @param path Path to modify\n   * @param aclSpec List describing modifications, which must include entries\n   *   for user, group, and others for compatibility with permission bits.\n   * @throws IOException if an ACL could not be modified\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).\n   */\n  public void setAcl(Path path, List<AclEntry> aclSpec) throws IOException {\n    throw new UnsupportedOperationException(getClass().getSimpleName()\n        + \" doesn't support setAcl\");\n  }\n\n  /**\n   * Gets the ACL of a file or directory.\n   *\n   * @param path Path to get\n   * @return AclStatus describing the ACL of the file or directory\n   * @throws IOException if an ACL could not be read\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).\n   */\n  public AclStatus getAclStatus(Path path) throws IOException {\n    throw new UnsupportedOperationException(getClass().getSimpleName()\n        + \" doesn't support getAclStatus\");\n  }\n\n  /**\n   * Set an xattr of a file or directory.\n   * The name must be prefixed with the namespace followed by \".\". For example,\n   * \"user.attr\".\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to modify\n   * @param name xattr name.\n   * @param value xattr value.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).\n   */\n  public void setXAttr(Path path, String name, byte[] value)\n      throws IOException {\n    setXAttr(path, name, value, EnumSet.of(XAttrSetFlag.CREATE,\n        XAttrSetFlag.REPLACE));\n  }\n\n  /**\n   * Set an xattr of a file or directory.\n   * The name must be prefixed with the namespace followed by \".\". For example,\n   * \"user.attr\".\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to modify\n   * @param name xattr name.\n   * @param value xattr value.\n   * @param flag xattr set flag\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).\n   */\n  public void setXAttr(Path path, String name, byte[] value,\n      EnumSet<XAttrSetFlag> flag) throws IOException {\n    throw new UnsupportedOperationException(getClass().getSimpleName()\n        + \" doesn't support setXAttr\");\n  }\n\n  /**\n   * Get an xattr name and value for a file or directory.\n   * The name must be prefixed with the namespace followed by \".\". For example,\n   * \"user.attr\".\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to get extended attribute\n   * @param name xattr name.\n   * @return byte[] xattr value.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).\n   */\n  public byte[] getXAttr(Path path, String name) throws IOException {\n    throw new UnsupportedOperationException(getClass().getSimpleName()\n        + \" doesn't support getXAttr\");\n  }\n\n  /**\n   * Get all of the xattr name/value pairs for a file or directory.\n   * Only those xattrs which the logged-in user has permissions to view\n   * are returned.\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to get extended attributes\n   * @return Map describing the XAttrs of the file or directory\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).\n   */\n  public Map<String, byte[]> getXAttrs(Path path) throws IOException {\n    throw new UnsupportedOperationException(getClass().getSimpleName()\n        + \" doesn't support getXAttrs\");\n  }\n\n  /**\n   * Get all of the xattrs name/value pairs for a file or directory.\n   * Only those xattrs which the logged-in user has permissions to view\n   * are returned.\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to get extended attributes\n   * @param names XAttr names.\n   * @return Map describing the XAttrs of the file or directory\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).\n   */\n  public Map<String, byte[]> getXAttrs(Path path, List<String> names)\n      throws IOException {\n    throw new UnsupportedOperationException(getClass().getSimpleName()\n        + \" doesn't support getXAttrs\");\n  }\n\n  /**\n   * Get all of the xattr names for a file or directory.\n   * Only those xattr names which the logged-in user has permissions to view\n   * are returned.\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to get extended attributes\n   * @return List{@literal <String>} of the XAttr names of the file or directory\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).\n   */\n  public List<String> listXAttrs(Path path) throws IOException {\n    throw new UnsupportedOperationException(getClass().getSimpleName()\n            + \" doesn't support listXAttrs\");\n  }\n\n  /**\n   * Remove an xattr of a file or directory.\n   * The name must be prefixed with the namespace followed by \".\". For example,\n   * \"user.attr\".\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to remove extended attribute\n   * @param name xattr name\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).\n   */\n  public void removeXAttr(Path path, String name) throws IOException {\n    throw new UnsupportedOperationException(getClass().getSimpleName()\n        + \" doesn't support removeXAttr\");\n  }\n\n  /**\n   * Set the source path to satisfy storage policy.\n   * @param path The source path referring to either a directory or a file.\n   * @throws IOException If an I/O error occurred.\n   */\n  public void satisfyStoragePolicy(final Path path) throws IOException {\n    throw new UnsupportedOperationException(\n        getClass().getSimpleName() + \" doesn't support setStoragePolicy\");\n  }\n\n  /**\n   * Set the storage policy for a given file or directory.\n   *\n   * @param src file or directory path.\n   * @param policyName the name of the target storage policy. The list\n   *                   of supported Storage policies can be retrieved\n   *                   via {@link #getAllStoragePolicies}.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).\n   */\n  public void setStoragePolicy(final Path src, final String policyName)\n      throws IOException {\n    throw new UnsupportedOperationException(getClass().getSimpleName()\n        + \" doesn't support setStoragePolicy\");\n  }\n\n  /**\n   * Unset the storage policy set for a given file or directory.\n   * @param src file or directory path.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).\n   */\n  public void unsetStoragePolicy(final Path src) throws IOException {\n    throw new UnsupportedOperationException(getClass().getSimpleName()\n        + \" doesn't support unsetStoragePolicy\");\n  }\n\n  /**\n   * Query the effective storage policy ID for the given file or directory.\n   *\n   * @param src file or directory path.\n   * @return storage policy for give file.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).\n   */\n  public BlockStoragePolicySpi getStoragePolicy(final Path src)\n      throws IOException {\n    throw new UnsupportedOperationException(getClass().getSimpleName()\n        + \" doesn't support getStoragePolicy\");\n  }\n\n  /**\n   * Retrieve all the storage policies supported by this file system.\n   *\n   * @return all storage policies supported by this filesystem.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).\n   */\n  public Collection<? extends BlockStoragePolicySpi> getAllStoragePolicies()\n      throws IOException {\n    throw new UnsupportedOperationException(getClass().getSimpleName()\n        + \" doesn't support getAllStoragePolicies\");\n  }\n\n  /**\n   * Get the root directory of Trash for current user when the path specified\n   * is deleted.\n   *\n   * @param path the trash root of the path to be determined.\n   * @return the default implementation returns {@code /user/$USER/.Trash}\n   */\n  public Path getTrashRoot(Path path) {\n    return this.makeQualified(new Path(getHomeDirectory().toUri().getPath(),\n        TRASH_PREFIX));\n  }\n\n  /**\n   * Get all the trash roots for current user or all users.\n   *\n   * @param allUsers return trash roots for all users if true.\n   * @return all the trash root directories.\n   *         Default FileSystem returns .Trash under users' home directories if\n   *         {@code /user/$USER/.Trash} exists.\n   */\n  public Collection<FileStatus> getTrashRoots(boolean allUsers) {\n    Path userHome = new Path(getHomeDirectory().toUri().getPath());\n    List<FileStatus> ret = new ArrayList<>();\n    try {\n      if (!allUsers) {\n        Path userTrash = new Path(userHome, TRASH_PREFIX);\n        if (exists(userTrash)) {\n          ret.add(getFileStatus(userTrash));\n        }\n      } else {\n        Path homeParent = userHome.getParent();\n        if (exists(homeParent)) {\n          FileStatus[] candidates = listStatus(homeParent);\n          for (FileStatus candidate : candidates) {\n            Path userTrash = new Path(candidate.getPath(), TRASH_PREFIX);\n            if (exists(userTrash)) {\n              candidate.setPath(userTrash);\n              ret.add(candidate);\n            }\n          }\n        }\n      }\n    } catch (IOException e) {\n      LOGGER.warn(\"Cannot get all trash roots\", e);\n    }\n    return ret;\n  }\n\n  /**\n   * The base FileSystem implementation generally has no knowledge\n   * of the capabilities of actual implementations.\n   * Unless it has a way to explicitly determine the capabilities,\n   * this method returns false.\n   * {@inheritDoc}\n   */\n  public boolean hasPathCapability(final Path path, final String capability)\n      throws IOException {\n    switch (validatePathCapabilityArgs(makeQualified(path), capability)) {\n    case CommonPathCapabilities.FS_SYMLINKS:\n      // delegate to the existing supportsSymlinks() call.\n      return supportsSymlinks() && areSymlinksEnabled();\n    default:\n      // the feature is not implemented.\n      return false;\n    }\n  }\n\n  // making it volatile to be able to do a double checked locking\n  private volatile static boolean FILE_SYSTEMS_LOADED = false;\n\n  /**\n   * Filesystems listed as services.\n   */\n  private static final Map<String, Class<? extends FileSystem>>\n      SERVICE_FILE_SYSTEMS = new HashMap<>();\n\n  /**\n   * Load the filesystem declarations from service resources.\n   * This is a synchronized operation.\n   */\n  private static void loadFileSystems() {\n    LOGGER.debug(\"Loading filesystems\");\n    synchronized (FileSystem.class) {\n      if (!FILE_SYSTEMS_LOADED) {\n        ServiceLoader<FileSystem> serviceLoader = ServiceLoader.load(FileSystem.class);\n        Iterator<FileSystem> it = serviceLoader.iterator();\n        while (it.hasNext()) {\n          FileSystem fs;\n          try {\n            fs = it.next();\n            try {\n              SERVICE_FILE_SYSTEMS.put(fs.getScheme(), fs.getClass());\n              if (LOGGER.isDebugEnabled()) {\n                LOGGER.debug(\"{}:// = {} from {}\",\n                    fs.getScheme(), fs.getClass(),\n                    ClassUtil.findContainingJar(fs.getClass()));\n              }\n            } catch (Exception e) {\n              LOGGER.warn(\"Cannot load: {} from {}\", fs,\n                  ClassUtil.findContainingJar(fs.getClass()));\n              LOGGER.info(\"Full exception loading: {}\", fs, e);\n            }\n          } catch (ServiceConfigurationError ee) {\n            LOGGER.warn(\"Cannot load filesystem\", ee);\n          }\n        }\n        FILE_SYSTEMS_LOADED = true;\n      }\n    }\n  }\n\n  /**\n   * Get the FileSystem implementation class of a filesystem.\n   * This triggers a scan and load of all FileSystem implementations listed as\n   * services and discovered via the {@link ServiceLoader}\n   * @param scheme URL scheme of FS\n   * @param conf configuration: can be null, in which case the check for\n   * a filesystem binding declaration in the configuration is skipped.\n   * @return the filesystem\n   * @throws UnsupportedFileSystemException if there was no known implementation\n   *         for the scheme.\n   * @throws IOException if the filesystem could not be loaded\n   */\n  public static Class<? extends FileSystem> getFileSystemClass(String scheme,\n      Configuration conf) throws IOException {\n    if (!FILE_SYSTEMS_LOADED) {\n      loadFileSystems();\n    }\n    LOGGER.debug(\"Looking for FS supporting {}\", scheme);\n    Class<? extends FileSystem> clazz = null;\n    if (conf != null) {\n      String property = \"fs.\" + scheme + \".impl\";\n      LOGGER.debug(\"looking for configuration option {}\", property);\n      clazz = (Class<? extends FileSystem>) conf.getClass(\n          property, null);\n    } else {\n      LOGGER.debug(\"No configuration: skipping check for fs.{}.impl\", scheme);\n    }\n    if (clazz == null) {\n      LOGGER.debug(\"Looking in service filesystems for implementation class\");\n      clazz = SERVICE_FILE_SYSTEMS.get(scheme);\n    } else {\n      LOGGER.debug(\"Filesystem {} defined in configuration option\", scheme);\n    }\n    if (clazz == null) {\n      throw new UnsupportedFileSystemException(\"No FileSystem for scheme \"\n          + \"\\\"\" + scheme + \"\\\"\");\n    }\n    LOGGER.debug(\"FS for {} is {}\", scheme, clazz);\n    return clazz;\n  }\n\n  /**\n   * Create and initialize a new instance of a FileSystem.\n   * @param uri URI containing the FS schema and FS details\n   * @param conf configuration to use to look for the FS instance declaration\n   * and to pass to the {@link FileSystem#initialize(URI, Configuration)}.\n   * @return the initialized filesystem.\n   * @throws IOException problems loading or initializing the FileSystem\n   */\n  private static FileSystem createFileSystem(URI uri, Configuration conf)\n      throws IOException {\n    Tracer tracer = FsTracer.get(conf);\n    try(TraceScope scope = tracer.newScope(\"FileSystem#createFileSystem\");\n        DurationInfo ignored =\n            new DurationInfo(LOGGER, false, \"Creating FS %s\", uri)) {\n      scope.addKVAnnotation(\"scheme\", uri.getScheme());\n      Class<? extends FileSystem> clazz =\n          getFileSystemClass(uri.getScheme(), conf);\n      FileSystem fs = ReflectionUtils.newInstance(clazz, conf);\n      try {\n        fs.initialize(uri, conf);\n      } catch (IOException | RuntimeException e) {\n        // exception raised during initialization.\n        // log summary at warn and full stack at debug\n        LOGGER.warn(\"Failed to initialize filesystem {}: {}\",\n            uri, e.toString());\n        LOGGER.debug(\"Failed to initialize filesystem\", e);\n        // then (robustly) close the FS, so as to invoke any\n        // cleanup code.\n        IOUtils.cleanupWithLogger(LOGGER, fs);\n        throw e;\n      }\n      return fs;\n    }\n  }\n\n  /** Caching FileSystem objects. */\n  static final class Cache {\n    private final ClientFinalizer clientFinalizer = new ClientFinalizer();\n\n    private final Map<Key, FileSystem> map = new HashMap<>();\n    private final Set<Key> toAutoClose = new HashSet<>();\n\n    /** Semaphore used to serialize creation of new FS instances. */\n    private final Semaphore creatorPermits;\n\n    /**\n     * Counter of the number of discarded filesystem instances\n     * in this cache. Primarily for testing, but it could possibly\n     * be made visible as some kind of metric.\n     */\n    private final AtomicLong discardedInstances = new AtomicLong(0);\n\n    /** A variable that makes all objects in the cache unique. */\n    private static AtomicLong unique = new AtomicLong(1);\n\n    /**\n     * Instantiate. The configuration is used to read the\n     * count of permits issued for concurrent creation\n     * of filesystem instances.\n     * @param conf configuration\n     */\n    Cache(final Configuration conf) {\n      int permits = conf.getInt(FS_CREATION_PARALLEL_COUNT,\n          FS_CREATION_PARALLEL_COUNT_DEFAULT);\n      checkArgument(permits > 0, \"Invalid value of %s: %s\",\n          FS_CREATION_PARALLEL_COUNT, permits);\n      creatorPermits = new Semaphore(permits);\n    }\n\n    FileSystem get(URI uri, Configuration conf) throws IOException{\n      Key key = new Key(uri, conf);\n      return getInternal(uri, conf, key);\n    }\n\n    /** The objects inserted into the cache using this method are all unique. */\n    FileSystem getUnique(URI uri, Configuration conf) throws IOException{\n      Key key = new Key(uri, conf, unique.getAndIncrement());\n      return getInternal(uri, conf, key);\n    }\n\n    /**\n     * Get the FS instance if the key maps to an instance, creating and\n     * initializing the FS if it is not found.\n     * If this is the first entry in the map and the JVM is not shutting down,\n     * this registers a shutdown hook to close filesystems, and adds this\n     * FS to the {@code toAutoClose} set if {@code \"fs.automatic.close\"}\n     * is set in the configuration (default: true).\n     * @param uri filesystem URI\n     * @param conf configuration\n     * @param key key to store/retrieve this FileSystem in the cache\n     * @return a cached or newly instantiated FileSystem.\n     * @throws IOException If an I/O error occurred.\n     */\n    private FileSystem getInternal(URI uri, Configuration conf, Key key)\n        throws IOException{\n      FileSystem fs;\n      synchronized (this) {\n        fs = map.get(key);\n      }\n      if (fs != null) {\n        return fs;\n      }\n      // fs not yet created, acquire lock\n      // to construct an instance.\n      try (DurationInfo d = new DurationInfo(LOGGER, false,\n          \"Acquiring creator semaphore for %s\", uri)) {\n        creatorPermits.acquireUninterruptibly();\n      }\n      FileSystem fsToClose = null;\n      try {\n        // See if FS was instantiated by another thread while waiting\n        // for the permit.\n        synchronized (this) {\n          fs = map.get(key);\n        }\n        if (fs != null) {\n          LOGGER.debug(\"Filesystem {} created while awaiting semaphore\", uri);\n          return fs;\n        }\n        // create the filesystem\n        fs = createFileSystem(uri, conf);\n        final long timeout = conf.getTimeDuration(SERVICE_SHUTDOWN_TIMEOUT,\n            SERVICE_SHUTDOWN_TIMEOUT_DEFAULT,\n            ShutdownHookManager.TIME_UNIT_DEFAULT);\n        // any FS to close outside of the synchronized section\n        synchronized (this) { // lock on the Cache object\n\n          // see if there is now an entry for the FS, which happens\n          // if another thread's creation overlapped with this one.\n          FileSystem oldfs = map.get(key);\n          if (oldfs != null) {\n            // a file system was created in a separate thread.\n            // save the FS reference to close outside all locks,\n            // and switch to returning the oldFS\n            fsToClose = fs;\n            fs = oldfs;\n          } else {\n            // register the clientFinalizer if needed and shutdown isn't\n            // already active\n            if (map.isEmpty()\n                && !ShutdownHookManager.get().isShutdownInProgress()) {\n              ShutdownHookManager.get().addShutdownHook(clientFinalizer,\n                  SHUTDOWN_HOOK_PRIORITY, timeout,\n                  ShutdownHookManager.TIME_UNIT_DEFAULT);\n            }\n            // insert the new file system into the map\n            fs.key = key;\n            map.put(key, fs);\n            if (conf.getBoolean(\n                FS_AUTOMATIC_CLOSE_KEY, FS_AUTOMATIC_CLOSE_DEFAULT)) {\n              toAutoClose.add(key);\n            }\n          }\n        } // end of synchronized block\n      } finally {\n        // release the creator permit.\n        creatorPermits.release();\n      }\n      if (fsToClose != null) {\n        LOGGER.debug(\"Duplicate FS created for {}; discarding {}\",\n            uri, fs);\n        discardedInstances.incrementAndGet();\n        // close the new file system\n        // note this will briefly remove and reinstate \"fsToClose\" from\n        // the map. It is done in a synchronized block so will not be\n        // visible to others.\n        IOUtils.cleanupWithLogger(LOGGER, fsToClose);\n      }\n      return fs;\n    }\n\n    /**\n     * Get the count of discarded instances.\n     * @return the new instance.\n     */\n    @VisibleForTesting\n    long getDiscardedInstances() {\n      return discardedInstances.get();\n    }\n\n    synchronized void remove(Key key, FileSystem fs) {\n      FileSystem cachedFs = map.remove(key);\n      if (fs == cachedFs) {\n        toAutoClose.remove(key);\n      } else if (cachedFs != null) {\n        map.put(key, cachedFs);\n      }\n    }\n\n    /**\n     * Close all FileSystems in the cache, whether they are marked for\n     * automatic closing or not.\n     * @throws IOException a problem arose closing one or more FileSystem.\n     */\n    synchronized void closeAll() throws IOException {\n      closeAll(false);\n    }\n\n    /**\n     * Close all FileSystem instances in the Cache.\n     * @param onlyAutomatic only close those that are marked for automatic closing\n     * @throws IOException a problem arose closing one or more FileSystem.\n     */\n    synchronized void closeAll(boolean onlyAutomatic) throws IOException {\n      List<IOException> exceptions = new ArrayList<>();\n\n      // Make a copy of the keys in the map since we'll be modifying\n      // the map while iterating over it, which isn't safe.\n      List<Key> keys = new ArrayList<>();\n      keys.addAll(map.keySet());\n\n      for (Key key : keys) {\n        final FileSystem fs = map.get(key);\n\n        if (onlyAutomatic && !toAutoClose.contains(key)) {\n          continue;\n        }\n\n        //remove from cache\n        map.remove(key);\n        toAutoClose.remove(key);\n\n        if (fs != null) {\n          try {\n            fs.close();\n          }\n          catch(IOException ioe) {\n            exceptions.add(ioe);\n          }\n        }\n      }\n\n      if (!exceptions.isEmpty()) {\n        throw MultipleIOException.createIOException(exceptions);\n      }\n    }\n\n    private class ClientFinalizer implements Runnable {\n      @Override\n      public synchronized void run() {\n        try {\n          closeAll(true);\n        } catch (IOException e) {\n          LOGGER.info(\"FileSystem.Cache.closeAll() threw an exception:\\n\" + e);\n        }\n      }\n    }\n\n    synchronized void closeAll(UserGroupInformation ugi) throws IOException {\n      List<FileSystem> targetFSList = new ArrayList<>(map.entrySet().size());\n      //Make a pass over the list and collect the FileSystems to close\n      //we cannot close inline since close() removes the entry from the Map\n      for (Map.Entry<Key, FileSystem> entry : map.entrySet()) {\n        final Key key = entry.getKey();\n        final FileSystem fs = entry.getValue();\n        if (ugi.equals(key.ugi) && fs != null) {\n          targetFSList.add(fs);\n        }\n      }\n      List<IOException> exceptions = new ArrayList<>();\n      //now make a pass over the target list and close each\n      for (FileSystem fs : targetFSList) {\n        try {\n          fs.close();\n        }\n        catch(IOException ioe) {\n          exceptions.add(ioe);\n        }\n      }\n      if (!exceptions.isEmpty()) {\n        throw MultipleIOException.createIOException(exceptions);\n      }\n    }\n\n    /** FileSystem.Cache.Key */\n    static class Key {\n      final String scheme;\n      final String authority;\n      final UserGroupInformation ugi;\n      final long unique;   // an artificial way to make a key unique\n\n      Key(URI uri, Configuration conf) throws IOException {\n        this(uri, conf, 0);\n      }\n\n      Key(URI uri, Configuration conf, long unique) throws IOException {\n        scheme = uri.getScheme()==null ?\n            \"\" : StringUtils.toLowerCase(uri.getScheme());\n        authority = uri.getAuthority()==null ?\n            \"\" : StringUtils.toLowerCase(uri.getAuthority());\n        this.unique = unique;\n\n        this.ugi = UserGroupInformation.getCurrentUser();\n      }\n\n      @Override\n      public int hashCode() {\n        return (scheme + authority).hashCode() + ugi.hashCode() + (int)unique;\n      }\n\n      static boolean isEqual(Object a, Object b) {\n        return a == b || (a != null && a.equals(b));\n      }\n\n      @Override\n      public boolean equals(Object obj) {\n        if (obj == this) {\n          return true;\n        }\n        if (obj instanceof Key) {\n          Key that = (Key)obj;\n          return isEqual(this.scheme, that.scheme)\n                 && isEqual(this.authority, that.authority)\n                 && isEqual(this.ugi, that.ugi)\n                 && (this.unique == that.unique);\n        }\n        return false;\n      }\n\n      @Override\n      public String toString() {\n        return \"(\"+ugi.toString() + \")@\" + scheme + \"://\" + authority;\n      }\n    }\n  }\n\n  /**\n   * Tracks statistics about how many reads, writes, and so forth have been\n   * done in a FileSystem.\n   *\n   * Since there is only one of these objects per FileSystem, there will\n   * typically be many threads writing to this object.  Almost every operation\n   * on an open file will involve a write to this object.  In contrast, reading\n   * statistics is done infrequently by most programs, and not at all by others.\n   * Hence, this is optimized for writes.\n   *\n   * Each thread writes to its own thread-local area of memory.  This removes\n   * contention and allows us to scale up to many, many threads.  To read\n   * statistics, the reader thread totals up the contents of all of the\n   * thread-local data areas.\n   */\n  public static final class Statistics {\n    /**\n     * Statistics data.\n     *\n     * There is only a single writer to thread-local StatisticsData objects.\n     * Hence, volatile is adequate here-- we do not need AtomicLong or similar\n     * to prevent lost updates.\n     * The Java specification guarantees that updates to volatile longs will\n     * be perceived as atomic with respect to other threads, which is all we\n     * need.\n     */\n    public static class StatisticsData {\n      private volatile long bytesRead;\n      private volatile long bytesWritten;\n      private volatile int readOps;\n      private volatile int largeReadOps;\n      private volatile int writeOps;\n      private volatile long bytesReadLocalHost;\n      private volatile long bytesReadDistanceOfOneOrTwo;\n      private volatile long bytesReadDistanceOfThreeOrFour;\n      private volatile long bytesReadDistanceOfFiveOrLarger;\n      private volatile long bytesReadErasureCoded;\n      private volatile long remoteReadTimeMS;\n\n      /**\n       * Add another StatisticsData object to this one.\n       */\n      void add(StatisticsData other) {\n        this.bytesRead += other.bytesRead;\n        this.bytesWritten += other.bytesWritten;\n        this.readOps += other.readOps;\n        this.largeReadOps += other.largeReadOps;\n        this.writeOps += other.writeOps;\n        this.bytesReadLocalHost += other.bytesReadLocalHost;\n        this.bytesReadDistanceOfOneOrTwo += other.bytesReadDistanceOfOneOrTwo;\n        this.bytesReadDistanceOfThreeOrFour +=\n            other.bytesReadDistanceOfThreeOrFour;\n        this.bytesReadDistanceOfFiveOrLarger +=\n            other.bytesReadDistanceOfFiveOrLarger;\n        this.bytesReadErasureCoded += other.bytesReadErasureCoded;\n        this.remoteReadTimeMS += other.remoteReadTimeMS;\n      }\n\n      /**\n       * Negate the values of all statistics.\n       */\n      void negate() {\n        this.bytesRead = -this.bytesRead;\n        this.bytesWritten = -this.bytesWritten;\n        this.readOps = -this.readOps;\n        this.largeReadOps = -this.largeReadOps;\n        this.writeOps = -this.writeOps;\n        this.bytesReadLocalHost = -this.bytesReadLocalHost;\n        this.bytesReadDistanceOfOneOrTwo = -this.bytesReadDistanceOfOneOrTwo;\n        this.bytesReadDistanceOfThreeOrFour =\n            -this.bytesReadDistanceOfThreeOrFour;\n        this.bytesReadDistanceOfFiveOrLarger =\n            -this.bytesReadDistanceOfFiveOrLarger;\n        this.bytesReadErasureCoded = -this.bytesReadErasureCoded;\n        this.remoteReadTimeMS = -this.remoteReadTimeMS;\n      }\n\n      @Override\n      public String toString() {\n        return bytesRead + \" bytes read, \" + bytesWritten + \" bytes written, \"\n            + readOps + \" read ops, \" + largeReadOps + \" large read ops, \"\n            + writeOps + \" write ops\";\n      }\n\n      public long getBytesRead() {\n        return bytesRead;\n      }\n\n      public long getBytesWritten() {\n        return bytesWritten;\n      }\n\n      public int getReadOps() {\n        return readOps;\n      }\n\n      public int getLargeReadOps() {\n        return largeReadOps;\n      }\n\n      public int getWriteOps() {\n        return writeOps;\n      }\n\n      public long getBytesReadLocalHost() {\n        return bytesReadLocalHost;\n      }\n\n      public long getBytesReadDistanceOfOneOrTwo() {\n        return bytesReadDistanceOfOneOrTwo;\n      }\n\n      public long getBytesReadDistanceOfThreeOrFour() {\n        return bytesReadDistanceOfThreeOrFour;\n      }\n\n      public long getBytesReadDistanceOfFiveOrLarger() {\n        return bytesReadDistanceOfFiveOrLarger;\n      }\n\n      public long getBytesReadErasureCoded() {\n        return bytesReadErasureCoded;\n      }\n\n      public long getRemoteReadTimeMS() {\n        return remoteReadTimeMS;\n      }\n    }\n\n    private interface StatisticsAggregator<T> {\n      void accept(StatisticsData data);\n      T aggregate();\n    }\n\n    private final String scheme;\n\n    /**\n     * rootData is data that doesn't belong to any thread, but will be added\n     * to the totals.  This is useful for making copies of Statistics objects,\n     * and for storing data that pertains to threads that have been garbage\n     * collected.  Protected by the Statistics lock.\n     */\n    private final StatisticsData rootData;\n\n    /**\n     * Thread-local data.\n     */\n    @SuppressWarnings(\"ThreadLocalNotStaticFinal\")\n    private final ThreadLocal<StatisticsData> threadData;\n\n    /**\n     * Set of all thread-local data areas.  Protected by the Statistics lock.\n     * The references to the statistics data are kept using weak references\n     * to the associated threads. Proper clean-up is performed by the cleaner\n     * thread when the threads are garbage collected.\n     */\n    private final Set<StatisticsDataReference> allData;\n\n    /**\n     * Global reference queue and a cleaner thread that manage statistics data\n     * references from all filesystem instances.\n     */\n    private static final ReferenceQueue<Thread> STATS_DATA_REF_QUEUE;\n    private static final Thread STATS_DATA_CLEANER;\n\n    static {\n      STATS_DATA_REF_QUEUE = new ReferenceQueue<>();\n      // start a single daemon cleaner thread\n      STATS_DATA_CLEANER = new Thread(new StatisticsDataReferenceCleaner());\n      STATS_DATA_CLEANER.\n          setName(StatisticsDataReferenceCleaner.class.getName());\n      STATS_DATA_CLEANER.setDaemon(true);\n      STATS_DATA_CLEANER.start();\n    }\n\n    public Statistics(String scheme) {\n      this.scheme = scheme;\n      this.rootData = new StatisticsData();\n      this.threadData = new ThreadLocal<>();\n      this.allData = new HashSet<>();\n    }\n\n    /**\n     * Copy constructor.\n     *\n     * @param other    The input Statistics object which is cloned.\n     */\n    public Statistics(Statistics other) {\n      this.scheme = other.scheme;\n      this.rootData = new StatisticsData();\n      other.visitAll(new StatisticsAggregator<Void>() {\n        @Override\n        public void accept(StatisticsData data) {\n          rootData.add(data);\n        }\n\n        public Void aggregate() {\n          return null;\n        }\n      });\n      this.threadData = new ThreadLocal<>();\n      this.allData = new HashSet<>();\n    }\n\n    /**\n     * A weak reference to a thread that also includes the data associated\n     * with that thread. On the thread being garbage collected, it is enqueued\n     * to the reference queue for clean-up.\n     */\n    private final class StatisticsDataReference extends WeakReference<Thread> {\n      private final StatisticsData data;\n\n      private StatisticsDataReference(StatisticsData data, Thread thread) {\n        super(thread, STATS_DATA_REF_QUEUE);\n        this.data = data;\n      }\n\n      public StatisticsData getData() {\n        return data;\n      }\n\n      /**\n       * Performs clean-up action when the associated thread is garbage\n       * collected.\n       */\n      public void cleanUp() {\n        // use the statistics lock for safety\n        synchronized (Statistics.this) {\n          /*\n           * If the thread that created this thread-local data no longer exists,\n           * remove the StatisticsData from our list and fold the values into\n           * rootData.\n           */\n          rootData.add(data);\n          allData.remove(this);\n        }\n      }\n    }\n\n    /**\n     * Background action to act on references being removed.\n     */\n    private static class StatisticsDataReferenceCleaner implements Runnable {\n      @Override\n      public void run() {\n        while (!Thread.interrupted()) {\n          try {\n            StatisticsDataReference ref =\n                (StatisticsDataReference)STATS_DATA_REF_QUEUE.remove();\n            ref.cleanUp();\n          } catch (InterruptedException ie) {\n            LOGGER.warn(\"Cleaner thread interrupted, will stop\", ie);\n            Thread.currentThread().interrupt();\n          } catch (Throwable th) {\n            LOGGER.warn(\"Exception in the cleaner thread but it will\" +\n                \" continue to run\", th);\n          }\n        }\n      }\n    }\n\n    /**\n     * Get or create the thread-local data associated with the current thread.\n     * @return statistics data.\n     */\n    public StatisticsData getThreadStatistics() {\n      StatisticsData data = threadData.get();\n      if (data == null) {\n        data = new StatisticsData();\n        threadData.set(data);\n        StatisticsDataReference ref =\n            new StatisticsDataReference(data, Thread.currentThread());\n        synchronized(this) {\n          allData.add(ref);\n        }\n      }\n      return data;\n    }\n\n    /**\n     * Increment the bytes read in the statistics.\n     * @param newBytes the additional bytes read\n     */\n    public void incrementBytesRead(long newBytes) {\n      getThreadStatistics().bytesRead += newBytes;\n    }\n\n    /**\n     * Increment the bytes written in the statistics.\n     * @param newBytes the additional bytes written\n     */\n    public void incrementBytesWritten(long newBytes) {\n      getThreadStatistics().bytesWritten += newBytes;\n    }\n\n    /**\n     * Increment the number of read operations.\n     * @param count number of read operations\n     */\n    public void incrementReadOps(int count) {\n      getThreadStatistics().readOps += count;\n    }\n\n    /**\n     * Increment the number of large read operations.\n     * @param count number of large read operations\n     */\n    public void incrementLargeReadOps(int count) {\n      getThreadStatistics().largeReadOps += count;\n    }\n\n    /**\n     * Increment the number of write operations.\n     * @param count number of write operations\n     */\n    public void incrementWriteOps(int count) {\n      getThreadStatistics().writeOps += count;\n    }\n\n    /**\n     * Increment the bytes read on erasure-coded files in the statistics.\n     * @param newBytes the additional bytes read\n     */\n    public void incrementBytesReadErasureCoded(long newBytes) {\n      getThreadStatistics().bytesReadErasureCoded += newBytes;\n    }\n\n    /**\n     * Increment the bytes read by the network distance in the statistics\n     * In the common network topology setup, distance value should be an even\n     * number such as 0, 2, 4, 6. To make it more general, we group distance\n     * by {1, 2}, {3, 4} and {5 and beyond} for accounting.\n     * @param distance the network distance\n     * @param newBytes the additional bytes read\n     */\n    public void incrementBytesReadByDistance(int distance, long newBytes) {\n      switch (distance) {\n      case 0:\n        getThreadStatistics().bytesReadLocalHost += newBytes;\n        break;\n      case 1:\n      case 2:\n        getThreadStatistics().bytesReadDistanceOfOneOrTwo += newBytes;\n        break;\n      case 3:\n      case 4:\n        getThreadStatistics().bytesReadDistanceOfThreeOrFour += newBytes;\n        break;\n      default:\n        getThreadStatistics().bytesReadDistanceOfFiveOrLarger += newBytes;\n        break;\n      }\n    }\n\n    /**\n     * Increment the time taken to read bytes from remote in the statistics.\n     * @param durationMS time taken in ms to read bytes from remote\n     */\n    public void increaseRemoteReadTime(final long durationMS) {\n      getThreadStatistics().remoteReadTimeMS += durationMS;\n    }\n\n    /**\n     * Apply the given aggregator to all StatisticsData objects associated with\n     * this Statistics object.\n     *\n     * For each StatisticsData object, we will call accept on the visitor.\n     * Finally, at the end, we will call aggregate to get the final total.\n     *\n     * @param         visitor to use.\n     * @return        The total.\n     */\n    private synchronized <T> T visitAll(StatisticsAggregator<T> visitor) {\n      visitor.accept(rootData);\n      for (StatisticsDataReference ref: allData) {\n        StatisticsData data = ref.getData();\n        visitor.accept(data);\n      }\n      return visitor.aggregate();\n    }\n\n    /**\n     * Get the total number of bytes read.\n     * @return the number of bytes\n     */\n    public long getBytesRead() {\n      return visitAll(new StatisticsAggregator<Long>() {\n        private long bytesRead = 0;\n\n        @Override\n        public void accept(StatisticsData data) {\n          bytesRead += data.bytesRead;\n        }\n\n        public Long aggregate() {\n          return bytesRead;\n        }\n      });\n    }\n\n    /**\n     * Get the total number of bytes written.\n     * @return the number of bytes\n     */\n    public long getBytesWritten() {\n      return visitAll(new StatisticsAggregator<Long>() {\n        private long bytesWritten = 0;\n\n        @Override\n        public void accept(StatisticsData data) {\n          bytesWritten += data.bytesWritten;\n        }\n\n        public Long aggregate() {\n          return bytesWritten;\n        }\n      });\n    }\n\n    /**\n     * Get the number of file system read operations such as list files.\n     * @return number of read operations\n     */\n    public int getReadOps() {\n      return visitAll(new StatisticsAggregator<Integer>() {\n        private int readOps = 0;\n\n        @Override\n        public void accept(StatisticsData data) {\n          readOps += data.readOps;\n          readOps += data.largeReadOps;\n        }\n\n        public Integer aggregate() {\n          return readOps;\n        }\n      });\n    }\n\n    /**\n     * Get the number of large file system read operations such as list files\n     * under a large directory.\n     * @return number of large read operations\n     */\n    public int getLargeReadOps() {\n      return visitAll(new StatisticsAggregator<Integer>() {\n        private int largeReadOps = 0;\n\n        @Override\n        public void accept(StatisticsData data) {\n          largeReadOps += data.largeReadOps;\n        }\n\n        public Integer aggregate() {\n          return largeReadOps;\n        }\n      });\n    }\n\n    /**\n     * Get the number of file system write operations such as create, append\n     * rename etc.\n     * @return number of write operations\n     */\n    public int getWriteOps() {\n      return visitAll(new StatisticsAggregator<Integer>() {\n        private int writeOps = 0;\n\n        @Override\n        public void accept(StatisticsData data) {\n          writeOps += data.writeOps;\n        }\n\n        public Integer aggregate() {\n          return writeOps;\n        }\n      });\n    }\n\n    /**\n     * In the common network topology setup, distance value should be an even\n     * number such as 0, 2, 4, 6. To make it more general, we group distance\n     * by {1, 2}, {3, 4} and {5 and beyond} for accounting. So if the caller\n     * ask for bytes read for distance 2, the function will return the value\n     * for group {1, 2}.\n     * @param distance the network distance\n     * @return the total number of bytes read by the network distance\n     */\n    public long getBytesReadByDistance(int distance) {\n      long bytesRead;\n      switch (distance) {\n      case 0:\n        bytesRead = getData().getBytesReadLocalHost();\n        break;\n      case 1:\n      case 2:\n        bytesRead = getData().getBytesReadDistanceOfOneOrTwo();\n        break;\n      case 3:\n      case 4:\n        bytesRead = getData().getBytesReadDistanceOfThreeOrFour();\n        break;\n      default:\n        bytesRead = getData().getBytesReadDistanceOfFiveOrLarger();\n        break;\n      }\n      return bytesRead;\n    }\n\n    /**\n     * Get total time taken in ms for bytes read from remote.\n     * @return time taken in ms for remote bytes read.\n     */\n    public long getRemoteReadTime() {\n      return visitAll(new StatisticsAggregator<Long>() {\n        private long remoteReadTimeMS = 0;\n\n        @Override\n        public void accept(StatisticsData data) {\n          remoteReadTimeMS += data.remoteReadTimeMS;\n        }\n\n        public Long aggregate() {\n          return remoteReadTimeMS;\n        }\n      });\n    }\n\n    /**\n     * Get all statistics data.\n     * MR or other frameworks can use the method to get all statistics at once.\n     * @return the StatisticsData\n     */\n    public StatisticsData getData() {\n      return visitAll(new StatisticsAggregator<StatisticsData>() {\n        private StatisticsData all = new StatisticsData();\n\n        @Override\n        public void accept(StatisticsData data) {\n          all.add(data);\n        }\n\n        public StatisticsData aggregate() {\n          return all;\n        }\n      });\n    }\n\n    /**\n     * Get the total number of bytes read on erasure-coded files.\n     * @return the number of bytes\n     */\n    public long getBytesReadErasureCoded() {\n      return visitAll(new StatisticsAggregator<Long>() {\n        private long bytesReadErasureCoded = 0;\n\n        @Override\n        public void accept(StatisticsData data) {\n          bytesReadErasureCoded += data.bytesReadErasureCoded;\n        }\n\n        public Long aggregate() {\n          return bytesReadErasureCoded;\n        }\n      });\n    }\n\n    @Override\n    public String toString() {\n      return visitAll(new StatisticsAggregator<String>() {\n        private StatisticsData total = new StatisticsData();\n\n        @Override\n        public void accept(StatisticsData data) {\n          total.add(data);\n        }\n\n        public String aggregate() {\n          return total.toString();\n        }\n      });\n    }\n\n    /**\n     * Resets all statistics to 0.\n     *\n     * In order to reset, we add up all the thread-local statistics data, and\n     * set rootData to the negative of that.\n     *\n     * This may seem like a counterintuitive way to reset the statistics.  Why\n     * can't we just zero out all the thread-local data?  Well, thread-local\n     * data can only be modified by the thread that owns it.  If we tried to\n     * modify the thread-local data from this thread, our modification might get\n     * interleaved with a read-modify-write operation done by the thread that\n     * owns the data.  That would result in our update getting lost.\n     *\n     * The approach used here avoids this problem because it only ever reads\n     * (not writes) the thread-local data.  Both reads and writes to rootData\n     * are done under the lock, so we're free to modify rootData from any thread\n     * that holds the lock.\n     */\n    public void reset() {\n      visitAll(new StatisticsAggregator<Void>() {\n        private StatisticsData total = new StatisticsData();\n\n        @Override\n        public void accept(StatisticsData data) {\n          total.add(data);\n        }\n\n        public Void aggregate() {\n          total.negate();\n          rootData.add(total);\n          return null;\n        }\n      });\n    }\n\n    /**\n     * Get the uri scheme associated with this statistics object.\n     * @return the schema associated with this set of statistics\n     */\n    public String getScheme() {\n      return scheme;\n    }\n\n    @VisibleForTesting\n    synchronized int getAllThreadLocalDataSize() {\n      return allData.size();\n    }\n  }\n\n  /**\n   * Get the Map of Statistics object indexed by URI Scheme.\n   * @return a Map having a key as URI scheme and value as Statistics object\n   * @deprecated use {@link #getGlobalStorageStatistics()}\n   */\n  @Deprecated\n  public static synchronized Map<String, Statistics> getStatistics() {\n    Map<String, Statistics> result = new HashMap<>();\n    for(Statistics stat: statisticsTable.values()) {\n      result.put(stat.getScheme(), stat);\n    }\n    return result;\n  }\n\n  /**\n   * Return the FileSystem classes that have Statistics.\n   * @deprecated use {@link #getGlobalStorageStatistics()}\n   * @return statistics lists.\n   */\n  @Deprecated\n  public static synchronized List<Statistics> getAllStatistics() {\n    return new ArrayList<>(statisticsTable.values());\n  }\n\n  /**\n   * Get the statistics for a particular file system.\n   * @param scheme scheme.\n   * @param cls the class to lookup\n   * @return a statistics object\n   * @deprecated use {@link #getGlobalStorageStatistics()}\n   */\n  @Deprecated\n  public static synchronized Statistics getStatistics(final String scheme,\n      Class<? extends FileSystem> cls) {\n    checkArgument(scheme != null,\n        \"No statistics is allowed for a file system with null scheme!\");\n    Statistics result = statisticsTable.get(cls);\n    if (result == null) {\n      final Statistics newStats = new Statistics(scheme);\n      statisticsTable.put(cls, newStats);\n      result = newStats;\n      GlobalStorageStatistics.INSTANCE.put(scheme,\n          new StorageStatisticsProvider() {\n            @Override\n            public StorageStatistics provide() {\n              return new FileSystemStorageStatistics(scheme, newStats);\n            }\n          });\n    }\n    return result;\n  }\n\n  /**\n   * Reset all statistics for all file systems.\n   */\n  public static synchronized void clearStatistics() {\n    GlobalStorageStatistics.INSTANCE.reset();\n  }\n\n  /**\n   * Print all statistics for all file systems to {@code System.out}\n   * @throws IOException If an I/O error occurred.\n   */\n  public static synchronized\n  void printStatistics() throws IOException {\n    for (Map.Entry<Class<? extends FileSystem>, Statistics> pair:\n            statisticsTable.entrySet()) {\n      System.out.println(\"  FileSystem \" + pair.getKey().getName() +\n                         \": \" + pair.getValue());\n    }\n  }\n\n  // Symlinks are temporarily disabled - see HADOOP-10020 and HADOOP-10052\n  private static boolean symlinksEnabled = false;\n\n  @VisibleForTesting\n  public static boolean areSymlinksEnabled() {\n    return symlinksEnabled;\n  }\n\n  @VisibleForTesting\n  public static void enableSymlinks() {\n    symlinksEnabled = true;\n  }\n\n  /**\n   * Get the StorageStatistics for this FileSystem object.  These statistics are\n   * per-instance.  They are not shared with any other FileSystem object.\n   *\n   * <p>This is a default method which is intended to be overridden by\n   * subclasses. The default implementation returns an empty storage statistics\n   * object.</p>\n   *\n   * @return    The StorageStatistics for this FileSystem instance.\n   *            Will never be null.\n   */\n  public StorageStatistics getStorageStatistics() {\n    return new EmptyStorageStatistics(getUri().toString());\n  }\n\n  /**\n   * Get the global storage statistics.\n   * @return global storage statistics.\n   */\n  public static GlobalStorageStatistics getGlobalStorageStatistics() {\n    return GlobalStorageStatistics.INSTANCE;\n  }\n\n  /**\n   * Create instance of the standard FSDataOutputStreamBuilder for the\n   * given filesystem and path.\n   * @param fileSystem owner\n   * @param path path to create\n   * @return a builder.\n   */\n  @InterfaceStability.Unstable\n  protected static FSDataOutputStreamBuilder createDataOutputStreamBuilder(\n      @Nonnull final FileSystem fileSystem,\n      @Nonnull final Path path) {\n    return new FileSystemDataOutputStreamBuilder(fileSystem, path);\n  }\n\n  /**\n   * Standard implementation of the FSDataOutputStreamBuilder; invokes\n   * create/createNonRecursive or Append depending upon the options.\n   */\n  private static final class FileSystemDataOutputStreamBuilder extends\n      FSDataOutputStreamBuilder<FSDataOutputStream,\n        FileSystemDataOutputStreamBuilder> {\n\n    /**\n     * Constructor.\n     * @param fileSystem owner\n     * @param p path to create\n     */\n    private FileSystemDataOutputStreamBuilder(FileSystem fileSystem, Path p) {\n      super(fileSystem, p);\n    }\n\n    @Override\n    public FSDataOutputStream build() throws IOException {\n      rejectUnknownMandatoryKeys(Collections.emptySet(),\n          \" for \" + getPath());\n      if (getFlags().contains(CreateFlag.CREATE) ||\n          getFlags().contains(CreateFlag.OVERWRITE)) {\n        if (isRecursive()) {\n          return getFS().create(getPath(), getPermission(), getFlags(),\n              getBufferSize(), getReplication(), getBlockSize(), getProgress(),\n              getChecksumOpt());\n        } else {\n          return getFS().createNonRecursive(getPath(), getPermission(),\n              getFlags(), getBufferSize(), getReplication(), getBlockSize(),\n              getProgress());\n        }\n      } else if (getFlags().contains(CreateFlag.APPEND)) {\n        return getFS().append(getPath(), getBufferSize(), getProgress());\n      }\n      throw new PathIOException(getPath().toString(),\n          \"Must specify either create, overwrite or append\");\n    }\n\n    @Override\n    public FileSystemDataOutputStreamBuilder getThisBuilder() {\n      return this;\n    }\n  }\n\n  /**\n   * Create a new FSDataOutputStreamBuilder for the file with path.\n   * Files are overwritten by default.\n   *\n   * @param path file path\n   * @return a FSDataOutputStreamBuilder object to build the file\n   *\n   * HADOOP-14384. Temporarily reduce the visibility of method before the\n   * builder interface becomes stable.\n   */\n  public FSDataOutputStreamBuilder createFile(Path path) {\n    return createDataOutputStreamBuilder(this, path)\n        .create().overwrite(true);\n  }\n\n  /**\n   * Create a Builder to append a file.\n   * @param path file path.\n   * @return a {@link FSDataOutputStreamBuilder} to build file append request.\n   */\n  public FSDataOutputStreamBuilder appendFile(Path path) {\n    return createDataOutputStreamBuilder(this, path).append();\n  }\n\n  /**\n   * Open a file for reading through a builder API.\n   * Ultimately calls {@link #open(Path, int)} unless a subclass\n   * executes the open command differently.\n   *\n   * The semantics of this call are therefore the same as that of\n   * {@link #open(Path, int)} with one special point: it is in\n   * {@code FSDataInputStreamBuilder.build()} in which the open operation\n   * takes place -it is there where all preconditions to the operation\n   * are checked.\n   * @param path file path\n   * @return a FSDataInputStreamBuilder object to build the input stream\n   * @throws IOException if some early checks cause IO failures.\n   * @throws UnsupportedOperationException if support is checked early.\n   */\n  @InterfaceStability.Unstable\n  public FutureDataInputStreamBuilder openFile(Path path)\n      throws IOException, UnsupportedOperationException {\n    return createDataInputStreamBuilder(this, path).getThisBuilder();\n  }\n\n  /**\n   * Open a file for reading through a builder API.\n   * Ultimately calls {@link #open(PathHandle, int)} unless a subclass\n   * executes the open command differently.\n   *\n   * If PathHandles are unsupported, this may fail in the\n   * {@code FSDataInputStreamBuilder.build()}  command,\n   * rather than in this {@code openFile()} operation.\n   * @param pathHandle path handle.\n   * @return a FSDataInputStreamBuilder object to build the input stream\n   * @throws IOException if some early checks cause IO failures.\n   * @throws UnsupportedOperationException if support is checked early.\n   */\n  @InterfaceStability.Unstable\n  public FutureDataInputStreamBuilder openFile(PathHandle pathHandle)\n      throws IOException, UnsupportedOperationException {\n    return createDataInputStreamBuilder(this, pathHandle)\n        .getThisBuilder();\n  }\n\n  /**\n   * Execute the actual open file operation.\n   *\n   * This is invoked from {@code FSDataInputStreamBuilder.build()}\n   * and from {@link DelegateToFileSystem} and is where\n   * the action of opening the file should begin.\n   *\n   * The base implementation performs a blocking\n   * call to {@link #open(Path, int)} in this call;\n   * the actual outcome is in the returned {@code CompletableFuture}.\n   * This avoids having to create some thread pool, while still\n   * setting up the expectation that the {@code get()} call\n   * is needed to evaluate the result.\n   * @param path path to the file\n   * @param parameters open file parameters from the builder.\n   * @return a future which will evaluate to the opened file.\n   * @throws IOException failure to resolve the link.\n   * @throws IllegalArgumentException unknown mandatory key\n   */\n  protected CompletableFuture<FSDataInputStream> openFileWithOptions(\n      final Path path,\n      final OpenFileParameters parameters) throws IOException {\n    AbstractFSBuilderImpl.rejectUnknownMandatoryKeys(\n        parameters.getMandatoryKeys(),\n        Options.OpenFileOptions.FS_OPTION_OPENFILE_STANDARD_OPTIONS,\n        \"for \" + path);\n    return LambdaUtils.eval(\n        new CompletableFuture<>(), () ->\n            open(path, parameters.getBufferSize()));\n  }\n\n  /**\n   * Execute the actual open file operation.\n   * The base implementation performs a blocking\n   * call to {@link #open(Path, int)} in this call;\n   * the actual outcome is in the returned {@code CompletableFuture}.\n   * This avoids having to create some thread pool, while still\n   * setting up the expectation that the {@code get()} call\n   * is needed to evaluate the result.\n   * @param pathHandle path to the file\n   * @param parameters open file parameters from the builder.\n   * @return a future which will evaluate to the opened file.\n   * @throws IOException failure to resolve the link.\n   * @throws IllegalArgumentException unknown mandatory key\n   * @throws UnsupportedOperationException PathHandles are not supported.\n   * This may be deferred until the future is evaluated.\n   */\n  protected CompletableFuture<FSDataInputStream> openFileWithOptions(\n      final PathHandle pathHandle,\n      final OpenFileParameters parameters) throws IOException {\n    AbstractFSBuilderImpl.rejectUnknownMandatoryKeys(\n        parameters.getMandatoryKeys(),\n        Options.OpenFileOptions.FS_OPTION_OPENFILE_STANDARD_OPTIONS, \"\");\n    CompletableFuture<FSDataInputStream> result = new CompletableFuture<>();\n    try {\n      result.complete(open(pathHandle, parameters.getBufferSize()));\n    } catch (UnsupportedOperationException tx) {\n      // fail fast here\n      throw tx;\n    } catch (Throwable tx) {\n      // fail lazily here to ensure callers expect all File IO operations to\n      // surface later\n      result.completeExceptionally(tx);\n    }\n    return result;\n  }\n\n  /**\n   * Helper method that throws an {@link UnsupportedOperationException} for the\n   * current {@link FileSystem} method being called.\n   */\n  private void methodNotSupported() {\n    // The order of the stacktrace elements is (from top to bottom):\n    //   - java.lang.Thread.getStackTrace\n    //   - org.apache.hadoop.fs.FileSystem.methodNotSupported\n    //   - <the FileSystem method>\n    // therefore, to find out the current method name, we use the element at\n    // index 2.\n    String name = Thread.currentThread().getStackTrace()[2].getMethodName();\n    throw new UnsupportedOperationException(getClass().getCanonicalName() +\n        \" does not support method \" + name);\n  }\n\n  /**\n   * Create instance of the standard {@link FSDataInputStreamBuilder} for the\n   * given filesystem and path.\n   * @param fileSystem owner\n   * @param path path to read\n   * @return a builder.\n   */\n  @InterfaceAudience.LimitedPrivate(\"Filesystems\")\n  @InterfaceStability.Unstable\n  protected static FSDataInputStreamBuilder createDataInputStreamBuilder(\n      @Nonnull final FileSystem fileSystem,\n      @Nonnull final Path path) {\n    return new FSDataInputStreamBuilder(fileSystem, path);\n  }\n\n  /**\n   * Create instance of the standard {@link FSDataInputStreamBuilder} for the\n   * given filesystem and path handle.\n   * @param fileSystem owner\n   * @param pathHandle path handle of file to open.\n   * @return a builder.\n   */\n  @InterfaceAudience.LimitedPrivate(\"Filesystems\")\n  @InterfaceStability.Unstable\n  protected static FSDataInputStreamBuilder createDataInputStreamBuilder(\n      @Nonnull final FileSystem fileSystem,\n      @Nonnull final PathHandle pathHandle) {\n    return new FSDataInputStreamBuilder(fileSystem, pathHandle);\n  }\n\n  /**\n   * Builder returned for {@code #openFile(Path)}\n   * and {@code #openFile(PathHandle)}.\n   */\n  private static class FSDataInputStreamBuilder\n      extends FutureDataInputStreamBuilderImpl\n      implements FutureDataInputStreamBuilder {\n\n    /**\n     * Path Constructor.\n     * @param fileSystem owner\n     * @param path path to open.\n     */\n    protected FSDataInputStreamBuilder(\n        @Nonnull final FileSystem fileSystem,\n        @Nonnull final Path path) {\n      super(fileSystem, path);\n    }\n\n    /**\n     * Construct from a path handle.\n     * @param fileSystem owner\n     * @param pathHandle path handle of file to open.\n     */\n    protected FSDataInputStreamBuilder(\n        @Nonnull final FileSystem fileSystem,\n        @Nonnull final PathHandle pathHandle) {\n      super(fileSystem, pathHandle);\n    }\n\n    /**\n     * Perform the open operation.\n     * Returns a future which, when get() or a chained completion\n     * operation is invoked, will supply the input stream of the file\n     * referenced by the path/path handle.\n     * @return a future to the input stream.\n     * @throws IOException early failure to open\n     * @throws UnsupportedOperationException if the specific operation\n     * is not supported.\n     * @throws IllegalArgumentException if the parameters are not valid.\n     */\n    @Override\n    public CompletableFuture<FSDataInputStream> build() throws IOException {\n      Optional<Path> optionalPath = getOptionalPath();\n      OpenFileParameters parameters = new OpenFileParameters()\n          .withMandatoryKeys(getMandatoryKeys())\n          .withOptionalKeys(getOptionalKeys())\n          .withOptions(getOptions())\n          .withStatus(super.getStatus())\n          .withBufferSize(\n              getOptions().getInt(FS_OPTION_OPENFILE_BUFFER_SIZE, getBufferSize()));\n      if(optionalPath.isPresent()) {\n        return getFS().openFileWithOptions(optionalPath.get(),\n            parameters);\n      } else {\n        return getFS().openFileWithOptions(getPathHandle(),\n            parameters);\n      }\n    }\n\n  }\n\n  /**\n   * Create a multipart uploader.\n   * @param basePath file path under which all files are uploaded\n   * @return a MultipartUploaderBuilder object to build the uploader\n   * @throws IOException if some early checks cause IO failures.\n   * @throws UnsupportedOperationException if support is checked early.\n   */\n  @InterfaceStability.Unstable\n  public MultipartUploaderBuilder createMultipartUploader(Path basePath)\n      throws IOException {\n    methodNotSupported();\n    return null;\n  }\n}\n"
    },
    {
      "file_path": "D:\\Disaster\\Codefield\\Code_Python\\Anti-patternRAG\\data\\AWD\\apache\\hadoop\\commit_1200\\1366\\before\\hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java",
      "chunk_type": "subType",
      "ast_subtree": "/**\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.hadoop.hdfs;\n\nimport org.apache.hadoop.fs.LeaseRecoverable;\nimport org.apache.hadoop.fs.SafeMode;\nimport org.apache.hadoop.hdfs.protocol.LocatedBlocks;\nimport org.apache.hadoop.security.AccessControlException;\nimport org.apache.hadoop.classification.VisibleForTesting;\nimport org.apache.hadoop.util.Preconditions;\nimport org.apache.hadoop.HadoopIllegalArgumentException;\nimport org.apache.hadoop.classification.InterfaceAudience;\nimport org.apache.hadoop.classification.InterfaceStability;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.crypto.key.KeyProvider;\nimport org.apache.hadoop.crypto.key.KeyProviderTokenIssuer;\nimport org.apache.hadoop.fs.BatchListingOperations;\nimport org.apache.hadoop.fs.BlockLocation;\nimport org.apache.hadoop.fs.BlockStoragePolicySpi;\nimport org.apache.hadoop.fs.CacheFlag;\nimport org.apache.hadoop.fs.CommonPathCapabilities;\nimport org.apache.hadoop.fs.ContentSummary;\nimport org.apache.hadoop.fs.CreateFlag;\nimport org.apache.hadoop.fs.FSDataInputStream;\nimport org.apache.hadoop.fs.FSDataOutputStream;\nimport org.apache.hadoop.fs.FSDataOutputStreamBuilder;\nimport org.apache.hadoop.fs.FSLinkResolver;\nimport org.apache.hadoop.fs.FileAlreadyExistsException;\nimport org.apache.hadoop.fs.FileChecksum;\nimport org.apache.hadoop.fs.FileEncryptionInfo;\nimport org.apache.hadoop.fs.FileStatus;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.FileSystemLinkResolver;\nimport org.apache.hadoop.fs.FsServerDefaults;\nimport org.apache.hadoop.fs.FsStatus;\nimport org.apache.hadoop.fs.GlobalStorageStatistics;\nimport org.apache.hadoop.fs.GlobalStorageStatistics.StorageStatisticsProvider;\nimport org.apache.hadoop.fs.InvalidPathHandleException;\nimport org.apache.hadoop.fs.PartialListing;\nimport org.apache.hadoop.fs.MultipartUploaderBuilder;\nimport org.apache.hadoop.fs.PathHandle;\nimport org.apache.hadoop.fs.LocatedFileStatus;\nimport org.apache.hadoop.fs.Options;\nimport org.apache.hadoop.fs.Options.ChecksumOpt;\nimport org.apache.hadoop.fs.Options.HandleOpt;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.fs.PathFilter;\nimport org.apache.hadoop.fs.QuotaUsage;\nimport org.apache.hadoop.fs.RemoteIterator;\nimport org.apache.hadoop.fs.SafeModeAction;\nimport org.apache.hadoop.fs.StorageStatistics;\nimport org.apache.hadoop.fs.StorageType;\nimport org.apache.hadoop.fs.UnresolvedLinkException;\nimport org.apache.hadoop.fs.UnsupportedFileSystemException;\nimport org.apache.hadoop.fs.XAttrSetFlag;\nimport org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder;\nimport org.apache.hadoop.fs.permission.AclEntry;\nimport org.apache.hadoop.fs.permission.AclStatus;\nimport org.apache.hadoop.fs.permission.FsAction;\nimport org.apache.hadoop.fs.permission.FsPermission;\nimport org.apache.hadoop.hdfs.DFSOpsCountStatistics.OpType;\nimport org.apache.hadoop.hdfs.client.DfsPathCapabilities;\nimport org.apache.hadoop.hdfs.client.HdfsClientConfigKeys;\nimport org.apache.hadoop.hdfs.client.HdfsDataOutputStream;\nimport org.apache.hadoop.hdfs.client.impl.CorruptFileBlockIterator;\nimport org.apache.hadoop.hdfs.protocol.AddErasureCodingPolicyResponse;\nimport org.apache.hadoop.hdfs.protocol.BatchedDirectoryListing;\nimport org.apache.hadoop.hdfs.protocol.BlockStoragePolicy;\nimport org.apache.hadoop.hdfs.protocol.CacheDirectiveEntry;\nimport org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo;\nimport org.apache.hadoop.hdfs.protocol.CachePoolEntry;\nimport org.apache.hadoop.hdfs.protocol.CachePoolInfo;\nimport org.apache.hadoop.hdfs.protocol.ClientProtocol;\nimport org.apache.hadoop.hdfs.protocol.DatanodeInfo;\nimport org.apache.hadoop.hdfs.protocol.DirectoryListing;\nimport org.apache.hadoop.hdfs.protocol.ECTopologyVerifierResult;\nimport org.apache.hadoop.hdfs.protocol.HdfsPartialListing;\nimport org.apache.hadoop.hdfs.protocol.EncryptionZone;\nimport org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy;\nimport org.apache.hadoop.hdfs.protocol.ErasureCodingPolicyInfo;\nimport org.apache.hadoop.hdfs.protocol.HdfsConstants;\nimport org.apache.hadoop.hdfs.protocol.HdfsConstants.DatanodeReportType;\nimport org.apache.hadoop.hdfs.protocol.HdfsConstants.ReencryptAction;\nimport org.apache.hadoop.hdfs.protocol.HdfsConstants.RollingUpgradeAction;\nimport org.apache.hadoop.hdfs.protocol.HdfsFileStatus;\nimport org.apache.hadoop.hdfs.protocol.HdfsPathHandle;\nimport org.apache.hadoop.hdfs.protocol.HdfsLocatedFileStatus;\nimport org.apache.hadoop.hdfs.protocol.OpenFileEntry;\nimport org.apache.hadoop.hdfs.protocol.OpenFilesIterator.OpenFilesType;\nimport org.apache.hadoop.hdfs.protocol.ZoneReencryptionStatus;\nimport org.apache.hadoop.hdfs.protocol.RollingUpgradeInfo;\nimport org.apache.hadoop.hdfs.protocol.SnapshotDiffReport;\nimport org.apache.hadoop.hdfs.protocol.SnapshotDiffReportListing;\nimport org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\nimport org.apache.hadoop.hdfs.protocol.SnapshotStatus;\nimport org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.net.NetUtils;\nimport org.apache.hadoop.security.token.Token;\nimport org.apache.hadoop.security.token.DelegationTokenIssuer;\nimport org.apache.hadoop.util.Lists;\nimport org.apache.hadoop.util.Progressable;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport javax.annotation.Nonnull;\nimport java.io.FileNotFoundException;\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.net.URI;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.EnumSet;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.NoSuchElementException;\nimport java.util.Optional;\nimport java.util.Set;\n\nimport static org.apache.hadoop.fs.impl.PathCapabilitiesSupport.validatePathCapabilityArgs;\n\n/****************************************************************\n * Implementation of the abstract FileSystem for the DFS system.\n * This object is the way end-user code interacts with a Hadoop\n * DistributedFileSystem.\n *\n *****************************************************************/\n@InterfaceAudience.LimitedPrivate({ \"MapReduce\", \"HBase\" })\n@InterfaceStability.Unstable\npublic class DistributedFileSystem extends FileSystem\n    implements KeyProviderTokenIssuer, BatchListingOperations, LeaseRecoverable, SafeMode {\n  private Path workingDir;\n  private URI uri;\n\n  DFSClient dfs;\n  private boolean verifyChecksum = true;\n\n  private DFSOpsCountStatistics storageStatistics;\n\n  static{\n    HdfsConfiguration.init();\n  }\n\n  public DistributedFileSystem() {\n  }\n\n  /**\n   * Return the protocol scheme for the FileSystem.\n   *\n   * @return <code>hdfs</code>\n   */\n  @Override\n  public String getScheme() {\n    return HdfsConstants.HDFS_URI_SCHEME;\n  }\n\n  @Override\n  public URI getUri() { return uri; }\n\n  @Override\n  public void initialize(URI uri, Configuration conf) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n\n    String host = uri.getHost();\n    if (host == null) {\n      throw new IOException(\"Incomplete HDFS URI, no host: \"+ uri);\n    }\n\n    initDFSClient(uri, conf);\n    this.uri = URI.create(uri.getScheme()+\"://\"+uri.getAuthority());\n    this.workingDir = getHomeDirectory();\n\n    storageStatistics = (DFSOpsCountStatistics) GlobalStorageStatistics.INSTANCE\n        .put(DFSOpsCountStatistics.NAME,\n          new StorageStatisticsProvider() {\n            @Override\n            public StorageStatistics provide() {\n              return new DFSOpsCountStatistics();\n            }\n          });\n  }\n\n  void initDFSClient(URI theUri, Configuration conf) throws IOException {\n    this.dfs =  new DFSClient(theUri, conf, statistics);\n  }\n\n  @Override\n  public Path getWorkingDirectory() {\n    return workingDir;\n  }\n\n  @Override\n  public long getDefaultBlockSize() {\n    return dfs.getConf().getDefaultBlockSize();\n  }\n\n  @Override\n  public short getDefaultReplication() {\n    return dfs.getConf().getDefaultReplication();\n  }\n\n  @Override\n  public void setWorkingDirectory(Path dir) {\n    String result = fixRelativePart(dir).toUri().getPath();\n    if (!DFSUtilClient.isValidName(result)) {\n      throw new IllegalArgumentException(\"Invalid DFS directory name \" +\n          result);\n    }\n    workingDir = fixRelativePart(dir);\n  }\n\n  @Override\n  public Path getHomeDirectory() {\n    return makeQualified(\n        new Path(DFSUtilClient.getHomeDirectory(getConf(), dfs.ugi)));\n  }\n\n  /**\n   * Returns the hedged read metrics object for this client.\n   *\n   * @return object of DFSHedgedReadMetrics\n   */\n  public DFSHedgedReadMetrics getHedgedReadMetrics() {\n    return dfs.getHedgedReadMetrics();\n  }\n\n  /**\n   * Checks that the passed URI belongs to this filesystem and returns\n   * just the path component. Expects a URI with an absolute path.\n   *\n   * @param file URI with absolute path\n   * @return path component of {file}\n   * @throws IllegalArgumentException if URI does not belong to this DFS\n   */\n  String getPathName(Path file) {\n    checkPath(file);\n    String result = file.toUri().getPath();\n    if (!DFSUtilClient.isValidName(result)) {\n      throw new IllegalArgumentException(\"Pathname \" + result + \" from \" +\n          file+\" is not a valid DFS filename.\");\n    }\n    return result;\n  }\n\n  @Override\n  public BlockLocation[] getFileBlockLocations(FileStatus file, long start,\n      long len) throws IOException {\n    if (file == null) {\n      return null;\n    }\n    return getFileBlockLocations(file.getPath(), start, len);\n  }\n\n  /**\n   * The returned BlockLocation will have different formats for replicated\n   * and erasure coded file.\n   * Please refer to\n   * {@link FileSystem#getFileBlockLocations(FileStatus, long, long)}\n   * for more details.\n   */\n  @Override\n  public BlockLocation[] getFileBlockLocations(Path p,\n      final long start, final long len) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_FILE_BLOCK_LOCATIONS);\n    final Path absF = fixRelativePart(p);\n    return new FileSystemLinkResolver<BlockLocation[]>() {\n      @Override\n      public BlockLocation[] doCall(final Path p) throws IOException {\n        return dfs.getBlockLocations(getPathName(p), start, len);\n      }\n      @Override\n      public BlockLocation[] next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.getFileBlockLocations(p, start, len);\n      }\n    }.resolve(this, absF);\n  }\n\n  @Override\n  public void setVerifyChecksum(boolean verifyChecksum) {\n    this.verifyChecksum = verifyChecksum;\n  }\n\n  /**\n   * Start the lease recovery of a file\n   *\n   * @param f a file\n   * @return true if the file is already closed\n   * @throws IOException if an error occurs\n   */\n  @Override\n  public boolean recoverLease(final Path f) throws IOException {\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p) throws IOException{\n        return dfs.recoverLease(getPathName(p));\n      }\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          return myDfs.recoverLease(p);\n        }\n        throw new UnsupportedOperationException(\"Cannot recoverLease through\" +\n            \" a symlink to a non-DistributedFileSystem: \" + f + \" -> \" + p);\n      }\n    }.resolve(this, absF);\n  }\n\n  @Override\n  public FSDataInputStream open(Path f, final int bufferSize)\n      throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.OPEN);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<FSDataInputStream>() {\n      @Override\n      public FSDataInputStream doCall(final Path p) throws IOException {\n        final DFSInputStream dfsis =\n            dfs.open(getPathName(p), bufferSize, verifyChecksum);\n        try {\n          return dfs.createWrappedInputStream(dfsis);\n        } catch (IOException ex){\n          dfsis.close();\n          throw ex;\n        }\n      }\n      @Override\n      public FSDataInputStream next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.open(p, bufferSize);\n      }\n    }.resolve(this, absF);\n  }\n\n  /**\n   * Opens an FSDataInputStream with the indicated file ID extracted from\n   * the {@link PathHandle}.\n   * @param fd Reference to entity in this FileSystem.\n   * @param bufferSize the size of the buffer to be used.\n   * @throws InvalidPathHandleException If PathHandle constraints do not hold\n   * @throws IOException On I/O errors\n   */\n  @Override\n  public FSDataInputStream open(PathHandle fd, int bufferSize)\n      throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.OPEN);\n    if (!(fd instanceof HdfsPathHandle)) {\n      fd = new HdfsPathHandle(fd.bytes());\n    }\n    HdfsPathHandle id = (HdfsPathHandle) fd;\n    final DFSInputStream dfsis = dfs.open(id, bufferSize, verifyChecksum);\n    return dfs.createWrappedInputStream(dfsis);\n  }\n\n  /**\n   * Create a handle to an HDFS file.\n   * @param st HdfsFileStatus instance from NameNode\n   * @param opts Standard handle arguments\n   * @throws IllegalArgumentException If the FileStatus instance refers to a\n   * directory, symlink, or another namesystem.\n   * @throws UnsupportedOperationException If opts are not specified or both\n   * data and location are not allowed to change.\n   * @return A handle to the file.\n   */\n  @Override\n  protected HdfsPathHandle createPathHandle(FileStatus st, HandleOpt... opts) {\n    if (!(st instanceof HdfsFileStatus)) {\n      throw new IllegalArgumentException(\"Invalid FileStatus \"\n          + st.getClass().getSimpleName());\n    }\n    if (st.isDirectory() || st.isSymlink()) {\n      throw new IllegalArgumentException(\"PathHandle only available for files\");\n    }\n    if (!getUri().getAuthority().equals(st.getPath().toUri().getAuthority())) {\n      throw new IllegalArgumentException(\"Wrong FileSystem: \" + st.getPath());\n    }\n    HandleOpt.Data data = HandleOpt.getOpt(HandleOpt.Data.class, opts)\n        .orElse(HandleOpt.changed(false));\n    HandleOpt.Location loc = HandleOpt.getOpt(HandleOpt.Location.class, opts)\n        .orElse(HandleOpt.moved(false));\n\n    HdfsFileStatus hst = (HdfsFileStatus) st;\n    final Path p;\n    final Optional<Long> inodeId;\n    if (loc.allowChange()) {\n      p = DFSUtilClient.makePathFromFileId(hst.getFileId());\n      inodeId = Optional.empty();\n    } else {\n      p = hst.getPath();\n      inodeId = Optional.of(hst.getFileId());\n    }\n    final Optional<Long> mtime = !data.allowChange()\n        ? Optional.of(hst.getModificationTime())\n        : Optional.empty();\n    return new HdfsPathHandle(getPathName(p), inodeId, mtime);\n  }\n\n  @Override\n  public FSDataOutputStream append(Path f, final int bufferSize,\n      final Progressable progress) throws IOException {\n    return append(f, EnumSet.of(CreateFlag.APPEND), bufferSize, progress);\n  }\n\n  @Override\n  public FSDataOutputStream append(Path f, final int bufferSize,\n      final Progressable progress, boolean appendToNewBlock) throws IOException {\n    EnumSet<CreateFlag> flag = EnumSet.of(CreateFlag.APPEND);\n    if (appendToNewBlock) {\n      flag.add(CreateFlag.NEW_BLOCK);\n    }\n    return append(f, flag, bufferSize, progress);\n  }\n\n  /**\n   * Append to an existing file (optional operation).\n   *\n   * @param f the existing file to be appended.\n   * @param flag Flags for the Append operation. CreateFlag.APPEND is mandatory\n   *          to be present.\n   * @param bufferSize the size of the buffer to be used.\n   * @param progress for reporting progress if it is not null.\n   * @return Returns instance of {@link FSDataOutputStream}\n   * @throws IOException\n   */\n  public FSDataOutputStream append(Path f, final EnumSet<CreateFlag> flag,\n      final int bufferSize, final Progressable progress) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.APPEND);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<FSDataOutputStream>() {\n      @Override\n      public FSDataOutputStream doCall(final Path p)\n          throws IOException {\n        return dfs.append(getPathName(p), bufferSize, flag, progress,\n            statistics);\n      }\n      @Override\n      public FSDataOutputStream next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.append(p, bufferSize);\n      }\n    }.resolve(this, absF);\n  }\n\n  /**\n   * Append to an existing file (optional operation).\n   *\n   * @param f the existing file to be appended.\n   * @param flag Flags for the Append operation. CreateFlag.APPEND is mandatory\n   *          to be present.\n   * @param bufferSize the size of the buffer to be used.\n   * @param progress for reporting progress if it is not null.\n   * @param favoredNodes Favored nodes for new blocks\n   * @return Returns instance of {@link FSDataOutputStream}\n   * @throws IOException\n   */\n  public FSDataOutputStream append(Path f, final EnumSet<CreateFlag> flag,\n      final int bufferSize, final Progressable progress,\n      final InetSocketAddress[] favoredNodes) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.APPEND);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<FSDataOutputStream>() {\n      @Override\n      public FSDataOutputStream doCall(final Path p)\n          throws IOException {\n        return dfs.append(getPathName(p), bufferSize, flag, progress,\n            statistics, favoredNodes);\n      }\n      @Override\n      public FSDataOutputStream next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.append(p, bufferSize);\n      }\n    }.resolve(this, absF);\n  }\n\n  @Override\n  public FSDataOutputStream create(Path f, FsPermission permission,\n      boolean overwrite, int bufferSize, short replication, long blockSize,\n      Progressable progress) throws IOException {\n    return this.create(f, permission,\n        overwrite ? EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE)\n            : EnumSet.of(CreateFlag.CREATE), bufferSize, replication,\n        blockSize, progress, null);\n  }\n\n  /**\n   * Same as\n   * {@link #create(Path, FsPermission, boolean, int, short, long,\n   * Progressable)} with the addition of favoredNodes that is a hint to\n   * where the namenode should place the file blocks.\n   * The favored nodes hint is not persisted in HDFS. Hence it may be honored\n   * at the creation time only. And with favored nodes, blocks will be pinned\n   * on the datanodes to prevent balancing move the block. HDFS could move the\n   * blocks during replication, to move the blocks from favored nodes. A value\n   * of null means no favored nodes for this create\n   */\n  public HdfsDataOutputStream create(final Path f,\n      final FsPermission permission, final boolean overwrite,\n      final int bufferSize, final short replication, final long blockSize,\n      final Progressable progress, final InetSocketAddress[] favoredNodes)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.CREATE);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<HdfsDataOutputStream>() {\n      @Override\n      public HdfsDataOutputStream doCall(final Path p) throws IOException {\n        final DFSOutputStream out = dfs.create(getPathName(f), permission,\n            overwrite ? EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE)\n                : EnumSet.of(CreateFlag.CREATE),\n            true, replication, blockSize, progress, bufferSize, null,\n            favoredNodes);\n        return safelyCreateWrappedOutputStream(out);\n      }\n      @Override\n      public HdfsDataOutputStream next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          return myDfs.create(p, permission, overwrite, bufferSize, replication,\n              blockSize, progress, favoredNodes);\n        }\n        throw new UnsupportedOperationException(\"Cannot create with\" +\n            \" favoredNodes through a symlink to a non-DistributedFileSystem: \"\n            + f + \" -> \" + p);\n      }\n    }.resolve(this, absF);\n  }\n\n  @Override\n  public FSDataOutputStream create(final Path f, final FsPermission permission,\n      final EnumSet<CreateFlag> cflags, final int bufferSize,\n      final short replication, final long blockSize,\n      final Progressable progress, final ChecksumOpt checksumOpt)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.CREATE);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<FSDataOutputStream>() {\n      @Override\n      public FSDataOutputStream doCall(final Path p) throws IOException {\n        final DFSOutputStream dfsos = dfs.create(getPathName(p), permission,\n            cflags, replication, blockSize, progress, bufferSize,\n            checksumOpt);\n        return safelyCreateWrappedOutputStream(dfsos);\n      }\n      @Override\n      public FSDataOutputStream next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.create(p, permission, cflags, bufferSize,\n            replication, blockSize, progress, checksumOpt);\n      }\n    }.resolve(this, absF);\n  }\n\n  /**\n   * Same as\n   * {@link #create(Path, FsPermission, EnumSet, int, short, long,\n   * Progressable, ChecksumOpt)} with a few additions. First, addition of\n   * favoredNodes that is a hint to where the namenode should place the file\n   * blocks. The favored nodes hint is not persisted in HDFS. Hence it may be\n   * honored at the creation time only. And with favored nodes, blocks will be\n   * pinned on the datanodes to prevent balancing move the block. HDFS could\n   * move the blocks during replication, to move the blocks from favored nodes.\n   * A value of null means no favored nodes for this create.\n   * The second addition is ecPolicyName. A non-null ecPolicyName specifies an\n   * explicit erasure coding policy for this file, overriding the inherited\n   * policy. A null ecPolicyName means the file will inherit its EC policy or\n   * replication policy from its ancestor (the default).\n   * ecPolicyName and SHOULD_REPLICATE CreateFlag are mutually exclusive. It's\n   * invalid to set both SHOULD_REPLICATE and a non-null ecPolicyName.\n   * The third addition is storagePolicyName. A non-null storage Policy\n   * specifies an explicit storage policy for this file, overriding the\n   * inherited policy.\n   *\n   */\n  private HdfsDataOutputStream create(final Path f,\n      final FsPermission permission, final EnumSet<CreateFlag> flag,\n      final int bufferSize, final short replication, final long blockSize,\n      final Progressable progress, final ChecksumOpt checksumOpt,\n      final InetSocketAddress[] favoredNodes, final String ecPolicyName,\n      final String storagePolicy)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.CREATE);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<HdfsDataOutputStream>() {\n      @Override\n      public HdfsDataOutputStream doCall(final Path p) throws IOException {\n        final DFSOutputStream out = dfs.create(getPathName(f), permission,\n            flag, true, replication, blockSize, progress, bufferSize,\n            checksumOpt, favoredNodes, ecPolicyName, storagePolicy);\n        return safelyCreateWrappedOutputStream(out);\n      }\n      @Override\n      public HdfsDataOutputStream next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          return myDfs.create(p, permission, flag, bufferSize, replication,\n              blockSize, progress, checksumOpt, favoredNodes, ecPolicyName,\n              storagePolicy);\n        }\n        throw new UnsupportedOperationException(\"Cannot create with\" +\n            \" favoredNodes through a symlink to a non-DistributedFileSystem: \"\n            + f + \" -> \" + p);\n      }\n    }.resolve(this, absF);\n  }\n\n  @Override\n  protected HdfsDataOutputStream primitiveCreate(Path f,\n      FsPermission absolutePermission, EnumSet<CreateFlag> flag, int bufferSize,\n      short replication, long blockSize, Progressable progress,\n      ChecksumOpt checksumOpt) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.PRIMITIVE_CREATE);\n    final DFSOutputStream dfsos = dfs.primitiveCreate(\n        getPathName(fixRelativePart(f)),\n        absolutePermission, flag, true, replication, blockSize,\n        progress, bufferSize, checksumOpt);\n    return safelyCreateWrappedOutputStream(dfsos);\n  }\n\n  /**\n   * Similar to {@link #create(Path, FsPermission, EnumSet, int, short, long,\n   * Progressable, ChecksumOpt, InetSocketAddress[], String, String)}, it provides a\n   * HDFS-specific version of {@link #createNonRecursive(Path, FsPermission,\n   * EnumSet, int, short, long, Progressable)} with a few additions.\n   *\n   * @see #create(Path, FsPermission, EnumSet, int, short, long, Progressable,\n   * ChecksumOpt, InetSocketAddress[], String, String) for the descriptions of\n   * additional parameters, i.e., favoredNodes, ecPolicyName and\n   * storagePolicyName.\n   */\n  private HdfsDataOutputStream createNonRecursive(final Path f,\n      final FsPermission permission, final EnumSet<CreateFlag> flag,\n      final int bufferSize, final short replication, final long blockSize,\n      final Progressable progress, final ChecksumOpt checksumOpt,\n      final InetSocketAddress[] favoredNodes, final String ecPolicyName,\n      final String storagePolicyName) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.CREATE);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<HdfsDataOutputStream>() {\n      @Override\n      public HdfsDataOutputStream doCall(final Path p) throws IOException {\n        final DFSOutputStream out = dfs.create(getPathName(f), permission,\n            flag, false, replication, blockSize, progress, bufferSize,\n            checksumOpt, favoredNodes, ecPolicyName, storagePolicyName);\n        return safelyCreateWrappedOutputStream(out);\n      }\n      @Override\n      public HdfsDataOutputStream next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          return myDfs.createNonRecursive(p, permission, flag, bufferSize,\n              replication, blockSize, progress, checksumOpt, favoredNodes,\n              ecPolicyName, storagePolicyName);\n        }\n        throw new UnsupportedOperationException(\"Cannot create with\" +\n            \" favoredNodes through a symlink to a non-DistributedFileSystem: \"\n            + f + \" -> \" + p);\n      }\n    }.resolve(this, absF);\n  }\n\n  /**\n   * Same as create(), except fails if parent directory doesn't already exist.\n   */\n  @Override\n  public FSDataOutputStream createNonRecursive(final Path f,\n      final FsPermission permission, final EnumSet<CreateFlag> flag,\n      final int bufferSize, final short replication, final long blockSize,\n      final Progressable progress) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.CREATE_NON_RECURSIVE);\n    if (flag.contains(CreateFlag.OVERWRITE)) {\n      flag.add(CreateFlag.CREATE);\n    }\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<FSDataOutputStream>() {\n      @Override\n      public FSDataOutputStream doCall(final Path p) throws IOException {\n        final DFSOutputStream dfsos = dfs.create(getPathName(p), permission,\n            flag, false, replication, blockSize, progress, bufferSize, null);\n        return safelyCreateWrappedOutputStream(dfsos);\n      }\n\n      @Override\n      public FSDataOutputStream next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.createNonRecursive(p, permission, flag, bufferSize,\n            replication, blockSize, progress);\n      }\n    }.resolve(this, absF);\n  }\n\n  // Private helper to ensure the wrapped inner stream is closed safely\n  // upon IOException throw during wrap.\n  // Assuming the caller owns the inner stream which needs to be closed upon\n  // wrap failure.\n  private HdfsDataOutputStream safelyCreateWrappedOutputStream(\n      DFSOutputStream dfsos) throws IOException {\n    try {\n      return dfs.createWrappedOutputStream(dfsos, statistics);\n    } catch (IOException ex) {\n      dfsos.close();\n      throw ex;\n    }\n  }\n\n  @Override\n  public boolean setReplication(Path src, final short replication)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.SET_REPLICATION);\n    Path absF = fixRelativePart(src);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p) throws IOException {\n        return dfs.setReplication(getPathName(p), replication);\n      }\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.setReplication(p, replication);\n      }\n    }.resolve(this, absF);\n  }\n\n  /**\n   * Set the source path to the specified storage policy.\n   *\n   * @param src The source path referring to either a directory or a file.\n   * @param policyName The name of the storage policy.\n   */\n  @Override\n  public void setStoragePolicy(final Path src, final String policyName)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.SET_STORAGE_POLICY);\n    Path absF = fixRelativePart(src);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.setStoragePolicy(getPathName(p), policyName);\n        return null;\n      }\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        fs.setStoragePolicy(p, policyName);\n        return null;\n      }\n    }.resolve(this, absF);\n  }\n\n  @Override\n  public void unsetStoragePolicy(final Path src)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.UNSET_STORAGE_POLICY);\n    Path absF = fixRelativePart(src);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.unsetStoragePolicy(getPathName(p));\n        return null;\n      }\n      @Override\n      public Void next(final FileSystem fs, final Path p) throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          ((DistributedFileSystem) fs).unsetStoragePolicy(p);\n          return null;\n        } else {\n          throw new UnsupportedOperationException(\n              \"Cannot perform unsetStoragePolicy on a \"\n                  + \"non-DistributedFileSystem: \" + src + \" -> \" + p);\n        }\n      }\n    }.resolve(this, absF);\n  }\n\n  @Override\n  public BlockStoragePolicySpi getStoragePolicy(Path path) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_STORAGE_POLICY);\n    Path absF = fixRelativePart(path);\n\n    return new FileSystemLinkResolver<BlockStoragePolicySpi>() {\n      @Override\n      public BlockStoragePolicySpi doCall(final Path p) throws IOException {\n        return getClient().getStoragePolicy(getPathName(p));\n      }\n\n      @Override\n      public BlockStoragePolicySpi next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.getStoragePolicy(p);\n      }\n    }.resolve(this, absF);\n  }\n\n  @Override\n  public Collection<BlockStoragePolicy> getAllStoragePolicies()\n      throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_STORAGE_POLICIES);\n    return Arrays.asList(dfs.getStoragePolicies());\n  }\n\n  /**\n   * Returns number of bytes within blocks with future generation stamp. These\n   * are bytes that will be potentially deleted if we forceExit from safe mode.\n   *\n   * @return number of bytes.\n   */\n  public long getBytesWithFutureGenerationStamps() throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_BYTES_WITH_FUTURE_GS);\n    return dfs.getBytesInFutureBlocks();\n  }\n\n  /**\n   * Deprecated. Prefer {@link FileSystem#getAllStoragePolicies()}\n   * @throws IOException\n   */\n  @Deprecated\n  public BlockStoragePolicy[] getStoragePolicies() throws IOException {\n    return getAllStoragePolicies().toArray(new BlockStoragePolicy[0]);\n  }\n\n  /**\n   * Move blocks from srcs to trg and delete srcs afterwards.\n   * The file block sizes must be the same.\n   *\n   * @param trg existing file to append to\n   * @param psrcs list of files (same block size, same replication)\n   * @throws IOException\n   */\n  @Override\n  public void concat(Path trg, Path [] psrcs) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.CONCAT);\n    // Make target absolute\n    Path absF = fixRelativePart(trg);\n    // Make all srcs absolute\n    Path[] srcs = new Path[psrcs.length];\n    for (int i=0; i<psrcs.length; i++) {\n      srcs[i] = fixRelativePart(psrcs[i]);\n    }\n    // Try the concat without resolving any links\n    String[] srcsStr = new String[psrcs.length];\n    try {\n      for (int i=0; i<psrcs.length; i++) {\n        srcsStr[i] = getPathName(srcs[i]);\n      }\n      dfs.concat(getPathName(absF), srcsStr);\n    } catch (UnresolvedLinkException e) {\n      // Exception could be from trg or any src.\n      // Fully resolve trg and srcs. Fail if any of them are a symlink.\n      FileStatus stat = getFileLinkStatus(absF);\n      if (stat.isSymlink()) {\n        throw new IOException(\"Cannot concat with a symlink target: \"\n            + trg + \" -> \" + stat.getPath());\n      }\n      absF = fixRelativePart(stat.getPath());\n      for (int i=0; i<psrcs.length; i++) {\n        stat = getFileLinkStatus(srcs[i]);\n        if (stat.isSymlink()) {\n          throw new IOException(\"Cannot concat with a symlink src: \"\n              + psrcs[i] + \" -> \" + stat.getPath());\n        }\n        srcs[i] = fixRelativePart(stat.getPath());\n      }\n      // Try concat again. Can still race with another symlink.\n      for (int i=0; i<psrcs.length; i++) {\n        srcsStr[i] = getPathName(srcs[i]);\n      }\n      dfs.concat(getPathName(absF), srcsStr);\n    }\n  }\n\n\n  @SuppressWarnings(\"deprecation\")\n  @Override\n  public boolean rename(Path src, Path dst) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.RENAME);\n\n    final Path absSrc = fixRelativePart(src);\n    final Path absDst = fixRelativePart(dst);\n\n    // Try the rename without resolving first\n    try {\n      return dfs.rename(getPathName(absSrc), getPathName(absDst));\n    } catch (UnresolvedLinkException e) {\n      // Fully resolve the source\n      final Path source = getFileLinkStatus(absSrc).getPath();\n      // Keep trying to resolve the destination\n      return new FileSystemLinkResolver<Boolean>() {\n        @Override\n        public Boolean doCall(final Path p) throws IOException {\n          return dfs.rename(getPathName(source), getPathName(p));\n        }\n        @Override\n        public Boolean next(final FileSystem fs, final Path p)\n            throws IOException {\n          // Should just throw an error in FileSystem#checkPath\n          return doCall(p);\n        }\n      }.resolve(this, absDst);\n    }\n  }\n\n  /**\n   * This rename operation is guaranteed to be atomic.\n   */\n  @SuppressWarnings(\"deprecation\")\n  @Override\n  public void rename(Path src, Path dst, final Options.Rename... options)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.RENAME);\n    final Path absSrc = fixRelativePart(src);\n    final Path absDst = fixRelativePart(dst);\n    // Try the rename without resolving first\n    try {\n      dfs.rename(getPathName(absSrc), getPathName(absDst), options);\n    } catch (UnresolvedLinkException e) {\n      // Fully resolve the source\n      final Path source = getFileLinkStatus(absSrc).getPath();\n      // Keep trying to resolve the destination\n      new FileSystemLinkResolver<Void>() {\n        @Override\n        public Void doCall(final Path p) throws IOException {\n          dfs.rename(getPathName(source), getPathName(p), options);\n          return null;\n        }\n        @Override\n        public Void next(final FileSystem fs, final Path p)\n            throws IOException {\n          // Should just throw an error in FileSystem#checkPath\n          return doCall(p);\n        }\n      }.resolve(this, absDst);\n    }\n  }\n\n  @Override\n  public boolean truncate(Path f, final long newLength) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.TRUNCATE);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p) throws IOException {\n        return dfs.truncate(getPathName(p), newLength);\n      }\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.truncate(p, newLength);\n      }\n    }.resolve(this, absF);\n  }\n\n  @Override\n  public boolean delete(Path f, final boolean recursive) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.DELETE);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p) throws IOException {\n        return dfs.delete(getPathName(p), recursive);\n      }\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.delete(p, recursive);\n      }\n    }.resolve(this, absF);\n  }\n\n  @Override\n  public ContentSummary getContentSummary(Path f) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_CONTENT_SUMMARY);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<ContentSummary>() {\n      @Override\n      public ContentSummary doCall(final Path p) throws IOException {\n        return dfs.getContentSummary(getPathName(p));\n      }\n      @Override\n      public ContentSummary next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.getContentSummary(p);\n      }\n    }.resolve(this, absF);\n  }\n\n  @Override\n  public QuotaUsage getQuotaUsage(Path f) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_QUOTA_USAGE);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<QuotaUsage>() {\n      @Override\n      public QuotaUsage doCall(final Path p)\n              throws IOException, UnresolvedLinkException {\n        return dfs.getQuotaUsage(getPathName(p));\n      }\n      @Override\n      public QuotaUsage next(final FileSystem fs, final Path p)\n              throws IOException {\n        return fs.getQuotaUsage(p);\n      }\n    }.resolve(this, absF);\n  }\n\n  /** Set a directory's quotas\n   * @see org.apache.hadoop.hdfs.protocol.ClientProtocol#setQuota(String,\n   * long, long, StorageType)\n   */\n  @Override\n  public void setQuota(Path src, final long namespaceQuota,\n      final long storagespaceQuota) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.SET_QUOTA_USAGE);\n    Path absF = fixRelativePart(src);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.setQuota(getPathName(p), namespaceQuota, storagespaceQuota);\n        return null;\n      }\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        // setQuota is not defined in FileSystem, so we only can resolve\n        // within this DFS\n        return doCall(p);\n      }\n    }.resolve(this, absF);\n  }\n\n  /**\n   * Set the per type storage quota of a directory.\n   *\n   * @param src target directory whose quota is to be modified.\n   * @param type storage type of the specific storage type quota to be modified.\n   * @param quota value of the specific storage type quota to be modified.\n   * Maybe {@link HdfsConstants#QUOTA_RESET} to clear quota by storage type.\n   */\n  @Override\n  public void setQuotaByStorageType(Path src, final StorageType type,\n      final long quota)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.SET_QUOTA_BYTSTORAGEYPE);\n    Path absF = fixRelativePart(src);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.setQuotaByStorageType(getPathName(p), type, quota);\n        return null;\n      }\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        // setQuotaByStorageType is not defined in FileSystem, so we only can\n        // resolve within this DFS\n        return doCall(p);\n      }\n    }.resolve(this, absF);\n  }\n\n  private FileStatus[] listStatusInternal(Path p) throws IOException {\n    String src = getPathName(p);\n\n    // fetch the first batch of entries in the directory\n    DirectoryListing thisListing = dfs.listPaths(\n        src, HdfsFileStatus.EMPTY_NAME);\n\n    if (thisListing == null) { // the directory does not exist\n      throw new FileNotFoundException(\"File \" + p + \" does not exist.\");\n    }\n\n    HdfsFileStatus[] partialListing = thisListing.getPartialListing();\n    if (!thisListing.hasMore()) { // got all entries of the directory\n      FileStatus[] stats = new FileStatus[partialListing.length];\n      for (int i = 0; i < partialListing.length; i++) {\n        stats[i] = partialListing[i].makeQualified(getUri(), p);\n      }\n      statistics.incrementReadOps(1);\n      storageStatistics.incrementOpCounter(OpType.LIST_STATUS);\n      return stats;\n    }\n\n    // The directory size is too big that it needs to fetch more\n    // estimate the total number of entries in the directory\n    int totalNumEntries =\n        partialListing.length + thisListing.getRemainingEntries();\n    ArrayList<FileStatus> listing =\n        new ArrayList<>(totalNumEntries);\n    // add the first batch of entries to the array list\n    for (HdfsFileStatus fileStatus : partialListing) {\n      listing.add(fileStatus.makeQualified(getUri(), p));\n    }\n    statistics.incrementLargeReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.LIST_STATUS);\n\n    // now fetch more entries\n    do {\n      thisListing = dfs.listPaths(src, thisListing.getLastName());\n\n      if (thisListing == null) { // the directory is deleted\n        throw new FileNotFoundException(\"File \" + p + \" does not exist.\");\n      }\n\n      partialListing = thisListing.getPartialListing();\n      for (HdfsFileStatus fileStatus : partialListing) {\n        listing.add(fileStatus.makeQualified(getUri(), p));\n      }\n      statistics.incrementLargeReadOps(1);\n      storageStatistics.incrementOpCounter(OpType.LIST_STATUS);\n    } while (thisListing.hasMore());\n\n    return listing.toArray(new FileStatus[listing.size()]);\n  }\n\n  /**\n   * List all the entries of a directory\n   *\n   * Note that this operation is not atomic for a large directory. The entries\n   * of a directory may be fetched from NameNode multiple times. It only\n   * guarantees that each name occurs once if a directory undergoes changes\n   * between the calls.\n   *\n   * If any of the the immediate children of the given path f is a symlink, the\n   * returned FileStatus object of that children would be represented as a\n   * symlink. It will not be resolved to the target path and will not get the\n   * target path FileStatus object. The target path will be available via\n   * getSymlink on that children's FileStatus object. Since it represents as\n   * symlink, isDirectory on that children's FileStatus will return false.\n   *\n   * If you want to get the FileStatus of target path for that children, you may\n   * want to use GetFileStatus API with that children's symlink path. Please see\n   * {@link DistributedFileSystem#getFileStatus(Path f)}\n   */\n  @Override\n  public FileStatus[] listStatus(Path p) throws IOException {\n    Path absF = fixRelativePart(p);\n    return new FileSystemLinkResolver<FileStatus[]>() {\n      @Override\n      public FileStatus[] doCall(final Path p) throws IOException {\n        return listStatusInternal(p);\n      }\n      @Override\n      public FileStatus[] next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.listStatus(p);\n      }\n    }.resolve(this, absF);\n  }\n\n  /**\n   * The BlockLocation of returned LocatedFileStatus will have different\n   * formats for replicated and erasure coded file.\n   * Please refer to\n   * {@link FileSystem#getFileBlockLocations(FileStatus, long, long)} for\n   * more details.\n   */\n  @Override\n  protected RemoteIterator<LocatedFileStatus> listLocatedStatus(final Path p,\n      final PathFilter filter)\n      throws IOException {\n    Path absF = fixRelativePart(p);\n    return new FileSystemLinkResolver<RemoteIterator<LocatedFileStatus>>() {\n      @Override\n      public RemoteIterator<LocatedFileStatus> doCall(final Path p)\n          throws IOException {\n        return new DirListingIterator<>(p, filter, true);\n      }\n\n      @Override\n      public RemoteIterator<LocatedFileStatus> next(final FileSystem fs,\n          final Path p) throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          return ((DistributedFileSystem)fs).listLocatedStatus(p, filter);\n        }\n        // symlink resolution for this methos does not work cross file systems\n        // because it is a protected method.\n        throw new IOException(\"Link resolution does not work with multiple \" +\n            \"file systems for listLocatedStatus(): \" + p);\n      }\n    }.resolve(this, absF);\n  }\n\n\n  /**\n   * Returns a remote iterator so that followup calls are made on demand\n   * while consuming the entries. This reduces memory consumption during\n   * listing of a large directory.\n   *\n   * @param p target path\n   * @return remote iterator\n   */\n  @Override\n  public RemoteIterator<FileStatus> listStatusIterator(final Path p)\n      throws IOException {\n    Path absF = fixRelativePart(p);\n    return new FileSystemLinkResolver<RemoteIterator<FileStatus>>() {\n      @Override\n      public RemoteIterator<FileStatus> doCall(final Path p)\n          throws IOException {\n        return new DirListingIterator<>(p, false);\n      }\n\n      @Override\n      public RemoteIterator<FileStatus> next(final FileSystem fs, final Path p)\n          throws IOException {\n        return ((DistributedFileSystem)fs).listStatusIterator(p);\n      }\n    }.resolve(this, absF);\n\n  }\n\n  /**\n   * This class defines an iterator that returns\n   * the file status of each file/subdirectory of a directory\n   *\n   * if needLocation, status contains block location if it is a file\n   * throws a RuntimeException with the error as its cause.\n   *\n   * @param <T> the type of the file status\n   */\n  private class  DirListingIterator<T extends FileStatus>\n      implements RemoteIterator<T> {\n    private DirectoryListing thisListing;\n    private int i;\n    private Path p;\n    private String src;\n    private T curStat = null;\n    private PathFilter filter;\n    private boolean needLocation;\n\n    private DirListingIterator(Path p, PathFilter filter,\n        boolean needLocation) throws IOException {\n      this.p = p;\n      this.src = getPathName(p);\n      this.filter = filter;\n      this.needLocation = needLocation;\n      // fetch the first batch of entries in the directory\n      thisListing = dfs.listPaths(src, HdfsFileStatus.EMPTY_NAME,\n          needLocation);\n      statistics.incrementReadOps(1);\n      if (needLocation) {\n        storageStatistics.incrementOpCounter(OpType.LIST_LOCATED_STATUS);\n      } else {\n        storageStatistics.incrementOpCounter(OpType.LIST_STATUS);\n      }\n      if (thisListing == null) { // the directory does not exist\n        throw new FileNotFoundException(\"File \" + p + \" does not exist.\");\n      }\n      i = 0;\n    }\n\n    private DirListingIterator(Path p, boolean needLocation)\n        throws IOException {\n      this(p, null, needLocation);\n    }\n\n    @Override\n    @SuppressWarnings(\"unchecked\")\n    public boolean hasNext() throws IOException {\n      while (curStat == null && hasNextNoFilter()) {\n        T next;\n        HdfsFileStatus fileStat = thisListing.getPartialListing()[i++];\n        if (needLocation) {\n          next = (T)((HdfsLocatedFileStatus)fileStat)\n              .makeQualifiedLocated(getUri(), p);\n        } else {\n          next = (T)fileStat.makeQualified(getUri(), p);\n        }\n        // apply filter if not null\n        if (filter == null || filter.accept(next.getPath())) {\n          curStat = next;\n        }\n      }\n      return curStat != null;\n    }\n\n    /** Check if there is a next item before applying the given filter */\n    private boolean hasNextNoFilter() throws IOException {\n      if (thisListing == null) {\n        return false;\n      }\n      if (i >= thisListing.getPartialListing().length\n          && thisListing.hasMore()) {\n        // current listing is exhausted & fetch a new listing\n        thisListing = dfs.listPaths(src, thisListing.getLastName(),\n            needLocation);\n        statistics.incrementReadOps(1);\n        if (thisListing == null) {\n          throw new FileNotFoundException(\"File \" + p + \" does not exist.\");\n        }\n        i = 0;\n      }\n      return (i < thisListing.getPartialListing().length);\n    }\n\n    @Override\n    public T next() throws IOException {\n      if (hasNext()) {\n        T tmp = curStat;\n        curStat = null;\n        return tmp;\n      }\n      throw new java.util.NoSuchElementException(\"No more entry in \" + p);\n    }\n  }\n\n  @Override\n  public RemoteIterator<PartialListing<FileStatus>> batchedListStatusIterator(\n      final List<Path> paths) throws IOException {\n    List<Path> absPaths = Lists.newArrayListWithCapacity(paths.size());\n    for (Path p : paths) {\n      absPaths.add(fixRelativePart(p));\n    }\n    return new PartialListingIterator<>(absPaths, false);\n  }\n\n  @Override\n  public RemoteIterator<PartialListing<LocatedFileStatus>> batchedListLocatedStatusIterator(\n      final List<Path> paths) throws IOException {\n    List<Path> absPaths = Lists.newArrayListWithCapacity(paths.size());\n    for (Path p : paths) {\n      absPaths.add(fixRelativePart(p));\n    }\n    return new PartialListingIterator<>(absPaths, true);\n  }\n\n  private static final Logger LBI_LOG =\n      LoggerFactory.getLogger(PartialListingIterator.class);\n\n  private class PartialListingIterator<T extends FileStatus>\n      implements RemoteIterator<PartialListing<T>> {\n\n    private List<Path> paths;\n    private String[] srcs;\n    private boolean needLocation;\n    private BatchedDirectoryListing batchedListing;\n    private int listingIdx = 0;\n\n    PartialListingIterator(List<Path> paths, boolean needLocation)\n        throws IOException {\n      this.paths = paths;\n      this.srcs = new String[paths.size()];\n      for (int i = 0; i < paths.size(); i++) {\n        this.srcs[i] = getPathName(paths.get(i));\n      }\n      this.needLocation = needLocation;\n\n      // Do the first listing\n      statistics.incrementReadOps(1);\n      storageStatistics.incrementOpCounter(OpType.LIST_LOCATED_STATUS);\n      batchedListing = dfs.batchedListPaths(\n          srcs, HdfsFileStatus.EMPTY_NAME, needLocation);\n      LBI_LOG.trace(\"Got batchedListing: {}\", batchedListing);\n      if (batchedListing == null) { // the directory does not exist\n        throw new FileNotFoundException(\"One or more paths do not exist.\");\n      }\n    }\n\n    @Override\n    public boolean hasNext() throws IOException {\n      if (batchedListing == null) {\n        return false;\n      }\n      // If we're done with the current batch, try to get the next batch\n      if (listingIdx >= batchedListing.getListings().length) {\n        if (!batchedListing.hasMore()) {\n          LBI_LOG.trace(\"No more elements\");\n          return false;\n        }\n        batchedListing = dfs.batchedListPaths(\n            srcs, batchedListing.getStartAfter(), needLocation);\n        LBI_LOG.trace(\"Got batchedListing: {}\", batchedListing);\n        listingIdx = 0;\n      }\n      return listingIdx < batchedListing.getListings().length;\n    }\n\n    @Override\n    @SuppressWarnings(\"unchecked\")\n    public PartialListing<T> next() throws IOException {\n      if (!hasNext()) {\n        throw new NoSuchElementException(\"No more entries\");\n      }\n      HdfsPartialListing listing = batchedListing.getListings()[listingIdx];\n      listingIdx++;\n\n      Path parent = paths.get(listing.getParentIdx());\n\n      if (listing.getException() != null) {\n        return new PartialListing<>(parent, listing.getException());\n      }\n\n      // Qualify paths for the client.\n      List<HdfsFileStatus> statuses = listing.getPartialListing();\n      List<T> qualifiedStatuses =\n          Lists.newArrayListWithCapacity(statuses.size());\n\n      for (HdfsFileStatus status : statuses) {\n        if (needLocation) {\n          qualifiedStatuses.add((T)((HdfsLocatedFileStatus) status)\n              .makeQualifiedLocated(getUri(), parent));\n        } else {\n          qualifiedStatuses.add((T)status.makeQualified(getUri(), parent));\n        }\n      }\n\n      return new PartialListing<>(parent, qualifiedStatuses);\n    }\n  }\n\n  /**\n   * Create a directory, only when the parent directories exist.\n   *\n   * See {@link FsPermission#applyUMask(FsPermission)} for details of how\n   * the permission is applied.\n   *\n   * @param f           The path to create\n   * @param permission  The permission.  See FsPermission#applyUMask for\n   *                    details about how this is used to calculate the\n   *                    effective permission.\n   */\n  public boolean mkdir(Path f, FsPermission permission) throws IOException {\n    return mkdirsInternal(f, permission, false);\n  }\n\n  /**\n   * Create a directory and its parent directories.\n   *\n   * See {@link FsPermission#applyUMask(FsPermission)} for details of how\n   * the permission is applied.\n   *\n   * @param f           The path to create\n   * @param permission  The permission.  See FsPermission#applyUMask for\n   *                    details about how this is used to calculate the\n   *                    effective permission.\n   */\n  @Override\n  public boolean mkdirs(Path f, FsPermission permission) throws IOException {\n    return mkdirsInternal(f, permission, true);\n  }\n\n  private boolean mkdirsInternal(Path f, final FsPermission permission,\n      final boolean createParent) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.MKDIRS);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p) throws IOException {\n        return dfs.mkdirs(getPathName(p), permission, createParent);\n      }\n\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        // FileSystem doesn't have a non-recursive mkdir() method\n        // Best we can do is error out\n        if (!createParent) {\n          throw new IOException(\"FileSystem does not support non-recursive\"\n              + \"mkdir\");\n        }\n        return fs.mkdirs(p, permission);\n      }\n    }.resolve(this, absF);\n  }\n\n  @SuppressWarnings(\"deprecation\")\n  @Override\n  protected boolean primitiveMkdir(Path f, FsPermission absolutePermission)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.PRIMITIVE_MKDIR);\n    return dfs.primitiveMkdir(getPathName(f), absolutePermission);\n  }\n\n\n  @Override\n  public void close() throws IOException {\n    try {\n      if (dfs != null) {\n        dfs.closeOutputStreams(false);\n      }\n      super.close();\n    } finally {\n      if (dfs != null) {\n        dfs.close();\n      }\n    }\n  }\n\n  @Override\n  public String toString() {\n    return \"DFS[\" + dfs + \"]\";\n  }\n\n  @InterfaceAudience.Private\n  @VisibleForTesting\n  public DFSClient getClient() {\n    return dfs;\n  }\n\n  @Override\n  public FsStatus getStatus(Path p) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_STATUS);\n    return dfs.getDiskStatus();\n  }\n\n  /**\n   * Returns count of blocks with no good replicas left. Normally should be\n   * zero.\n   *\n   * @throws IOException\n   */\n  public long getMissingBlocksCount() throws IOException {\n    return dfs.getMissingBlocksCount();\n  }\n\n  /**\n   * Returns count of blocks pending on deletion.\n   *\n   * @throws IOException\n   */\n  public long getPendingDeletionBlocksCount() throws IOException {\n    return dfs.getPendingDeletionBlocksCount();\n  }\n\n  /**\n   * Returns count of blocks with replication factor 1 and have\n   * lost the only replica.\n   *\n   * @throws IOException\n   */\n  public long getMissingReplOneBlocksCount() throws IOException {\n    return dfs.getMissingReplOneBlocksCount();\n  }\n\n  /**\n   * Returns aggregated count of blocks with less redundancy.\n   *\n   * @throws IOException\n   */\n  public long getLowRedundancyBlocksCount() throws IOException {\n    return dfs.getLowRedundancyBlocksCount();\n  }\n\n  /**\n   * Returns count of blocks with at least one replica marked corrupt.\n   *\n   * @throws IOException\n   */\n  public long getCorruptBlocksCount() throws IOException {\n    return dfs.getCorruptBlocksCount();\n  }\n\n  @Override\n  public RemoteIterator<Path> listCorruptFileBlocks(final Path path)\n      throws IOException {\n    Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<RemoteIterator<Path>>() {\n      @Override\n      public RemoteIterator<Path> doCall(final Path path) throws IOException,\n          UnresolvedLinkException {\n        return new CorruptFileBlockIterator(dfs, path);\n      }\n\n      @Override\n      public RemoteIterator<Path> next(final FileSystem fs, final Path path)\n          throws IOException {\n        return fs.listCorruptFileBlocks(path);\n      }\n    }.resolve(this, absF);\n  }\n\n  /** @return datanode statistics. */\n  public DatanodeInfo[] getDataNodeStats() throws IOException {\n    return getDataNodeStats(DatanodeReportType.ALL);\n  }\n\n  /** @return datanode statistics for the given type. */\n  public DatanodeInfo[] getDataNodeStats(final DatanodeReportType type)\n      throws IOException {\n    return dfs.datanodeReport(type);\n  }\n\n  /**\n   * Enter, leave or get safe mode.\n   *\n   * @see org.apache.hadoop.hdfs.protocol.ClientProtocol#setSafeMode(\n   *    HdfsConstants.SafeModeAction,boolean)\n   */\n  @Override\n  public boolean setSafeMode(SafeModeAction action)\n      throws IOException {\n    return setSafeMode(action, false);\n  }\n\n  /**\n   * Enter, leave or get safe mode.\n   *\n   * @param action\n   *          One of SafeModeAction.ENTER, SafeModeAction.LEAVE and\n   *          SafeModeAction.GET.\n   * @param isChecked\n   *          If true check only for Active NNs status, else check first NN's\n   *          status.\n   */\n  @Override\n  @SuppressWarnings(\"deprecation\")\n  public boolean setSafeMode(SafeModeAction action, boolean isChecked)\n      throws IOException {\n    return this.setSafeMode(convertToClientProtocolSafeModeAction(action), isChecked);\n  }\n\n  /**\n   * Translating the {@link SafeModeAction} into {@link HdfsConstants.SafeModeAction}\n   * that is used by {@link DFSClient#setSafeMode(HdfsConstants.SafeModeAction, boolean)}.\n   *\n   * @param action any supported action listed in {@link SafeModeAction}.\n   * @return the converted {@link HdfsConstants.SafeModeAction}.\n   * @throws UnsupportedOperationException if the provided {@link SafeModeAction} cannot be\n   *           translated.\n   */\n  private static HdfsConstants.SafeModeAction convertToClientProtocolSafeModeAction(\n      SafeModeAction action) {\n    switch (action) {\n    case ENTER:\n      return HdfsConstants.SafeModeAction.SAFEMODE_ENTER;\n    case LEAVE:\n      return HdfsConstants.SafeModeAction.SAFEMODE_LEAVE;\n    case FORCE_EXIT:\n      return HdfsConstants.SafeModeAction.SAFEMODE_FORCE_EXIT;\n    case GET:\n      return HdfsConstants.SafeModeAction.SAFEMODE_GET;\n    default:\n      throw new UnsupportedOperationException(\"Unsupported safe mode action \" + action);\n    }\n  }\n\n  /**\n   * Enter, leave or get safe mode.\n   *\n   * @see org.apache.hadoop.hdfs.protocol.ClientProtocol#setSafeMode(HdfsConstants.SafeModeAction,\n   * boolean)\n   *\n   * @deprecated please instead use {@link #setSafeMode(SafeModeAction)}.\n   */\n  @Deprecated\n  public boolean setSafeMode(HdfsConstants.SafeModeAction action)\n      throws IOException {\n    return setSafeMode(action, false);\n  }\n\n  /**\n   * Enter, leave or get safe mode.\n   *\n   * @param action\n   *          One of SafeModeAction.ENTER, SafeModeAction.LEAVE and\n   *          SafeModeAction.GET.\n   * @param isChecked\n   *          If true check only for Active NNs status, else check first NN's\n   *          status.\n   *\n   * @see org.apache.hadoop.hdfs.protocol.ClientProtocol#setSafeMode(HdfsConstants.SafeModeAction,\n   * boolean)\n   *\n   * @deprecated please instead use\n   *               {@link DistributedFileSystem#setSafeMode(SafeModeAction, boolean)}.\n   */\n  @Deprecated\n  public boolean setSafeMode(HdfsConstants.SafeModeAction action,\n      boolean isChecked) throws IOException {\n    return dfs.setSafeMode(action, isChecked);\n  }\n\n  /**\n   * Save namespace image.\n   *\n   * @param timeWindow NameNode can ignore this command if the latest\n   *                   checkpoint was done within the given time period (in\n   *                   seconds).\n   * @return true if a new checkpoint has been made\n   * @see ClientProtocol#saveNamespace(long, long)\n   */\n  public boolean saveNamespace(long timeWindow, long txGap) throws IOException {\n    return dfs.saveNamespace(timeWindow, txGap);\n  }\n\n  /**\n   * Save namespace image. NameNode always does the checkpoint.\n   */\n  public void saveNamespace() throws IOException {\n    saveNamespace(0, 0);\n  }\n\n  /**\n   * Rolls the edit log on the active NameNode.\n   * Requires super-user privileges.\n   * @see org.apache.hadoop.hdfs.protocol.ClientProtocol#rollEdits()\n   * @return the transaction ID of the newly created segment\n   */\n  public long rollEdits() throws IOException {\n    return dfs.rollEdits();\n  }\n\n  /**\n   * enable/disable/check restoreFaileStorage.\n   *\n   * @see org.apache.hadoop.hdfs.protocol.ClientProtocol#restoreFailedStorage(String arg)\n   */\n  public boolean restoreFailedStorage(String arg) throws IOException {\n    return dfs.restoreFailedStorage(arg);\n  }\n\n\n  /**\n   * Refreshes the list of hosts and excluded hosts from the configured\n   * files.\n   */\n  public void refreshNodes() throws IOException {\n    dfs.refreshNodes();\n  }\n\n  /**\n   * Finalize previously upgraded files system state.\n   * @throws IOException\n   */\n  public void finalizeUpgrade() throws IOException {\n    dfs.finalizeUpgrade();\n  }\n\n  /**\n   * Get status of upgrade - finalized or not.\n   * @return true if upgrade is finalized or if no upgrade is in progress and\n   * false otherwise.\n   * @throws IOException\n   */\n  public boolean upgradeStatus() throws IOException {\n    return dfs.upgradeStatus();\n  }\n\n  /**\n   * Rolling upgrade: prepare/finalize/query.\n   */\n  public RollingUpgradeInfo rollingUpgrade(RollingUpgradeAction action)\n      throws IOException {\n    return dfs.rollingUpgrade(action);\n  }\n\n  /*\n   * Requests the namenode to dump data strcutures into specified\n   * file.\n   */\n  public void metaSave(String pathname) throws IOException {\n    dfs.metaSave(pathname);\n  }\n\n  @Override\n  public FsServerDefaults getServerDefaults() throws IOException {\n    return dfs.getServerDefaults();\n  }\n\n  /**\n   * Returns the stat information about the file.\n   *\n   * If the given path is a symlink, the path will be resolved to a target path\n   * and it will get the resolved path's FileStatus object. It will not be\n   * represented as a symlink and isDirectory API returns true if the resolved\n   * path is a directory, false otherwise.\n   *\n   * @throws FileNotFoundException if the file does not exist.\n   */\n  @Override\n  public FileStatus getFileStatus(Path f) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_FILE_STATUS);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<FileStatus>() {\n      @Override\n      public FileStatus doCall(final Path p) throws IOException {\n        HdfsFileStatus fi = dfs.getFileInfo(getPathName(p));\n        if (fi != null) {\n          return fi.makeQualified(getUri(), p);\n        } else {\n          throw new FileNotFoundException(\"File does not exist: \" + p);\n        }\n      }\n      @Override\n      public FileStatus next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.getFileStatus(p);\n      }\n    }.resolve(this, absF);\n  }\n\n  /**\n   * Synchronize client metadata state with Active NameNode.\n   * <p>\n   * In HA the client synchronizes its state with the Active NameNode\n   * in order to guarantee subsequent read consistency from Observer Nodes.\n   * @throws IOException\n   */\n  @Override\n  public void msync() throws IOException {\n    dfs.msync();\n  }\n\n  @SuppressWarnings(\"deprecation\")\n  @Override\n  public void createSymlink(final Path target, final Path link,\n      final boolean createParent) throws IOException {\n    if (!FileSystem.areSymlinksEnabled()) {\n      throw new UnsupportedOperationException(\"Symlinks not supported\");\n    }\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.CREATE_SYM_LINK);\n    final Path absF = fixRelativePart(link);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.createSymlink(target.toString(), getPathName(p), createParent);\n        return null;\n      }\n      @Override\n      public Void next(final FileSystem fs, final Path p) throws IOException {\n        fs.createSymlink(target, p, createParent);\n        return null;\n      }\n    }.resolve(this, absF);\n  }\n\n  @Override\n  public boolean supportsSymlinks() {\n    return true;\n  }\n\n  @Override\n  public FileStatus getFileLinkStatus(final Path f) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_FILE_LINK_STATUS);\n    final Path absF = fixRelativePart(f);\n    FileStatus status = new FileSystemLinkResolver<FileStatus>() {\n      @Override\n      public FileStatus doCall(final Path p) throws IOException {\n        HdfsFileStatus fi = dfs.getFileLinkInfo(getPathName(p));\n        if (fi != null) {\n          return fi.makeQualified(getUri(), p);\n        } else {\n          throw new FileNotFoundException(\"File does not exist: \" + p);\n        }\n      }\n      @Override\n      public FileStatus next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.getFileLinkStatus(p);\n      }\n    }.resolve(this, absF);\n    // Fully-qualify the symlink\n    if (status.isSymlink()) {\n      Path targetQual = FSLinkResolver.qualifySymlinkTarget(this.getUri(),\n          status.getPath(), status.getSymlink());\n      status.setSymlink(targetQual);\n    }\n    return status;\n  }\n\n  @Override\n  public Path getLinkTarget(final Path f) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_LINK_TARGET);\n    final Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<Path>() {\n      @Override\n      public Path doCall(final Path p) throws IOException {\n        HdfsFileStatus fi = dfs.getFileLinkInfo(getPathName(p));\n        if (fi != null) {\n          return fi.makeQualified(getUri(), p).getSymlink();\n        } else {\n          throw new FileNotFoundException(\"File does not exist: \" + p);\n        }\n      }\n      @Override\n      public Path next(final FileSystem fs, final Path p) throws IOException {\n        return fs.getLinkTarget(p);\n      }\n    }.resolve(this, absF);\n  }\n\n  @Override\n  protected Path resolveLink(Path f) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.RESOLVE_LINK);\n    String target = dfs.getLinkTarget(getPathName(fixRelativePart(f)));\n    if (target == null) {\n      throw new FileNotFoundException(\"File does not exist: \" + f.toString());\n    }\n    return new Path(target);\n  }\n\n  @Override\n  public FileChecksum getFileChecksum(Path f) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_FILE_CHECKSUM);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<FileChecksum>() {\n      @Override\n      public FileChecksum doCall(final Path p) throws IOException {\n        return dfs.getFileChecksumWithCombineMode(\n            getPathName(p), Long.MAX_VALUE);\n      }\n\n      @Override\n      public FileChecksum next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.getFileChecksum(p);\n      }\n    }.resolve(this, absF);\n  }\n\n  @Override\n  public FileChecksum getFileChecksum(Path f, final long length)\n      throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_FILE_CHECKSUM);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<FileChecksum>() {\n      @Override\n      public FileChecksum doCall(final Path p) throws IOException {\n        return dfs.getFileChecksumWithCombineMode(getPathName(p), length);\n      }\n\n      @Override\n      public FileChecksum next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          return fs.getFileChecksum(p, length);\n        } else {\n          throw new UnsupportedFileSystemException(\n              \"getFileChecksum(Path, long) is not supported by \"\n                  + fs.getClass().getSimpleName());\n        }\n      }\n    }.resolve(this, absF);\n  }\n\n  @Override\n  public void setPermission(Path p, final FsPermission permission\n  ) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.SET_PERMISSION);\n    Path absF = fixRelativePart(p);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.setPermission(getPathName(p), permission);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        fs.setPermission(p, permission);\n        return null;\n      }\n    }.resolve(this, absF);\n  }\n\n  @Override\n  public void setOwner(Path p, final String username, final String groupname)\n      throws IOException {\n    if (username == null && groupname == null) {\n      throw new IOException(\"username == null && groupname == null\");\n    }\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.SET_OWNER);\n    Path absF = fixRelativePart(p);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.setOwner(getPathName(p), username, groupname);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        fs.setOwner(p, username, groupname);\n        return null;\n      }\n    }.resolve(this, absF);\n  }\n\n  @Override\n  public void setTimes(Path p, final long mtime, final long atime)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.SET_TIMES);\n    Path absF = fixRelativePart(p);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.setTimes(getPathName(p), mtime, atime);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        fs.setTimes(p, mtime, atime);\n        return null;\n      }\n    }.resolve(this, absF);\n  }\n\n\n  @Override\n  protected int getDefaultPort() {\n    return HdfsClientConfigKeys.DFS_NAMENODE_RPC_PORT_DEFAULT;\n  }\n\n  @Override\n  public Token<DelegationTokenIdentifier> getDelegationToken(String renewer)\n      throws IOException {\n    return dfs.getDelegationToken(renewer == null ? null : new Text(renewer));\n  }\n\n  /**\n   * Requests the namenode to tell all datanodes to use a new, non-persistent\n   * bandwidth value for dfs.datanode.balance.bandwidthPerSec.\n   * The bandwidth parameter is the max number of bytes per second of network\n   * bandwidth to be used by a datanode during balancing.\n   *\n   * @param bandwidth Balancer bandwidth in bytes per second for all datanodes.\n   * @throws IOException\n   */\n  public void setBalancerBandwidth(long bandwidth) throws IOException {\n    dfs.setBalancerBandwidth(bandwidth);\n  }\n\n  /**\n   * Get a canonical service name for this file system. If the URI is logical,\n   * the hostname part of the URI will be returned.\n   * @return a service string that uniquely identifies this file system.\n   */\n  @Override\n  public String getCanonicalServiceName() {\n    return dfs.getCanonicalServiceName();\n  }\n\n  @Override\n  protected URI canonicalizeUri(URI uri) {\n    if (HAUtilClient.isLogicalUri(getConf(), uri)) {\n      // Don't try to DNS-resolve logical URIs, since the 'authority'\n      // portion isn't a proper hostname\n      return uri;\n    } else {\n      return NetUtils.getCanonicalUri(uri, getDefaultPort());\n    }\n  }\n\n  /**\n   * Utility function that returns if the NameNode is in safemode or not. In HA\n   * mode, this API will return only ActiveNN's safemode status.\n   *\n   * @return true if NameNode is in safemode, false otherwise.\n   * @throws IOException\n   *           when there is an issue communicating with the NameNode\n   */\n  public boolean isInSafeMode() throws IOException {\n    return setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET, true);\n  }\n\n  /**\n   * HDFS only.\n   *\n   * Returns if the NameNode enabled the snapshot trash root configuration\n   * dfs.namenode.snapshot.trashroot.enabled\n   * @return true if NameNode enabled snapshot trash root\n   * @throws IOException\n   *           when there is an issue communicating with the NameNode\n   */\n  public boolean isSnapshotTrashRootEnabled() throws IOException {\n    return dfs.isSnapshotTrashRootEnabled();\n  }\n\n  /** @see org.apache.hadoop.hdfs.client.HdfsAdmin#allowSnapshot(Path) */\n  public void allowSnapshot(final Path path) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.ALLOW_SNAPSHOT);\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.allowSnapshot(getPathName(p));\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.allowSnapshot(p);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + path + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }\n\n  /** @see org.apache.hadoop.hdfs.client.HdfsAdmin#disallowSnapshot(Path) */\n  public void disallowSnapshot(final Path path) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.DISALLOW_SNAPSHOT);\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        checkTrashRootAndRemoveIfEmpty(p);\n        dfs.disallowSnapshot(getPathName(p));\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.checkTrashRootAndRemoveIfEmpty(p);\n          myDfs.disallowSnapshot(p);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + path + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }\n\n  /**\n   * Helper function to check if a trash root exists in the given directory,\n   * remove the trash root if it is empty, or throw IOException if not empty\n   * @param p Path to a directory.\n   */\n  private void checkTrashRootAndRemoveIfEmpty(final Path p) throws IOException {\n    // If p is EZ root, skip the check\n    if (dfs.isHDFSEncryptionEnabled() && dfs.isEZRoot(p)) {\n      DFSClient.LOG.debug(\"{} is an encryption zone root. \"\n          + \"Skipping empty trash root check.\", p);\n      return;\n    }\n    Path trashRoot = new Path(p, FileSystem.TRASH_PREFIX);\n    try {\n      // listStatus has 4 possible outcomes here:\n      // 1) throws FileNotFoundException: the trash root doesn't exist.\n      // 2) returns empty array: the trash path is an empty directory.\n      // 3) returns non-empty array, len >= 2: the trash root is not empty.\n      // 4) returns non-empty array, len == 1:\n      //    i) if the element's path is exactly p, the trash path is not a dir.\n      //       e.g. a file named .Trash. Ignore.\n      //   ii) if the element's path isn't p, the trash root is not empty.\n      FileStatus[] fileStatuses = listStatus(trashRoot);\n      if (fileStatuses.length == 0) {\n        DFSClient.LOG.debug(\"Removing empty trash root {}\", trashRoot);\n        delete(trashRoot, false);\n      } else {\n        if (fileStatuses.length == 1\n            && !fileStatuses[0].isDirectory()\n            && fileStatuses[0].getPath().toUri().getPath().equals(\n                trashRoot.toString())) {\n          // Ignore the trash path because it is not a directory.\n          DFSClient.LOG.warn(\"{} is not a directory. Ignored.\", trashRoot);\n        } else {\n          throw new IOException(\"Found non-empty trash root at \" +\n              trashRoot + \". Rename or delete it, then try again.\");\n        }\n      }\n    } catch (FileNotFoundException | AccessControlException ignored) {\n    }\n  }\n\n  @Override\n  public Path createSnapshot(final Path path, final String snapshotName)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.CREATE_SNAPSHOT);\n    Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<Path>() {\n      @Override\n      public Path doCall(final Path p) throws IOException {\n        return new Path(dfs.createSnapshot(getPathName(p), snapshotName));\n      }\n\n      @Override\n      public Path next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          return myDfs.createSnapshot(p);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + path + \" -> \" + p);\n        }\n      }\n    }.resolve(this, absF);\n  }\n\n  @Override\n  public void renameSnapshot(final Path path, final String snapshotOldName,\n      final String snapshotNewName) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.RENAME_SNAPSHOT);\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.renameSnapshot(getPathName(p), snapshotOldName, snapshotNewName);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.renameSnapshot(p, snapshotOldName, snapshotNewName);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + path + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }\n\n  /**\n   * Get the list of snapshottable directories that are owned\n   * by the current user. Return all the snapshottable directories if the\n   * current user is a super user.\n   * @return The list of all the current snapshottable directories.\n   * @throws IOException If an I/O error occurred.\n   */\n  public SnapshottableDirectoryStatus[] getSnapshottableDirListing()\n      throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics\n        .incrementOpCounter(OpType.GET_SNAPSHOTTABLE_DIRECTORY_LIST);\n    return dfs.getSnapshottableDirListing();\n  }\n\n  /**\n   * @return all the snapshots for a snapshottable directory\n   * @throws IOException\n   */\n  public SnapshotStatus[] getSnapshotListing(Path snapshotRoot)\n      throws IOException {\n    Path absF = fixRelativePart(snapshotRoot);\n    statistics.incrementReadOps(1);\n    storageStatistics\n        .incrementOpCounter(OpType.GET_SNAPSHOT_LIST);\n    return dfs.getSnapshotListing(getPathName(absF));\n  }\n\n  @Override\n  public void deleteSnapshot(final Path snapshotDir, final String snapshotName)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.DELETE_SNAPSHOT);\n    Path absF = fixRelativePart(snapshotDir);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.deleteSnapshot(getPathName(p), snapshotName);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.deleteSnapshot(p, snapshotName);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + snapshotDir + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }\n\n  /**\n   * Returns a remote iterator so that followup calls are made on demand\n   * while consuming the SnapshotDiffReportListing entries.\n   * This reduces memory consumption overhead in case the snapshotDiffReport\n   * is huge.\n   *\n   * @param snapshotDir\n   *          full path of the directory where snapshots are taken\n   * @param fromSnapshot\n   *          snapshot name of the from point. Null indicates the current\n   *          tree\n   * @param toSnapshot\n   *          snapshot name of the to point. Null indicates the current\n   *          tree.\n   * @return Remote iterator\n   */\n  public RemoteIterator\n      <SnapshotDiffReportListing> snapshotDiffReportListingRemoteIterator(\n      final Path snapshotDir, final String fromSnapshot,\n      final String toSnapshot) throws IOException {\n    Path absF = fixRelativePart(snapshotDir);\n    return new FileSystemLinkResolver\n        <RemoteIterator<SnapshotDiffReportListing>>() {\n      @Override\n      public RemoteIterator<SnapshotDiffReportListing> doCall(final Path p)\n          throws IOException {\n        if (!DFSUtilClient.isValidSnapshotName(fromSnapshot) ||\n            !DFSUtilClient.isValidSnapshotName(toSnapshot)) {\n          throw new UnsupportedOperationException(\"Remote Iterator is\"\n              + \"supported for snapshotDiffReport between two snapshots\");\n        }\n        return new SnapshotDiffReportListingIterator(getPathName(p),\n            fromSnapshot, toSnapshot);\n      }\n\n      @Override\n      public RemoteIterator<SnapshotDiffReportListing> next(final FileSystem fs,\n          final Path p) throws IOException {\n        return ((DistributedFileSystem) fs)\n            .snapshotDiffReportListingRemoteIterator(p, fromSnapshot,\n                toSnapshot);\n      }\n    }.resolve(this, absF);\n\n  }\n\n  /**\n   * This class defines an iterator that returns\n   * the SnapshotDiffReportListing for a snapshottable directory\n   * between two given snapshots.\n   */\n  private final class SnapshotDiffReportListingIterator implements\n      RemoteIterator<SnapshotDiffReportListing> {\n    private final String snapshotDir;\n    private final String fromSnapshot;\n    private final String toSnapshot;\n\n    private byte[] startPath;\n    private int index;\n    private boolean hasNext = true;\n\n    private SnapshotDiffReportListingIterator(String snapshotDir,\n        String fromSnapshot, String toSnapshot) {\n      this.snapshotDir = snapshotDir;\n      this.fromSnapshot = fromSnapshot;\n      this.toSnapshot = toSnapshot;\n      this.startPath = DFSUtilClient.EMPTY_BYTES;\n      this.index = -1;\n    }\n\n    @Override\n    public boolean hasNext() {\n      return hasNext;\n    }\n\n    @Override\n    public SnapshotDiffReportListing next() throws IOException {\n      if (!hasNext) {\n        throw new java.util.NoSuchElementException(\n            \"No more entry in SnapshotDiffReport for \" + snapshotDir);\n      }\n      final SnapshotDiffReportListing part =\n          dfs.getSnapshotDiffReportListing(snapshotDir, fromSnapshot,\n              toSnapshot, startPath, index);\n      startPath = part.getLastPath();\n      index = part.getLastIndex();\n      hasNext =\n          !(Arrays.equals(startPath, DFSUtilClient.EMPTY_BYTES) && index == -1);\n      return part;\n    }\n  }\n\n  private SnapshotDiffReport getSnapshotDiffReportInternal(\n      final String snapshotDir, final String fromSnapshot,\n      final String toSnapshot) throws IOException {\n    return  DFSUtilClient.getSnapshotDiffReport(snapshotDir, fromSnapshot, toSnapshot,\n        dfs::getSnapshotDiffReport, dfs::getSnapshotDiffReportListing);\n  }\n\n  /**\n   * Get the difference between two snapshots, or between a snapshot and the\n   * current tree of a directory.\n   *\n   * @see DFSClient#getSnapshotDiffReportListing\n   */\n  public SnapshotDiffReport getSnapshotDiffReport(final Path snapshotDir,\n      final String fromSnapshot, final String toSnapshot) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_SNAPSHOT_DIFF);\n    Path absF = fixRelativePart(snapshotDir);\n    return new FileSystemLinkResolver<SnapshotDiffReport>() {\n      @Override\n      public SnapshotDiffReport doCall(final Path p)\n          throws IOException {\n        return getSnapshotDiffReportInternal(getPathName(p), fromSnapshot,\n            toSnapshot);\n      }\n\n      @Override\n      public SnapshotDiffReport next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.getSnapshotDiffReport(p, fromSnapshot, toSnapshot);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + snapshotDir + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }\n\n  /**\n   * Get the difference between two snapshots of a directory iteratively.\n   *\n   * @param snapshotDir full path of the directory where snapshots are taken.\n   * @param fromSnapshotName snapshot name of the from point. Null indicates the current tree.\n   * @param toSnapshotName snapshot name of the to point. Null indicates the current tree.\n   * @param snapshotDiffStartPath path relative to the snapshottable root directory from where\n   *     the snapshotdiff computation needs to start.\n   * @param snapshotDiffIndex index in the created or deleted list of the directory at which the\n   *     snapshotdiff computation stopped during the last rpc call. -1 indicates the diff\n   *     computation needs to start right from the start path.\n   * @return the difference report represented as a {@link SnapshotDiffReportListing}.\n   * @throws IOException if an I/O error occurred.\n   */\n  public SnapshotDiffReportListing getSnapshotDiffReportListing(Path snapshotDir,\n      String fromSnapshotName, String toSnapshotName, String snapshotDiffStartPath,\n      int snapshotDiffIndex) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_SNAPSHOT_DIFF);\n    Path absF = fixRelativePart(snapshotDir);\n    return new FileSystemLinkResolver<SnapshotDiffReportListing>() {\n\n      @Override\n      public SnapshotDiffReportListing doCall(final Path p) throws IOException {\n        return dfs.getSnapshotDiffReportListing(getPathName(p), fromSnapshotName, toSnapshotName,\n            DFSUtilClient.string2Bytes(snapshotDiffStartPath), snapshotDiffIndex);\n      }\n\n      @Override\n      public SnapshotDiffReportListing next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem distributedFileSystem = (DistributedFileSystem)fs;\n          distributedFileSystem.getSnapshotDiffReportListing(p, fromSnapshotName, toSnapshotName,\n              snapshotDiffStartPath, snapshotDiffIndex);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + snapshotDir + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }\n\n  /**\n   * Get the close status of a file\n   * @param src The path to the file\n   *\n   * @return return true if file is closed\n   * @throws FileNotFoundException if the file does not exist.\n   * @throws IOException If an I/O error occurred\n   */\n  @Override\n  public boolean isFileClosed(final Path src) throws IOException {\n    Path absF = fixRelativePart(src);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p) throws IOException {\n        return dfs.isFileClosed(getPathName(p));\n      }\n\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          return myDfs.isFileClosed(p);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot call isFileClosed\"\n              + \" on a symlink to a non-DistributedFileSystem: \"\n              + src + \" -> \" + p);\n        }\n      }\n    }.resolve(this, absF);\n  }\n\n  /**\n   * @see #addCacheDirective(CacheDirectiveInfo, EnumSet)\n   */\n  public long addCacheDirective(CacheDirectiveInfo info) throws IOException {\n    return addCacheDirective(info, EnumSet.noneOf(CacheFlag.class));\n  }\n\n  /**\n   * Add a new CacheDirective.\n   *\n   * @param info Information about a directive to add.\n   * @param flags {@link CacheFlag}s to use for this operation.\n   * @return the ID of the directive that was created.\n   * @throws IOException if the directive could not be added\n   */\n  public long addCacheDirective(\n      CacheDirectiveInfo info, EnumSet<CacheFlag> flags) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.ADD_CACHE_DIRECTIVE);\n    Preconditions.checkNotNull(info.getPath());\n    Path path = new Path(getPathName(fixRelativePart(info.getPath()))).\n        makeQualified(getUri(), getWorkingDirectory());\n    return dfs.addCacheDirective(\n        new CacheDirectiveInfo.Builder(info).\n            setPath(path).\n            build(),\n        flags);\n  }\n\n  /**\n   * @see #modifyCacheDirective(CacheDirectiveInfo, EnumSet)\n   */\n  public void modifyCacheDirective(CacheDirectiveInfo info) throws IOException {\n    modifyCacheDirective(info, EnumSet.noneOf(CacheFlag.class));\n  }\n\n  /**\n   * Modify a CacheDirective.\n   *\n   * @param info Information about the directive to modify. You must set the ID\n   *          to indicate which CacheDirective you want to modify.\n   * @param flags {@link CacheFlag}s to use for this operation.\n   * @throws IOException if the directive could not be modified\n   */\n  public void modifyCacheDirective(\n      CacheDirectiveInfo info, EnumSet<CacheFlag> flags) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.MODIFY_CACHE_DIRECTIVE);\n    if (info.getPath() != null) {\n      info = new CacheDirectiveInfo.Builder(info).\n          setPath(new Path(getPathName(fixRelativePart(info.getPath()))).\n              makeQualified(getUri(), getWorkingDirectory())).build();\n    }\n    dfs.modifyCacheDirective(info, flags);\n  }\n\n  /**\n   * Remove a CacheDirectiveInfo.\n   *\n   * @param id identifier of the CacheDirectiveInfo to remove\n   * @throws IOException if the directive could not be removed\n   */\n  public void removeCacheDirective(long id)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.REMOVE_CACHE_DIRECTIVE);\n    dfs.removeCacheDirective(id);\n  }\n\n  /**\n   * List cache directives.  Incrementally fetches results from the server.\n   *\n   * @param filter Filter parameters to use when listing the directives, null to\n   *               list all directives visible to us.\n   * @return A RemoteIterator which returns CacheDirectiveInfo objects.\n   */\n  public RemoteIterator<CacheDirectiveEntry> listCacheDirectives(\n      CacheDirectiveInfo filter) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.LIST_CACHE_DIRECTIVE);\n    if (filter == null) {\n      filter = new CacheDirectiveInfo.Builder().build();\n    }\n    if (filter.getPath() != null) {\n      filter = new CacheDirectiveInfo.Builder(filter).\n          setPath(new Path(getPathName(fixRelativePart(filter.getPath())))).\n          build();\n    }\n    final RemoteIterator<CacheDirectiveEntry> iter =\n        dfs.listCacheDirectives(filter);\n    return new RemoteIterator<CacheDirectiveEntry>() {\n      @Override\n      public boolean hasNext() throws IOException {\n        return iter.hasNext();\n      }\n\n      @Override\n      public CacheDirectiveEntry next() throws IOException {\n        // Although the paths we get back from the NameNode should always be\n        // absolute, we call makeQualified to add the scheme and authority of\n        // this DistributedFilesystem.\n        CacheDirectiveEntry desc = iter.next();\n        CacheDirectiveInfo info = desc.getInfo();\n        Path p = info.getPath().makeQualified(getUri(), getWorkingDirectory());\n        return new CacheDirectiveEntry(\n            new CacheDirectiveInfo.Builder(info).setPath(p).build(),\n            desc.getStats());\n      }\n    };\n  }\n\n  /**\n   * Add a cache pool.\n   *\n   * @param info\n   *          The request to add a cache pool.\n   * @throws IOException\n   *          If the request could not be completed.\n   */\n  public void addCachePool(CachePoolInfo info) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.ADD_CACHE_POOL);\n    CachePoolInfo.validate(info);\n    dfs.addCachePool(info);\n  }\n\n  /**\n   * Modify an existing cache pool.\n   *\n   * @param info\n   *          The request to modify a cache pool.\n   * @throws IOException\n   *          If the request could not be completed.\n   */\n  public void modifyCachePool(CachePoolInfo info) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.MODIFY_CACHE_POOL);\n    CachePoolInfo.validate(info);\n    dfs.modifyCachePool(info);\n  }\n\n  /**\n   * Remove a cache pool.\n   *\n   * @param poolName\n   *          Name of the cache pool to remove.\n   * @throws IOException\n   *          if the cache pool did not exist, or could not be removed.\n   */\n  public void removeCachePool(String poolName) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.REMOVE_CACHE_POOL);\n    CachePoolInfo.validateName(poolName);\n    dfs.removeCachePool(poolName);\n  }\n\n  /**\n   * List all cache pools.\n   *\n   * @return A remote iterator from which you can get CachePoolEntry objects.\n   *          Requests will be made as needed.\n   * @throws IOException\n   *          If there was an error listing cache pools.\n   */\n  public RemoteIterator<CachePoolEntry> listCachePools() throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.LIST_CACHE_POOL);\n    return dfs.listCachePools();\n  }\n\n  /**\n   * {@inheritDoc}\n   */\n  @Override\n  public void modifyAclEntries(Path path, final List<AclEntry> aclSpec)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.MODIFY_ACL_ENTRIES);\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.modifyAclEntries(getPathName(p), aclSpec);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p) throws IOException {\n        fs.modifyAclEntries(p, aclSpec);\n        return null;\n      }\n    }.resolve(this, absF);\n  }\n\n  /**\n   * {@inheritDoc}\n   */\n  @Override\n  public void removeAclEntries(Path path, final List<AclEntry> aclSpec)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.REMOVE_ACL_ENTRIES);\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.removeAclEntries(getPathName(p), aclSpec);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p) throws IOException {\n        fs.removeAclEntries(p, aclSpec);\n        return null;\n      }\n    }.resolve(this, absF);\n  }\n\n  /**\n   * {@inheritDoc}\n   */\n  @Override\n  public void removeDefaultAcl(Path path) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.REMOVE_DEFAULT_ACL);\n    final Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.removeDefaultAcl(getPathName(p));\n        return null;\n      }\n      @Override\n      public Void next(final FileSystem fs, final Path p) throws IOException {\n        fs.removeDefaultAcl(p);\n        return null;\n      }\n    }.resolve(this, absF);\n  }\n\n  /**\n   * {@inheritDoc}\n   */\n  @Override\n  public void removeAcl(Path path) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.REMOVE_ACL);\n    final Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.removeAcl(getPathName(p));\n        return null;\n      }\n      @Override\n      public Void next(final FileSystem fs, final Path p) throws IOException {\n        fs.removeAcl(p);\n        return null;\n      }\n    }.resolve(this, absF);\n  }\n\n  /**\n   * {@inheritDoc}\n   */\n  @Override\n  public void setAcl(Path path, final List<AclEntry> aclSpec)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.SET_ACL);\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.setAcl(getPathName(p), aclSpec);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p) throws IOException {\n        fs.setAcl(p, aclSpec);\n        return null;\n      }\n    }.resolve(this, absF);\n  }\n\n  /**\n   * {@inheritDoc}\n   */\n  @Override\n  public AclStatus getAclStatus(Path path) throws IOException {\n    final Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<AclStatus>() {\n      @Override\n      public AclStatus doCall(final Path p) throws IOException {\n        return dfs.getAclStatus(getPathName(p));\n      }\n      @Override\n      public AclStatus next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.getAclStatus(p);\n      }\n    }.resolve(this, absF);\n  }\n\n  /* HDFS only */\n  public void createEncryptionZone(final Path path, final String keyName)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.CREATE_ENCRYPTION_ZONE);\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.createEncryptionZone(getPathName(p), keyName);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p) throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem) fs;\n          myDfs.createEncryptionZone(p, keyName);\n          return null;\n        } else {\n          throw new UnsupportedOperationException(\n              \"Cannot call createEncryptionZone\"\n                  + \" on a symlink to a non-DistributedFileSystem: \" + path\n                  + \" -> \" + p);\n        }\n      }\n    }.resolve(this, absF);\n  }\n\n  /* HDFS only */\n  public EncryptionZone getEZForPath(final Path path)\n      throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_ENCRYPTION_ZONE);\n    Preconditions.checkNotNull(path);\n    Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<EncryptionZone>() {\n      @Override\n      public EncryptionZone doCall(final Path p) throws IOException {\n        return dfs.getEZForPath(getPathName(p));\n      }\n\n      @Override\n      public EncryptionZone next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem) fs;\n          return myDfs.getEZForPath(p);\n        } else {\n          throw new UnsupportedOperationException(\n              \"Cannot call getEZForPath\"\n                  + \" on a symlink to a non-DistributedFileSystem: \" + path\n                  + \" -> \" + p);\n        }\n      }\n    }.resolve(this, absF);\n  }\n\n  /* HDFS only */\n  public RemoteIterator<EncryptionZone> listEncryptionZones()\n      throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.LIST_ENCRYPTION_ZONE);\n    return dfs.listEncryptionZones();\n  }\n\n  /* HDFS only */\n  public void reencryptEncryptionZone(final Path zone,\n      final ReencryptAction action) throws IOException {\n    final Path absF = fixRelativePart(zone);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.reencryptEncryptionZone(getPathName(p), action);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p) throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem) fs;\n          myDfs.reencryptEncryptionZone(p, action);\n          return null;\n        }\n        throw new UnsupportedOperationException(\n            \"Cannot call reencryptEncryptionZone\"\n                + \" on a symlink to a non-DistributedFileSystem: \" + zone\n                + \" -> \" + p);\n      }\n    }.resolve(this, absF);\n  }\n\n  /* HDFS only */\n  public RemoteIterator<ZoneReencryptionStatus> listReencryptionStatus()\n      throws IOException {\n    return dfs.listReencryptionStatus();\n  }\n\n  /* HDFS only */\n  public FileEncryptionInfo getFileEncryptionInfo(final Path path)\n      throws IOException {\n    Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<FileEncryptionInfo>() {\n      @Override\n      public FileEncryptionInfo doCall(final Path p) throws IOException {\n        final HdfsFileStatus fi = dfs.getFileInfo(getPathName(p));\n        if (fi == null) {\n          throw new FileNotFoundException(\"File does not exist: \" + p);\n        }\n        return fi.getFileEncryptionInfo();\n      }\n\n      @Override\n      public FileEncryptionInfo next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          return myDfs.getFileEncryptionInfo(p);\n        }\n        throw new UnsupportedOperationException(\n            \"Cannot call getFileEncryptionInfo\"\n                + \" on a symlink to a non-DistributedFileSystem: \" + path\n                + \" -> \" + p);\n      }\n    }.resolve(this, absF);\n  }\n\n  /* HDFS only */\n  public void provisionEZTrash(final Path path,\n      final FsPermission trashPermission) throws IOException {\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(Path p) throws IOException {\n        provisionEZTrash(getPathName(p), trashPermission);\n        return null;\n      }\n\n      @Override\n      public Void next(FileSystem fs, Path p) throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.provisionEZTrash(p, trashPermission);\n          return null;\n        }\n        throw new UnsupportedOperationException(\"Cannot provisionEZTrash \" +\n            \"through a symlink to a non-DistributedFileSystem: \" + fs + \" -> \"\n            + p);\n      }\n    }.resolve(this, absF);\n  }\n\n  private void provisionEZTrash(String path, FsPermission trashPermission)\n      throws IOException {\n    // make sure the path is an EZ\n    EncryptionZone ez = dfs.getEZForPath(path);\n    if (ez == null) {\n      throw new IllegalArgumentException(path + \" is not an encryption zone.\");\n    }\n\n    String ezPath = ez.getPath();\n    if (!path.toString().equals(ezPath)) {\n      throw new IllegalArgumentException(path + \" is not the root of an \" +\n          \"encryption zone. Do you mean \" + ez.getPath() + \"?\");\n    }\n\n    // check if the trash directory exists\n    Path trashPath = new Path(ez.getPath(), FileSystem.TRASH_PREFIX);\n    try {\n      FileStatus trashFileStatus = getFileStatus(trashPath);\n      String errMessage = \"Will not provision new trash directory for \" +\n          \"encryption zone \" + ez.getPath() + \". Path already exists.\";\n      if (!trashFileStatus.isDirectory()) {\n        errMessage += \"\\r\\n\" +\n            \"Warning: \" + trashPath.toString() + \" is not a directory\";\n      }\n      if (!trashFileStatus.getPermission().equals(trashPermission)) {\n        errMessage += \"\\r\\n\" +\n            \"Warning: the permission of \" +\n            trashPath.toString() + \" is not \" + trashPermission;\n      }\n      throw new FileAlreadyExistsException(errMessage);\n    } catch (FileNotFoundException ignored) {\n      // no trash path\n    }\n\n    // Update the permission bits\n    mkdir(trashPath, trashPermission);\n    setPermission(trashPath, trashPermission);\n  }\n\n  /**\n   * HDFS only.\n   *\n   * Provision snapshottable directory trash.\n   * @param path Path to a snapshottable directory.\n   * @param trashPermission Expected FsPermission of the trash root.\n   * @return Path of the provisioned trash root\n   */\n  public Path provisionSnapshotTrash(final Path path,\n      final FsPermission trashPermission) throws IOException {\n    Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<Path>() {\n      @Override\n      public Path doCall(Path p) throws IOException {\n        return provisionSnapshotTrash(getPathName(p), trashPermission);\n      }\n\n      @Override\n      public Path next(FileSystem fs, Path p) throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          return myDfs.provisionSnapshotTrash(p, trashPermission);\n        }\n        throw new UnsupportedOperationException(\n            \"Cannot provisionSnapshotTrash through a symlink to\" +\n            \" a non-DistributedFileSystem: \" + fs + \" -> \" + p);\n      }\n    }.resolve(this, absF);\n  }\n\n  private Path provisionSnapshotTrash(\n      String pathStr, FsPermission trashPermission) throws IOException {\n    Path path = new Path(pathStr);\n    // Given path must be a snapshottable directory\n    FileStatus fileStatus = getFileStatus(path);\n    if (!fileStatus.isSnapshotEnabled()) {\n      throw new IllegalArgumentException(\n          path + \" is not a snapshottable directory.\");\n    }\n\n    // Check if trash root already exists\n    Path trashPath = new Path(path, FileSystem.TRASH_PREFIX);\n    try {\n      FileStatus trashFileStatus = getFileStatus(trashPath);\n      boolean throwException = false;\n      String errMessage = \"Can't provision trash for snapshottable directory \" +\n          pathStr + \" because trash path \" + trashPath.toString() +\n          \" already exists.\";\n      if (!trashFileStatus.isDirectory()) {\n        throwException = true;\n        errMessage += \"\\r\\n\" +\n            \"WARNING: \" + trashPath.toString() + \" is not a directory.\";\n      }\n      if (!trashFileStatus.getPermission().equals(trashPermission)) {\n        throwException = true;\n        errMessage += \"\\r\\n\" +\n            \"WARNING: Permission of \" + trashPath.toString() +\n            \" differs from provided permission \" + trashPermission;\n      }\n      if (throwException) {\n        throw new FileAlreadyExistsException(errMessage);\n      }\n    } catch (FileNotFoundException ignored) {\n      // Trash path doesn't exist. Continue\n    }\n\n    // Create trash root and set the permission\n    mkdir(trashPath, trashPermission);\n    setPermission(trashPath, trashPermission);\n\n    // Print a warning if snapshot trash root feature is not enabled\n    if (!isSnapshotTrashRootEnabled()) {\n      DFSClient.LOG.warn(\"New trash is provisioned, but the snapshot trash root\"\n          + \" feature is disabled. This new trash but won't be automatically\"\n          + \" utilized unless the feature is enabled on the NameNode.\");\n    }\n    return trashPath;\n  }\n\n  @Override\n  public void setXAttr(Path path, final String name, final byte[] value,\n      final EnumSet<XAttrSetFlag> flag) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.SET_XATTR);\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.setXAttr(getPathName(p), name, value, flag);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p) throws IOException {\n        fs.setXAttr(p, name, value, flag);\n        return null;\n      }\n    }.resolve(this, absF);\n  }\n\n  @Override\n  public byte[] getXAttr(Path path, final String name) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_XATTR);\n    final Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<byte[]>() {\n      @Override\n      public byte[] doCall(final Path p) throws IOException {\n        return dfs.getXAttr(getPathName(p), name);\n      }\n      @Override\n      public byte[] next(final FileSystem fs, final Path p) throws IOException {\n        return fs.getXAttr(p, name);\n      }\n    }.resolve(this, absF);\n  }\n\n  @Override\n  public Map<String, byte[]> getXAttrs(Path path) throws IOException {\n    final Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<Map<String, byte[]>>() {\n      @Override\n      public Map<String, byte[]> doCall(final Path p) throws IOException {\n        return dfs.getXAttrs(getPathName(p));\n      }\n      @Override\n      public Map<String, byte[]> next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.getXAttrs(p);\n      }\n    }.resolve(this, absF);\n  }\n\n  @Override\n  public Map<String, byte[]> getXAttrs(Path path, final List<String> names)\n      throws IOException {\n    final Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<Map<String, byte[]>>() {\n      @Override\n      public Map<String, byte[]> doCall(final Path p) throws IOException {\n        return dfs.getXAttrs(getPathName(p), names);\n      }\n      @Override\n      public Map<String, byte[]> next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.getXAttrs(p, names);\n      }\n    }.resolve(this, absF);\n  }\n\n  @Override\n  public List<String> listXAttrs(Path path)\n      throws IOException {\n    final Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<List<String>>() {\n      @Override\n      public List<String> doCall(final Path p) throws IOException {\n        return dfs.listXAttrs(getPathName(p));\n      }\n      @Override\n      public List<String> next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.listXAttrs(p);\n      }\n    }.resolve(this, absF);\n  }\n\n  @Override\n  public void removeXAttr(Path path, final String name) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.REMOVE_XATTR);\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.removeXAttr(getPathName(p), name);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p) throws IOException {\n        fs.removeXAttr(p, name);\n        return null;\n      }\n    }.resolve(this, absF);\n  }\n\n  @Override\n  public void access(Path path, final FsAction mode) throws IOException {\n    final Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.checkAccess(getPathName(p), mode);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        fs.access(p, mode);\n        return null;\n      }\n    }.resolve(this, absF);\n  }\n\n  @Override\n  public URI getKeyProviderUri() throws IOException {\n    return dfs.getKeyProviderUri();\n  }\n\n  @Override\n  public KeyProvider getKeyProvider() throws IOException {\n    return dfs.getKeyProvider();\n  }\n\n  @Override\n  public DelegationTokenIssuer[] getAdditionalTokenIssuers()\n      throws IOException {\n    KeyProvider keyProvider = getKeyProvider();\n    if (keyProvider instanceof DelegationTokenIssuer) {\n      return new DelegationTokenIssuer[]{(DelegationTokenIssuer)keyProvider};\n    }\n    return null;\n  }\n\n  public DFSInotifyEventInputStream getInotifyEventStream() throws IOException {\n    return dfs.getInotifyEventStream();\n  }\n\n  public DFSInotifyEventInputStream getInotifyEventStream(long lastReadTxid)\n      throws IOException {\n    return dfs.getInotifyEventStream(lastReadTxid);\n  }\n\n  /**\n   * Set the source path to the specified erasure coding policy.\n   *\n   * @param path     The directory to set the policy\n   * @param ecPolicyName The erasure coding policy name.\n   * @throws IOException\n   */\n  public void setErasureCodingPolicy(final Path path,\n      final String ecPolicyName) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.SET_EC_POLICY);\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.setErasureCodingPolicy(getPathName(p), ecPolicyName);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p) throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem) fs;\n          myDfs.setErasureCodingPolicy(p, ecPolicyName);\n          return null;\n        }\n        throw new UnsupportedOperationException(\n            \"Cannot setErasureCodingPolicy through a symlink to a \"\n                + \"non-DistributedFileSystem: \" + path + \" -> \" + p);\n      }\n    }.resolve(this, absF);\n  }\n\n  /**\n   * Set the source path to satisfy storage policy.\n   * @param path The source path referring to either a directory or a file.\n   * @throws IOException\n   */\n  public void satisfyStoragePolicy(final Path path) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.SATISFY_STORAGE_POLICY);\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n\n      @Override\n      public Void doCall(Path p) throws IOException {\n        dfs.satisfyStoragePolicy(getPathName(p));\n        return null;\n      }\n\n      @Override\n      public Void next(FileSystem fs, Path p) throws IOException {\n        // DFS only\n        if (fs instanceof  DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem) fs;\n          myDfs.satisfyStoragePolicy(p);\n          return null;\n        }\n        throw new UnsupportedOperationException(\n            \"Cannot satisfyStoragePolicy through a symlink to a \"\n                + \"non-DistributedFileSystem: \" + path + \" -> \" + p);\n      }\n    }.resolve(this, absF);\n  }\n\n  /**\n   * Get erasure coding policy information for the specified path.\n   *\n   * @param path The path of the file or directory\n   * @return Returns the policy information if file or directory on the path\n   * is erasure coded, null otherwise. Null will be returned if directory or\n   * file has REPLICATION policy.\n   * @throws IOException\n   */\n  public ErasureCodingPolicy getErasureCodingPolicy(final Path path)\n      throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_EC_POLICY);\n    Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<ErasureCodingPolicy>() {\n      @Override\n      public ErasureCodingPolicy doCall(final Path p) throws IOException {\n        return dfs.getErasureCodingPolicy(getPathName(p));\n      }\n\n      @Override\n      public ErasureCodingPolicy next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem) fs;\n          return myDfs.getErasureCodingPolicy(p);\n        }\n        throw new UnsupportedOperationException(\n            \"Cannot getErasureCodingPolicy through a symlink to a \"\n                + \"non-DistributedFileSystem: \" + path + \" -> \" + p);\n      }\n    }.resolve(this, absF);\n  }\n\n  /**\n   * Retrieve all the erasure coding policies supported by this file system,\n   * including enabled, disabled and removed policies, but excluding\n   * REPLICATION policy.\n   *\n   * @return all erasure coding policies supported by this file system.\n   * @throws IOException\n   */\n  public Collection<ErasureCodingPolicyInfo> getAllErasureCodingPolicies()\n      throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_EC_POLICIES);\n    return Arrays.asList(dfs.getErasureCodingPolicies());\n  }\n\n  /**\n   * Retrieve all the erasure coding codecs and coders supported by this file\n   * system.\n   *\n   * @return all erasure coding codecs and coders supported by this file system.\n   * @throws IOException\n   */\n  public Map<String, String> getAllErasureCodingCodecs()\n      throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_EC_CODECS);\n    return dfs.getErasureCodingCodecs();\n  }\n\n  /**\n   * Add Erasure coding policies to HDFS. For each policy input, schema and\n   * cellSize are musts, name and id are ignored. They will be automatically\n   * created and assigned by Namenode once the policy is successfully added,\n   * and will be returned in the response; policy states will be set to\n   * DISABLED automatically.\n   *\n   * @param policies The user defined ec policy list to add.\n   * @return Return the response list of adding operations.\n   * @throws IOException\n   */\n  public AddErasureCodingPolicyResponse[] addErasureCodingPolicies(\n      ErasureCodingPolicy[] policies)  throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.ADD_EC_POLICY);\n    return dfs.addErasureCodingPolicies(policies);\n  }\n\n  /**\n   * Remove erasure coding policy.\n   *\n   * @param ecPolicyName The name of the policy to be removed.\n   * @throws IOException\n   */\n  public void removeErasureCodingPolicy(String ecPolicyName)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.REMOVE_EC_POLICY);\n    dfs.removeErasureCodingPolicy(ecPolicyName);\n  }\n\n  /**\n   * Enable erasure coding policy.\n   *\n   * @param ecPolicyName The name of the policy to be enabled.\n   * @throws IOException\n   */\n  public void enableErasureCodingPolicy(String ecPolicyName)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.ENABLE_EC_POLICY);\n    dfs.enableErasureCodingPolicy(ecPolicyName);\n  }\n\n  /**\n   * Disable erasure coding policy.\n   *\n   * @param ecPolicyName The name of the policy to be disabled.\n   * @throws IOException\n   */\n  public void disableErasureCodingPolicy(String ecPolicyName)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.DISABLE_EC_POLICY);\n    dfs.disableErasureCodingPolicy(ecPolicyName);\n  }\n\n  /**\n   * Unset the erasure coding policy from the source path.\n   *\n   * @param path     The directory to unset the policy\n   * @throws IOException\n   */\n  public void unsetErasureCodingPolicy(final Path path) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.UNSET_EC_POLICY);\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.unsetErasureCodingPolicy(getPathName(p));\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p) throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem) fs;\n          myDfs.unsetErasureCodingPolicy(p);\n          return null;\n        }\n        throw new UnsupportedOperationException(\n            \"Cannot unsetErasureCodingPolicy through a symlink to a \"\n                + \"non-DistributedFileSystem: \" + path + \" -> \" + p);\n      }\n    }.resolve(this, absF);\n  }\n\n  /**\n   * Verifies if the given policies are supported in the given cluster setup.\n   * If not policy is specified checks for all enabled policies.\n   * @param policyNames name of policies.\n   * @return the result if the given policies are supported in the cluster setup\n   * @throws IOException\n   */\n  public ECTopologyVerifierResult getECTopologyResultForPolicies(\n      final String... policyNames) throws IOException {\n    return dfs.getECTopologyResultForPolicies(policyNames);\n  }\n\n  /**\n   * Get the root directory of Trash for a path in HDFS.\n   * 1. File in encryption zone returns /ez1/.Trash/username\n   * 2. File in snapshottable directory returns /snapdir1/.Trash/username\n   *    if dfs.namenode.snapshot.trashroot.enabled is set to true.\n   * 3. In other cases, or encountered exception when checking the encryption\n   *    zone or when checking snapshot root of the path, returns\n   *    /users/username/.Trash\n   * Caller appends either Current or checkpoint timestamp for trash destination\n   * @param path the trash root of the path to be determined.\n   * @return trash root\n   */\n  @Override\n  public Path getTrashRoot(Path path) {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_TRASH_ROOT);\n    if (path == null) {\n      return super.getTrashRoot(null);\n    }\n\n    // Snapshottable directory trash root, not null if path is inside a\n    // snapshottable directory and isSnapshotTrashRootEnabled is true from NN.\n    String ssTrashRoot = null;\n    try {\n      if (dfs.isSnapshotTrashRootEnabled()) {\n        String ssRoot = dfs.getSnapshotRoot(path);\n        if (ssRoot != null) {\n          ssTrashRoot = DFSUtilClient.getSnapshotTrashRoot(ssRoot, dfs.ugi);\n        }\n      }\n    } catch (IOException ioe) {\n      DFSClient.LOG.warn(\"Exception while checking whether the path is in a \"\n          + \"snapshottable directory\", ioe);\n    }\n\n    try {\n      if (!dfs.isHDFSEncryptionEnabled()) {\n        if (ssTrashRoot == null) {\n          // the path is not in a snapshottable directory and EZ is not enabled\n          return super.getTrashRoot(path);\n        } else {\n          return this.makeQualified(new Path(ssTrashRoot));\n        }\n      }\n    } catch (IOException ioe) {\n      DFSClient.LOG.warn(\"Exception while checking whether encryption zone is \"\n          + \"supported\", ioe);\n    }\n\n    // HDFS encryption is enabled on the cluster at this point, does not\n    // necessary mean the given path is in an EZ hence the check.\n    String parentSrc = path.isRoot() ?\n        path.toUri().getPath() : path.getParent().toUri().getPath();\n    String ezTrashRoot = null;\n    try {\n      EncryptionZone ez = dfs.getEZForPath(parentSrc);\n      if ((ez != null)) {\n        ezTrashRoot = DFSUtilClient.getEZTrashRoot(ez, dfs.ugi);\n      }\n    } catch (IOException e) {\n      DFSClient.LOG.warn(\"Exception in checking the encryption zone for the \" +\n          \"path \" + parentSrc + \". \" + e.getMessage());\n    }\n\n    if (ssTrashRoot == null) {\n      if (ezTrashRoot == null) {\n        // The path is neither in a snapshottable directory nor in an EZ\n        return super.getTrashRoot(path);\n      } else {\n        return this.makeQualified(new Path(ezTrashRoot));\n      }\n    } else {\n      if (ezTrashRoot == null) {\n        return this.makeQualified(new Path(ssTrashRoot));\n      } else {\n        // The path is in EZ and in a snapshottable directory\n        return this.makeQualified(new Path(\n            ssTrashRoot.length() > ezTrashRoot.length() ?\n                ssTrashRoot : ezTrashRoot));\n      }\n    }\n  }\n\n  /**\n   * Get all the trash roots of HDFS for current user or for all the users.\n   * 1. File deleted from encryption zones\n   *    e.g., ez1 rooted at /ez1 has its trash root at /ez1/.Trash/$USER\n   * 2. File deleted from snapshottable directories\n   *    if dfs.namenode.snapshot.trashroot.enabled is set to true.\n   *    e.g., snapshottable directory /snapdir1 has its trash root\n   *    at /snapdir1/.Trash/$USER\n   * 3. File deleted from other directories\n   *    /user/username/.Trash\n   * @param allUsers return trashRoots of all users if true, used by emptier\n   * @return trash roots of HDFS\n   */\n  @Override\n  public Collection<FileStatus> getTrashRoots(boolean allUsers) {\n    Set<FileStatus> ret = new HashSet<>();\n    // Get normal trash roots\n    ret.addAll(super.getTrashRoots(allUsers));\n\n    try {\n      // Get EZ Trash roots\n      final RemoteIterator<EncryptionZone> it = dfs.listEncryptionZones();\n      while (it.hasNext()) {\n        EncryptionZone ez = it.next();\n        Path ezTrashRoot = new Path(ez.getPath(),\n            FileSystem.TRASH_PREFIX);\n        if (!exists(ezTrashRoot)) {\n          continue;\n        }\n        if (allUsers) {\n          for (FileStatus candidate : listStatus(ezTrashRoot)) {\n            if (exists(candidate.getPath())) {\n              ret.add(candidate);\n            }\n          }\n        } else {\n          Path userTrash = new Path(DFSUtilClient.getEZTrashRoot(ez, dfs.ugi));\n          try {\n            ret.add(getFileStatus(userTrash));\n          } catch (FileNotFoundException ignored) {\n          }\n        }\n      }\n    } catch (IOException e){\n      DFSClient.LOG.warn(\"Cannot get all encrypted trash roots\", e);\n    }\n\n    try {\n      // Get snapshottable directory trash roots\n      if (dfs.isSnapshotTrashRootEnabled()) {\n        SnapshottableDirectoryStatus[] lst = dfs.getSnapshottableDirListing();\n        if (lst != null) {\n          for (SnapshottableDirectoryStatus dirStatus : lst) {\n            String ssDir = dirStatus.getFullPath().toString();\n            Path ssTrashRoot = new Path(ssDir, FileSystem.TRASH_PREFIX);\n            if (!exists(ssTrashRoot)) {\n              continue;\n            }\n            if (allUsers) {\n              for (FileStatus candidate : listStatus(ssTrashRoot)) {\n                if (exists(candidate.getPath())) {\n                  ret.add(candidate);\n                }\n              }\n            } else {\n              Path userTrash = new Path(DFSUtilClient.getSnapshotTrashRoot(\n                  ssDir, dfs.ugi));\n              try {\n                ret.add(getFileStatus(userTrash));\n              } catch (FileNotFoundException ignored) {\n              }\n            }\n          }\n        }\n      }\n    } catch (IOException e) {\n      DFSClient.LOG.warn(\"Cannot get snapshot trash roots\", e);\n    }\n\n    return ret;\n  }\n\n  @Override\n  protected Path fixRelativePart(Path p) {\n    return super.fixRelativePart(p);\n  }\n\n  Statistics getFsStatistics() {\n    return statistics;\n  }\n\n  DFSOpsCountStatistics getDFSOpsCountStatistics() {\n    return storageStatistics;\n  }\n\n  /**\n   * HdfsDataOutputStreamBuilder provides the HDFS-specific capabilities to\n   * write file on HDFS.\n   */\n  public static final class HdfsDataOutputStreamBuilder\n      extends FSDataOutputStreamBuilder<\n      FSDataOutputStream, HdfsDataOutputStreamBuilder> {\n    private final DistributedFileSystem dfs;\n    private InetSocketAddress[] favoredNodes = null;\n    private String ecPolicyName = null;\n    private String storagePolicyName = null;\n\n    /**\n     * Construct a HdfsDataOutputStream builder for a file.\n     * @param dfs the {@link DistributedFileSystem} instance.\n     * @param path the path of the file to create / append.\n     */\n    private HdfsDataOutputStreamBuilder(DistributedFileSystem dfs, Path path) {\n      super(dfs, path);\n      this.dfs = dfs;\n    }\n\n    @Override\n    public HdfsDataOutputStreamBuilder getThisBuilder() {\n      return this;\n    }\n\n    private InetSocketAddress[] getFavoredNodes() {\n      return favoredNodes;\n    }\n\n    /**\n     * Set favored DataNodes.\n     * @param nodes the addresses of the favored DataNodes.\n     */\n    public HdfsDataOutputStreamBuilder favoredNodes(\n        @Nonnull final InetSocketAddress[] nodes) {\n      Preconditions.checkNotNull(nodes);\n      favoredNodes = nodes.clone();\n      return this;\n    }\n\n    /**\n     * Force closed blocks to disk.\n     *\n     * @see CreateFlag for the details.\n     */\n    public HdfsDataOutputStreamBuilder syncBlock() {\n      getFlags().add(CreateFlag.SYNC_BLOCK);\n      return this;\n    }\n\n    /**\n     * Create the block on transient storage if possible.\n     *\n     * @see CreateFlag for the details.\n     */\n    public HdfsDataOutputStreamBuilder lazyPersist() {\n      getFlags().add(CreateFlag.LAZY_PERSIST);\n      return this;\n    }\n\n    /**\n     * Append data to a new block instead of the end of the last partial block.\n     *\n     * @see CreateFlag for the details.\n     */\n    public HdfsDataOutputStreamBuilder newBlock() {\n      getFlags().add(CreateFlag.NEW_BLOCK);\n      return this;\n    }\n\n    /**\n     * Advise that a block replica NOT be written to the local DataNode.\n     *\n     * @see CreateFlag for the details.\n     */\n    public HdfsDataOutputStreamBuilder noLocalWrite() {\n      getFlags().add(CreateFlag.NO_LOCAL_WRITE);\n      return this;\n    }\n\n    /**\n     * Advise that a block replica NOT be written to the local rack DataNode.\n     *\n     * @see CreateFlag for the details.\n     */\n    public HdfsDataOutputStreamBuilder noLocalRack() {\n      getFlags().add(CreateFlag.NO_LOCAL_RACK);\n      return this;\n    }\n\n    @VisibleForTesting\n    String getStoragePolicyName() {\n      return storagePolicyName;\n    }\n\n    /**\n     * Enforce a file to follow the specified storage policy irrespective of the\n     * storage policy of its parent directory.\n     */\n    public HdfsDataOutputStreamBuilder storagePolicyName(\n        @Nonnull final String policyName) {\n      Preconditions.checkNotNull(policyName);\n      storagePolicyName = policyName;\n      return this;\n    }\n\n    @VisibleForTesting\n    String getEcPolicyName() {\n      return ecPolicyName;\n    }\n\n    /**\n     * Enforce the file to be a striped file with erasure coding policy\n     * 'policyName', no matter what its parent directory's replication\n     * or erasure coding policy is. Don't call this function and\n     * enforceReplicate() in the same builder since they have conflict\n     * of interest.\n     */\n    public HdfsDataOutputStreamBuilder ecPolicyName(\n        @Nonnull final String policyName) {\n      Preconditions.checkNotNull(policyName);\n      ecPolicyName = policyName;\n      return this;\n    }\n\n    @VisibleForTesting\n    boolean shouldReplicate() {\n      return getFlags().contains(CreateFlag.SHOULD_REPLICATE);\n    }\n\n    /**\n     * Enforce the file to be a replicated file, no matter what its parent\n     * directory's replication or erasure coding policy is. Don't call this\n     * function and setEcPolicyName() in the same builder since they have\n     * conflict of interest.\n     */\n    public HdfsDataOutputStreamBuilder replicate() {\n      getFlags().add(CreateFlag.SHOULD_REPLICATE);\n      return this;\n    }\n\n    /**\n     * Advise that the first block replica be written without regard to the\n     * client locality.\n     *\n     * @see CreateFlag for the details.\n     */\n    public HdfsDataOutputStreamBuilder ignoreClientLocality() {\n      getFlags().add(CreateFlag.IGNORE_CLIENT_LOCALITY);\n      return this;\n    }\n\n    @VisibleForTesting\n    @Override\n    protected EnumSet<CreateFlag> getFlags() {\n      return super.getFlags();\n    }\n\n    /**\n     * Build HdfsDataOutputStream to write.\n     *\n     * @return a fully-initialized OutputStream.\n     * @throws IOException on I/O errors.\n     */\n    @Override\n    public FSDataOutputStream build() throws IOException {\n      if (getFlags().contains(CreateFlag.CREATE) ||\n          getFlags().contains(CreateFlag.OVERWRITE)) {\n        if (isRecursive()) {\n          return dfs.create(getPath(), getPermission(), getFlags(),\n              getBufferSize(), getReplication(), getBlockSize(),\n              getProgress(), getChecksumOpt(), getFavoredNodes(),\n              getEcPolicyName(), getStoragePolicyName());\n        } else {\n          return dfs.createNonRecursive(getPath(), getPermission(), getFlags(),\n              getBufferSize(), getReplication(), getBlockSize(), getProgress(),\n              getChecksumOpt(), getFavoredNodes(), getEcPolicyName(),\n              getStoragePolicyName());\n        }\n      } else if (getFlags().contains(CreateFlag.APPEND)) {\n        return dfs.append(getPath(), getFlags(), getBufferSize(), getProgress(),\n            getFavoredNodes());\n      }\n      throw new HadoopIllegalArgumentException(\n          \"Must specify either create or append\");\n    }\n  }\n\n  /**\n   * Create a HdfsDataOutputStreamBuilder to create a file on DFS.\n   * Similar to {@link #create(Path)}, file is overwritten by default.\n   *\n   * @param path the path of the file to create.\n   * @return A HdfsDataOutputStreamBuilder for creating a file.\n   */\n  @Override\n  public HdfsDataOutputStreamBuilder createFile(Path path) {\n    return new HdfsDataOutputStreamBuilder(this, path).create().overwrite(true);\n  }\n\n  /**\n   * Returns a RemoteIterator which can be used to list all open files\n   * currently managed by the NameNode. For large numbers of open files,\n   * iterator will fetch the list in batches of configured size.\n   * <p>\n   * Since the list is fetched in batches, it does not represent a\n   * consistent snapshot of the all open files.\n   * <p>\n   * This method can only be called by HDFS superusers.\n   */\n  @Deprecated\n  public RemoteIterator<OpenFileEntry> listOpenFiles() throws IOException {\n    return dfs.listOpenFiles();\n  }\n\n  @Deprecated\n  public RemoteIterator<OpenFileEntry> listOpenFiles(\n      EnumSet<OpenFilesType> openFilesTypes) throws IOException {\n    return dfs.listOpenFiles(openFilesTypes);\n  }\n\n  public RemoteIterator<OpenFileEntry> listOpenFiles(\n      EnumSet<OpenFilesType> openFilesTypes, String path) throws IOException {\n    Path absF = fixRelativePart(new Path(path));\n    return dfs.listOpenFiles(openFilesTypes, getPathName(absF));\n  }\n\n\n  /**\n   * Create a {@link HdfsDataOutputStreamBuilder} to append a file on DFS.\n   *\n   * @param path file path.\n   * @return A {@link HdfsDataOutputStreamBuilder} for appending a file.\n   */\n  @Override\n  public HdfsDataOutputStreamBuilder appendFile(Path path) {\n    return new HdfsDataOutputStreamBuilder(this, path).append();\n  }\n\n  /**\n   * HDFS client capabilities.\n   * Uses {@link DfsPathCapabilities} to keep {@code WebHdfsFileSystem} in sync.\n   * {@inheritDoc}\n   */\n  @Override\n  public boolean hasPathCapability(final Path path, final String capability)\n      throws IOException {\n    // qualify the path to make sure that it refers to the current FS.\n    final Path p = makeQualified(path);\n    Optional<Boolean> cap = DfsPathCapabilities.hasPathCapability(p,\n        capability);\n    if (cap.isPresent()) {\n      return cap.get();\n    }\n    // this switch is for features which are in the DFS client but not\n    // (yet/ever) in the WebHDFS API.\n    switch (validatePathCapabilityArgs(path, capability)) {\n    case CommonPathCapabilities.FS_EXPERIMENTAL_BATCH_LISTING:\n    case CommonPathCapabilities.LEASE_RECOVERABLE:\n      return true;\n    default:\n      // fall through\n    }\n\n    return super.hasPathCapability(p, capability);\n  }\n\n  @Override\n  public MultipartUploaderBuilder createMultipartUploader(final Path basePath)\n      throws IOException {\n    return new FileSystemMultipartUploaderBuilder(this, basePath);\n  }\n\n  /**\n   * Retrieve stats for slow running datanodes.\n   *\n   * @return An array of slow datanode info.\n   * @throws IOException If an I/O error occurs.\n   */\n  public DatanodeInfo[] getSlowDatanodeStats() throws IOException {\n    return dfs.slowDatanodeReport();\n  }\n\n  /**\n   * Returns LocatedBlocks of the corresponding HDFS file p from offset start\n   * for length len.\n   * This is similar to {@link #getFileBlockLocations(Path, long, long)} except\n   * that it returns LocatedBlocks rather than BlockLocation array.\n   * @param p path representing the file of interest.\n   * @param start offset\n   * @param len length\n   * @return a LocatedBlocks object\n   * @throws IOException\n   */\n  public LocatedBlocks getLocatedBlocks(Path p, long start, long len)\n      throws IOException {\n    final Path absF = fixRelativePart(p);\n    return new FileSystemLinkResolver<LocatedBlocks>() {\n      @Override\n      public LocatedBlocks doCall(final Path p) throws IOException {\n        return dfs.getLocatedBlocks(getPathName(p), start, len);\n      }\n      @Override\n      public LocatedBlocks next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          return myDfs.getLocatedBlocks(p, start, len);\n        }\n        throw new UnsupportedOperationException(\"Cannot getLocatedBlocks \" +\n            \"through a symlink to a non-DistributedFileSystem: \" + fs + \" -> \"+\n            p);\n      }\n    }.resolve(this, absF);\n  }\n}\n"
    }
  ]
}