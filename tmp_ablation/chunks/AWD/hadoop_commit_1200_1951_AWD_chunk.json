{
  "antipattern_type": "AWD",
  "project_name": "hadoop",
  "commit_number": "commit_1200",
  "id": "1951",
  "group_id": 3,
  "chunks": [
    {
      "file_path": "D:\\Disaster\\Codefield\\Code_Python\\Anti-patternRAG\\data\\AWD\\apache\\hadoop\\commit_1200\\1951\\before\\hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "chunk_type": "clientClass",
      "ast_subtree": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.hadoop.fs.s3a;\n\nimport java.io.File;\nimport java.io.FileNotFoundException;\nimport java.io.IOException;\nimport java.io.InterruptedIOException;\nimport java.io.UncheckedIOException;\nimport java.net.URI;\nimport java.nio.file.AccessDeniedException;\nimport java.text.DateFormat;\nimport java.text.SimpleDateFormat;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Date;\nimport java.util.EnumSet;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Locale;\nimport java.util.Map;\nimport java.util.Optional;\nimport java.util.Set;\nimport java.util.Objects;\nimport java.util.TreeSet;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.LinkedBlockingQueue;\nimport java.util.concurrent.ThreadPoolExecutor;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.atomic.AtomicBoolean;\nimport javax.annotation.Nullable;\n\nimport com.amazonaws.AmazonClientException;\nimport com.amazonaws.AmazonServiceException;\nimport com.amazonaws.SdkBaseException;\nimport com.amazonaws.services.s3.AmazonS3;\nimport com.amazonaws.services.s3.Headers;\nimport com.amazonaws.services.s3.model.CannedAccessControlList;\nimport com.amazonaws.services.s3.model.CompleteMultipartUploadRequest;\nimport com.amazonaws.services.s3.model.CompleteMultipartUploadResult;\nimport com.amazonaws.services.s3.model.CopyObjectRequest;\nimport com.amazonaws.services.s3.model.DeleteObjectsRequest;\nimport com.amazonaws.services.s3.model.DeleteObjectsResult;\nimport com.amazonaws.services.s3.model.GetObjectMetadataRequest;\nimport com.amazonaws.services.s3.model.GetObjectRequest;\nimport com.amazonaws.services.s3.model.InitiateMultipartUploadRequest;\nimport com.amazonaws.services.s3.model.InitiateMultipartUploadResult;\nimport com.amazonaws.services.s3.model.ListMultipartUploadsRequest;\nimport com.amazonaws.services.s3.model.ListObjectsRequest;\nimport com.amazonaws.services.s3.model.ListObjectsV2Request;\nimport com.amazonaws.services.s3.model.MultiObjectDeleteException;\nimport com.amazonaws.services.s3.model.MultipartUpload;\nimport com.amazonaws.services.s3.model.ObjectMetadata;\nimport com.amazonaws.services.s3.model.PutObjectRequest;\nimport com.amazonaws.services.s3.model.PutObjectResult;\nimport com.amazonaws.services.s3.model.S3Object;\nimport com.amazonaws.services.s3.model.SelectObjectContentRequest;\nimport com.amazonaws.services.s3.model.SelectObjectContentResult;\nimport com.amazonaws.services.s3.model.StorageClass;\nimport com.amazonaws.services.s3.model.UploadPartRequest;\nimport com.amazonaws.services.s3.model.UploadPartResult;\nimport com.amazonaws.services.s3.transfer.Copy;\nimport com.amazonaws.services.s3.transfer.TransferManager;\nimport com.amazonaws.services.s3.transfer.TransferManagerConfiguration;\nimport com.amazonaws.services.s3.transfer.Upload;\nimport com.amazonaws.services.s3.transfer.model.CopyResult;\nimport com.amazonaws.services.s3.transfer.model.UploadResult;\nimport com.amazonaws.event.ProgressListener;\n\nimport org.apache.hadoop.fs.impl.prefetch.ExecutorServiceFuturePool;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport org.apache.commons.lang3.tuple.Pair;\nimport org.apache.hadoop.classification.InterfaceAudience;\nimport org.apache.hadoop.classification.InterfaceStability;\nimport org.apache.hadoop.classification.VisibleForTesting;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.CommonPathCapabilities;\nimport org.apache.hadoop.fs.ContentSummary;\nimport org.apache.hadoop.fs.CreateFlag;\nimport org.apache.hadoop.fs.FSDataInputStream;\nimport org.apache.hadoop.fs.FSDataOutputStream;\nimport org.apache.hadoop.fs.FSDataOutputStreamBuilder;\nimport org.apache.hadoop.fs.Globber;\nimport org.apache.hadoop.fs.Options;\nimport org.apache.hadoop.fs.impl.OpenFileParameters;\nimport org.apache.hadoop.fs.permission.FsAction;\nimport org.apache.hadoop.fs.s3a.audit.AuditSpanS3A;\nimport org.apache.hadoop.fs.s3a.auth.SignerManager;\nimport org.apache.hadoop.fs.s3a.auth.delegation.DelegationOperations;\nimport org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider;\nimport org.apache.hadoop.fs.s3a.impl.BulkDeleteRetryHandler;\nimport org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy;\nimport org.apache.hadoop.fs.s3a.impl.ContextAccessors;\nimport org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation;\nimport org.apache.hadoop.fs.s3a.impl.CopyOutcome;\nimport org.apache.hadoop.fs.s3a.impl.CreateFileBuilder;\nimport org.apache.hadoop.fs.s3a.impl.DeleteOperation;\nimport org.apache.hadoop.fs.s3a.impl.DirectoryPolicy;\nimport org.apache.hadoop.fs.s3a.impl.DirectoryPolicyImpl;\nimport org.apache.hadoop.fs.s3a.impl.GetContentSummaryOperation;\nimport org.apache.hadoop.fs.s3a.impl.HeaderProcessing;\nimport org.apache.hadoop.fs.s3a.impl.InternalConstants;\nimport org.apache.hadoop.fs.s3a.impl.ListingOperationCallbacks;\nimport org.apache.hadoop.fs.s3a.impl.MkdirOperation;\nimport org.apache.hadoop.fs.s3a.impl.OpenFileSupport;\nimport org.apache.hadoop.fs.s3a.impl.OperationCallbacks;\nimport org.apache.hadoop.fs.s3a.impl.PutObjectOptions;\nimport org.apache.hadoop.fs.s3a.impl.RenameOperation;\nimport org.apache.hadoop.fs.s3a.impl.RequestFactoryImpl;\nimport org.apache.hadoop.fs.s3a.impl.S3AMultipartUploaderBuilder;\nimport org.apache.hadoop.fs.s3a.impl.StatusProbeEnum;\nimport org.apache.hadoop.fs.s3a.impl.StoreContext;\nimport org.apache.hadoop.fs.s3a.impl.StoreContextBuilder;\nimport org.apache.hadoop.fs.s3a.impl.V2Migration;\nimport org.apache.hadoop.fs.s3a.prefetch.S3APrefetchingInputStream;\nimport org.apache.hadoop.fs.s3a.tools.MarkerToolOperations;\nimport org.apache.hadoop.fs.s3a.tools.MarkerToolOperationsImpl;\nimport org.apache.hadoop.fs.statistics.DurationTracker;\nimport org.apache.hadoop.fs.statistics.DurationTrackerFactory;\nimport org.apache.hadoop.fs.statistics.IOStatistics;\nimport org.apache.hadoop.fs.statistics.IOStatisticsSource;\nimport org.apache.hadoop.fs.statistics.IOStatisticsContext;\nimport org.apache.hadoop.fs.statistics.impl.IOStatisticsStore;\nimport org.apache.hadoop.fs.store.audit.AuditEntryPoint;\nimport org.apache.hadoop.fs.store.audit.ActiveThreadSpanSource;\nimport org.apache.hadoop.fs.store.audit.AuditSpan;\nimport org.apache.hadoop.fs.store.audit.AuditSpanSource;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.security.AccessControlException;\nimport org.apache.hadoop.security.token.DelegationTokenIssuer;\nimport org.apache.hadoop.security.token.TokenIdentifier;\nimport org.apache.hadoop.util.DurationInfo;\nimport org.apache.hadoop.util.LambdaUtils;\nimport org.apache.hadoop.util.Lists;\nimport org.apache.hadoop.util.Preconditions;\nimport org.apache.hadoop.fs.FileAlreadyExistsException;\nimport org.apache.hadoop.fs.FileStatus;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.GlobalStorageStatistics;\nimport org.apache.hadoop.fs.InvalidRequestException;\nimport org.apache.hadoop.fs.LocalDirAllocator;\nimport org.apache.hadoop.fs.LocalFileSystem;\nimport org.apache.hadoop.fs.LocatedFileStatus;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.fs.PathFilter;\nimport org.apache.hadoop.fs.PathIOException;\nimport org.apache.hadoop.fs.RemoteIterator;\nimport org.apache.hadoop.fs.StreamCapabilities;\nimport org.apache.hadoop.fs.permission.FsPermission;\nimport org.apache.hadoop.fs.s3a.api.RequestFactory;\nimport org.apache.hadoop.fs.s3a.audit.AuditManagerS3A;\nimport org.apache.hadoop.fs.s3a.audit.AuditIntegration;\nimport org.apache.hadoop.fs.s3a.audit.OperationAuditor;\nimport org.apache.hadoop.fs.s3a.auth.RoleModel;\nimport org.apache.hadoop.fs.s3a.auth.delegation.AWSPolicyProvider;\nimport org.apache.hadoop.fs.s3a.auth.delegation.EncryptionSecrets;\nimport org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens;\nimport org.apache.hadoop.fs.s3a.auth.delegation.AbstractS3ATokenIdentifier;\nimport org.apache.hadoop.fs.s3a.commit.CommitConstants;\nimport org.apache.hadoop.fs.s3a.commit.PutTracker;\nimport org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration;\nimport org.apache.hadoop.fs.s3a.impl.ChangeTracker;\nimport org.apache.hadoop.fs.s3a.select.SelectBinding;\nimport org.apache.hadoop.fs.s3a.select.SelectConstants;\nimport org.apache.hadoop.fs.s3a.s3guard.S3Guard;\nimport org.apache.hadoop.fs.s3a.statistics.BlockOutputStreamStatistics;\nimport org.apache.hadoop.fs.s3a.statistics.CommitterStatistics;\nimport org.apache.hadoop.fs.s3a.statistics.S3AInputStreamStatistics;\nimport org.apache.hadoop.fs.s3a.statistics.S3AStatisticsContext;\nimport org.apache.hadoop.fs.s3a.statistics.impl.BondedS3AStatisticsContext;\nimport org.apache.hadoop.fs.s3native.S3xLoginHelper;\nimport org.apache.hadoop.io.retry.RetryPolicies;\nimport org.apache.hadoop.fs.store.EtagChecksum;\nimport org.apache.hadoop.security.UserGroupInformation;\nimport org.apache.hadoop.util.BlockingThreadPoolExecutorService;\nimport org.apache.hadoop.security.ProviderUtils;\nimport org.apache.hadoop.security.token.Token;\nimport org.apache.hadoop.util.Progressable;\nimport org.apache.hadoop.util.ReflectionUtils;\nimport org.apache.hadoop.util.SemaphoredDelegatingExecutor;\nimport org.apache.hadoop.util.concurrent.HadoopExecutors;\nimport org.apache.hadoop.util.functional.CallableRaisingIOE;\n\nimport static java.util.Objects.requireNonNull;\nimport static org.apache.hadoop.fs.CommonConfigurationKeys.IOSTATISTICS_LOGGING_LEVEL;\nimport static org.apache.hadoop.fs.CommonConfigurationKeys.IOSTATISTICS_LOGGING_LEVEL_DEFAULT;\nimport static org.apache.hadoop.fs.CommonConfigurationKeysPublic.IO_FILE_BUFFER_SIZE_DEFAULT;\nimport static org.apache.hadoop.fs.CommonConfigurationKeysPublic.IO_FILE_BUFFER_SIZE_KEY;\nimport static org.apache.hadoop.fs.impl.PathCapabilitiesSupport.validatePathCapabilityArgs;\nimport static org.apache.hadoop.fs.s3a.Constants.*;\nimport static org.apache.hadoop.fs.s3a.Invoker.*;\nimport static org.apache.hadoop.fs.s3a.Listing.toLocatedFileStatusIterator;\nimport static org.apache.hadoop.fs.s3a.S3AUtils.*;\nimport static org.apache.hadoop.fs.s3a.Statistic.*;\nimport static org.apache.hadoop.fs.s3a.audit.S3AAuditConstants.INITIALIZE_SPAN;\nimport static org.apache.hadoop.fs.s3a.auth.RolePolicies.STATEMENT_ALLOW_SSE_KMS_RW;\nimport static org.apache.hadoop.fs.s3a.auth.RolePolicies.allowS3Operations;\nimport static org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens.TokenIssuingPolicy.NoTokensAvailable;\nimport static org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens.hasDelegationTokenBinding;\nimport static org.apache.hadoop.fs.s3a.commit.CommitConstants.FS_S3A_COMMITTER_ABORT_PENDING_UPLOADS;\nimport static org.apache.hadoop.fs.s3a.commit.CommitConstants.FS_S3A_COMMITTER_STAGING_ABORT_PENDING_UPLOADS;\nimport static org.apache.hadoop.fs.s3a.impl.CallableSupplier.submit;\nimport static org.apache.hadoop.fs.s3a.impl.CreateFileBuilder.OPTIONS_CREATE_FILE_NO_OVERWRITE;\nimport static org.apache.hadoop.fs.s3a.impl.CreateFileBuilder.OPTIONS_CREATE_FILE_OVERWRITE;\nimport static org.apache.hadoop.fs.s3a.impl.ErrorTranslation.isObjectNotFound;\nimport static org.apache.hadoop.fs.s3a.impl.ErrorTranslation.isUnknownBucket;\nimport static org.apache.hadoop.fs.s3a.impl.InternalConstants.AP_INACCESSIBLE;\nimport static org.apache.hadoop.fs.s3a.impl.InternalConstants.AP_REQUIRED_EXCEPTION;\nimport static org.apache.hadoop.fs.s3a.impl.InternalConstants.ARN_BUCKET_OPTION;\nimport static org.apache.hadoop.fs.s3a.impl.InternalConstants.CSE_PADDING_LENGTH;\nimport static org.apache.hadoop.fs.s3a.impl.InternalConstants.DEFAULT_UPLOAD_PART_COUNT_LIMIT;\nimport static org.apache.hadoop.fs.s3a.impl.InternalConstants.DELETE_CONSIDERED_IDEMPOTENT;\nimport static org.apache.hadoop.fs.s3a.impl.InternalConstants.SC_403;\nimport static org.apache.hadoop.fs.s3a.impl.InternalConstants.SC_404;\nimport static org.apache.hadoop.fs.s3a.impl.InternalConstants.UPLOAD_PART_COUNT_LIMIT;\nimport static org.apache.hadoop.fs.s3a.impl.NetworkBinding.fixBucketRegion;\nimport static org.apache.hadoop.fs.s3a.impl.NetworkBinding.logDnsLookup;\nimport static org.apache.hadoop.fs.s3a.s3guard.S3Guard.checkNoS3Guard;\nimport static org.apache.hadoop.fs.statistics.IOStatisticsLogging.logIOStatisticsAtLevel;\nimport static org.apache.hadoop.fs.statistics.StoreStatisticNames.OBJECT_CONTINUE_LIST_REQUEST;\nimport static org.apache.hadoop.fs.statistics.StoreStatisticNames.OBJECT_LIST_REQUEST;\nimport static org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.pairedTrackerFactory;\nimport static org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration;\nimport static org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation;\nimport static org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfOperation;\nimport static org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfSupplier;\nimport static org.apache.hadoop.io.IOUtils.cleanupWithLogger;\nimport static org.apache.hadoop.util.Preconditions.checkArgument;\nimport static org.apache.hadoop.util.functional.RemoteIterators.typeCastingRemoteIterator;\n\n/**\n * The core S3A Filesystem implementation.\n *\n * This subclass is marked as private as code should not be creating it\n * directly; use {@link FileSystem#get(Configuration)} and variants to\n * create one.\n *\n * If cast to {@code S3AFileSystem}, extra methods and features may be accessed.\n * Consider those private and unstable.\n *\n * Because it prints some of the state of the instrumentation,\n * the output of {@link #toString()} must also be considered unstable.\n */\n@InterfaceAudience.Private\n@InterfaceStability.Evolving\npublic class S3AFileSystem extends FileSystem implements StreamCapabilities,\n    AWSPolicyProvider, DelegationTokenProvider, IOStatisticsSource,\n    AuditSpanSource<AuditSpanS3A>, ActiveThreadSpanSource<AuditSpanS3A> {\n\n  /**\n   * Default blocksize as used in blocksize and FS status queries.\n   */\n  public static final int DEFAULT_BLOCKSIZE = 32 * 1024 * 1024;\n\n  private URI uri;\n  private Path workingDir;\n  private String username;\n  private AmazonS3 s3;\n  // initial callback policy is fail-once; it's there just to assist\n  // some mock tests and other codepaths trying to call the low level\n  // APIs on an uninitialized filesystem.\n  private Invoker invoker = new Invoker(RetryPolicies.TRY_ONCE_THEN_FAIL,\n      Invoker.LOG_EVENT);\n\n  private final Retried onRetry = this::operationRetried;\n\n  /**\n   * Represents bucket name for all S3 operations. If per bucket override for\n   * {@link InternalConstants#ARN_BUCKET_OPTION} property  is set, then the bucket is updated to\n   * point to the configured Arn.\n   */\n  private String bucket;\n  private int maxKeys;\n  private Listing listing;\n  private long partSize;\n  private boolean enableMultiObjectsDelete;\n  private TransferManager transfers;\n  private ExecutorService boundedThreadPool;\n  private ThreadPoolExecutor unboundedThreadPool;\n\n  // S3 reads are prefetched asynchronously using this future pool.\n  private ExecutorServiceFuturePool futurePool;\n\n  // If true, the prefetching input stream is used for reads.\n  private boolean prefetchEnabled;\n\n  // Size in bytes of a single prefetch block.\n  private int prefetchBlockSize;\n\n  // Size of prefetch queue (in number of blocks).\n  private int prefetchBlockCount;\n\n  private int executorCapacity;\n  private long multiPartThreshold;\n  public static final Logger LOG = LoggerFactory.getLogger(S3AFileSystem.class);\n  private static final Logger PROGRESS =\n      LoggerFactory.getLogger(\"org.apache.hadoop.fs.s3a.S3AFileSystem.Progress\");\n  private LocalDirAllocator directoryAllocator;\n  private CannedAccessControlList cannedACL;\n\n  /**\n   * This must never be null; until initialized it just declares that there\n   * is no encryption.\n   */\n  private EncryptionSecrets encryptionSecrets = new EncryptionSecrets();\n  /** The core instrumentation. */\n  private S3AInstrumentation instrumentation;\n  /** Accessors to statistics for this FS. */\n  private S3AStatisticsContext statisticsContext;\n  /** Storage Statistics Bonded to the instrumentation. */\n  private S3AStorageStatistics storageStatistics;\n\n  /**\n   * Default input policy; may be overridden in\n   * {@code openFile()}.\n   */\n  private S3AInputPolicy inputPolicy;\n  /** Vectored IO context. */\n  private VectoredIOContext vectoredIOContext;\n\n  /**\n   * Maximum number of active range read operation a single\n   * input stream can have.\n   */\n  private int vectoredActiveRangeReads;\n\n  private long readAhead;\n  private ChangeDetectionPolicy changeDetectionPolicy;\n  private final AtomicBoolean closed = new AtomicBoolean(false);\n  private volatile boolean isClosed = false;\n  private Collection<String> allowAuthoritativePaths;\n\n  /** Delegation token integration; non-empty when DT support is enabled. */\n  private Optional<S3ADelegationTokens> delegationTokens = Optional.empty();\n\n  /** Principal who created the FS; recorded during initialization. */\n  private UserGroupInformation owner;\n\n  private String blockOutputBuffer;\n  private S3ADataBlocks.BlockFactory blockFactory;\n  private int blockOutputActiveBlocks;\n  private boolean useListV1;\n  private MagicCommitIntegration committerIntegration;\n\n  private AWSCredentialProviderList credentials;\n  private SignerManager signerManager;\n\n  /**\n   * Page size for deletions.\n   */\n  private int pageSize;\n\n  private final ListingOperationCallbacks listingOperationCallbacks =\n          new ListingOperationCallbacksImpl();\n\n  /**\n   * Helper for the openFile() method.\n   */\n  private OpenFileSupport openFileHelper;\n\n  /**\n   * Directory policy.\n   */\n  private DirectoryPolicy directoryPolicy;\n\n  /**\n   * Context accessors for re-use.\n   */\n  private final ContextAccessors contextAccessors = new ContextAccessorsImpl();\n\n  /**\n   * Factory for AWS requests.\n   */\n  private RequestFactory requestFactory;\n\n  /**\n   * Audit manager (service lifecycle).\n   * Creates the audit service and manages the binding of different audit spans\n   * to different threads.\n   * Initially this is a no-op manager; once the service is initialized it will\n   * be replaced with a configured one.\n   */\n  private AuditManagerS3A auditManager =\n      AuditIntegration.stubAuditManager();\n\n  /**\n   * Is this S3A FS instance using S3 client side encryption?\n   */\n  private boolean isCSEEnabled;\n\n  /**\n   * Bucket AccessPoint.\n   */\n  private ArnResource accessPoint;\n\n  /**\n   * Does this S3A FS instance have multipart upload enabled?\n   */\n  private boolean isMultipartUploadEnabled = DEFAULT_MULTIPART_UPLOAD_ENABLED;\n\n  /**\n   * A cache of files that should be deleted when the FileSystem is closed\n   * or the JVM is exited.\n   */\n  private final Set<Path> deleteOnExit = new TreeSet<>();\n\n  /**\n   * Scheme for the current filesystem.\n   */\n  private String scheme = FS_S3A;\n\n  /** Add any deprecated keys. */\n  @SuppressWarnings(\"deprecation\")\n  private static void addDeprecatedKeys() {\n    Configuration.DeprecationDelta[] deltas = {\n        new Configuration.DeprecationDelta(\n            FS_S3A_COMMITTER_STAGING_ABORT_PENDING_UPLOADS,\n            FS_S3A_COMMITTER_ABORT_PENDING_UPLOADS),\n        new Configuration.DeprecationDelta(\n            SERVER_SIDE_ENCRYPTION_ALGORITHM,\n            S3_ENCRYPTION_ALGORITHM),\n        new Configuration.DeprecationDelta(\n            SERVER_SIDE_ENCRYPTION_KEY,\n            S3_ENCRYPTION_KEY)\n    };\n\n    if (deltas.length > 0) {\n      Configuration.addDeprecations(deltas);\n      Configuration.reloadExistingConfigurations();\n    }\n  }\n\n  static {\n    addDeprecatedKeys();\n  }\n\n  /** Called after a new FileSystem instance is constructed.\n   * @param name a uri whose authority section names the host, port, etc.\n   *   for this FileSystem\n   * @param originalConf the configuration to use for the FS. The\n   * bucket-specific options are patched over the base ones before any use is\n   * made of the config.\n   */\n  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket = name.getHost();\n    AuditSpan span = null;\n    try {\n      LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n      if (LOG.isTraceEnabled()) {\n        // log a full trace for deep diagnostics of where an object is created,\n        // for tracking down memory leak issues.\n        LOG.trace(\"Filesystem for {} created; fs.s3a.impl.disable.cache = {}\",\n            name, originalConf.getBoolean(\"fs.s3a.impl.disable.cache\", false),\n            new RuntimeException(super.toString()));\n      }\n      // clone the configuration into one with propagated bucket options\n      Configuration conf = propagateBucketOptions(originalConf, bucket);\n      // HADOOP-17894. remove references to s3a stores in JCEKS credentials.\n      conf = ProviderUtils.excludeIncompatibleCredentialProviders(\n          conf, S3AFileSystem.class);\n      String arn = String.format(ARN_BUCKET_OPTION, bucket);\n      String configuredArn = conf.getTrimmed(arn, \"\");\n      if (!configuredArn.isEmpty()) {\n        accessPoint = ArnResource.accessPointFromArn(configuredArn);\n        LOG.info(\"Using AccessPoint ARN \\\"{}\\\" for bucket {}\", configuredArn, bucket);\n        bucket = accessPoint.getFullArn();\n      } else if (conf.getBoolean(AWS_S3_ACCESSPOINT_REQUIRED, false)) {\n        LOG.warn(\"Access Point usage is required because \\\"{}\\\" is enabled,\" +\n            \" but not configured for the bucket: {}\", AWS_S3_ACCESSPOINT_REQUIRED, bucket);\n        throw new PathIOException(bucket, AP_REQUIRED_EXCEPTION);\n      }\n\n      // fix up the classloader of the configuration to be whatever\n      // classloader loaded this filesystem.\n      // See: HADOOP-17372\n      conf.setClassLoader(this.getClass().getClassLoader());\n\n      // patch the Hadoop security providers\n      patchSecurityCredentialProviders(conf);\n      // look for delegation token support early.\n      boolean delegationTokensEnabled = hasDelegationTokenBinding(conf);\n      if (delegationTokensEnabled) {\n        LOG.debug(\"Using delegation tokens\");\n      }\n      // set the URI, this will do any fixup of the URI to remove secrets,\n      // canonicalize.\n      setUri(name, delegationTokensEnabled);\n      super.initialize(uri, conf);\n      setConf(conf);\n\n      // look for encryption data\n      // DT Bindings may override this\n      setEncryptionSecrets(\n          buildEncryptionSecrets(bucket, conf));\n\n      invoker = new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n      instrumentation = new S3AInstrumentation(uri);\n      initializeStatisticsBinding();\n      // If CSE-KMS method is set then CSE is enabled.\n      isCSEEnabled = S3AEncryptionMethods.CSE_KMS.getMethod()\n          .equals(getS3EncryptionAlgorithm().getMethod());\n      LOG.debug(\"Client Side Encryption enabled: {}\", isCSEEnabled);\n      setCSEGauge();\n      // Username is the current user at the time the FS was instantiated.\n      owner = UserGroupInformation.getCurrentUser();\n      username = owner.getShortUserName();\n      workingDir = new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      maxKeys = intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      partSize = getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold = getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete = conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      this.prefetchEnabled = conf.getBoolean(PREFETCH_ENABLED_KEY, PREFETCH_ENABLED_DEFAULT);\n      long prefetchBlockSizeLong =\n          longBytesOption(conf, PREFETCH_BLOCK_SIZE_KEY, PREFETCH_BLOCK_DEFAULT_SIZE, 1);\n      if (prefetchBlockSizeLong > (long) Integer.MAX_VALUE) {\n        throw new IOException(\"S3A prefatch block size exceeds int limit\");\n      }\n      this.prefetchBlockSize = (int) prefetchBlockSizeLong;\n      this.prefetchBlockCount =\n          intOption(conf, PREFETCH_BLOCK_COUNT_KEY, PREFETCH_BLOCK_DEFAULT_COUNT, 1);\n      this.isMultipartUploadEnabled = conf.getBoolean(MULTIPART_UPLOADS_ENABLED,\n          DEFAULT_MULTIPART_UPLOAD_ENABLED);\n      initThreadPools(conf);\n\n      int listVersion = conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion < 1 || listVersion > 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 = (listVersion == 1);\n      if (accessPoint != null && useListV1) {\n        LOG.warn(\"V1 list configured in fs.s3a.list.version. This is not supported in by\" +\n            \" access points. Upgrading to V2\");\n        useListV1 = false;\n      }\n\n      signerManager = new SignerManager(bucket, this, conf, owner);\n      signerManager.initCustomSigners();\n\n      // start auditing\n      initializeAuditService();\n\n      // create the requestFactory.\n      // requires the audit manager to be initialized.\n      requestFactory = createRequestFactory();\n\n      // create an initial span for all other operations.\n      span = createSpan(INITIALIZE_SPAN, bucket, null);\n\n      // creates the AWS client, including overriding auth chain if\n      // the FS came with a DT\n      // this may do some patching of the configuration (e.g. setting\n      // the encryption algorithms)\n      bindAWSClient(name, delegationTokensEnabled);\n\n      initTransferManager();\n\n\n      // This initiates a probe against S3 for the bucket existing.\n      doBucketProbing();\n\n      inputPolicy = S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE,\n              Options.OpenFileOptions.FS_OPTION_OPENFILE_READ_POLICY_DEFAULT),\n          S3AInputPolicy.Normal);\n      LOG.debug(\"Input fadvise policy = {}\", inputPolicy);\n      changeDetectionPolicy = ChangeDetectionPolicy.getPolicy(conf);\n      LOG.debug(\"Change detection policy = {}\", changeDetectionPolicy);\n      boolean magicCommitterEnabled = conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration = new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      boolean blockUploadEnabled = conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer = conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      blockFactory = S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks = intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      // If CSE is enabled, do multipart uploads serially.\n      if (isCSEEnabled) {\n        blockOutputActiveBlocks = 1;\n      }\n      LOG.debug(\"Using S3ABlockOutputStream with buffer = {}; block={};\" +\n              \" queue limit={}; multipart={}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks, isMultipartUploadEnabled);\n      // verify there's no S3Guard in the store config.\n      checkNoS3Guard(this.getUri(), getConf());\n\n      allowAuthoritativePaths = S3Guard.getAuthoritativePaths(this);\n\n      // directory policy, which may look at authoritative paths\n      directoryPolicy = DirectoryPolicyImpl.getDirectoryPolicy(conf,\n          this::allowAuthoritative);\n      LOG.debug(\"Directory marker retention policy is {}\", directoryPolicy);\n\n      initMultipartUploads(conf);\n\n      pageSize = intOption(getConf(), BULK_DELETE_PAGE_SIZE,\n          BULK_DELETE_PAGE_SIZE_DEFAULT, 0);\n      checkArgument(pageSize <= InternalConstants.MAX_ENTRIES_TO_DELETE,\n              \"page size out of range: %s\", pageSize);\n      listing = new Listing(listingOperationCallbacks, createStoreContext());\n      // now the open file logic\n      openFileHelper = new OpenFileSupport(\n          changeDetectionPolicy,\n          longBytesOption(conf, READAHEAD_RANGE,\n              DEFAULT_READAHEAD_RANGE, 0),\n          username,\n          intOption(conf, IO_FILE_BUFFER_SIZE_KEY,\n              IO_FILE_BUFFER_SIZE_DEFAULT, 0),\n          longBytesOption(conf, ASYNC_DRAIN_THRESHOLD,\n                        DEFAULT_ASYNC_DRAIN_THRESHOLD, 0),\n          inputPolicy);\n      vectoredActiveRangeReads = intOption(conf,\n              AWS_S3_VECTOR_ACTIVE_RANGE_READS, DEFAULT_AWS_S3_VECTOR_ACTIVE_RANGE_READS, 1);\n      vectoredIOContext = populateVectoredIOContext(conf);\n      scheme = (this.uri != null && this.uri.getScheme() != null) ? this.uri.getScheme() : FS_S3A;\n    } catch (AmazonClientException e) {\n      // amazon client exception: stop all services then throw the translation\n      cleanupWithLogger(LOG, span);\n      stopAllServices();\n      throw translateException(\"initializing \", new Path(name), e);\n    } catch (IOException | RuntimeException e) {\n      // other exceptions: stop the services.\n      cleanupWithLogger(LOG, span);\n      stopAllServices();\n      throw e;\n    }\n  }\n\n  /**\n   * Populates the configurations related to vectored IO operation\n   * in the context which has to passed down to input streams.\n   * @param conf configuration object.\n   * @return VectoredIOContext.\n   */\n  private VectoredIOContext populateVectoredIOContext(Configuration conf) {\n    final int minSeekVectored = (int) longBytesOption(conf, AWS_S3_VECTOR_READS_MIN_SEEK_SIZE,\n            DEFAULT_AWS_S3_VECTOR_READS_MIN_SEEK_SIZE, 0);\n    final int maxReadSizeVectored = (int) longBytesOption(conf, AWS_S3_VECTOR_READS_MAX_MERGED_READ_SIZE,\n            DEFAULT_AWS_S3_VECTOR_READS_MAX_MERGED_READ_SIZE, 0);\n    return new VectoredIOContext()\n            .setMinSeekForVectoredReads(minSeekVectored)\n            .setMaxReadSizeForVectoredReads(maxReadSizeVectored)\n            .build();\n  }\n\n  /**\n   * Set the client side encryption gauge to 0 or 1, indicating if CSE is\n   * enabled through the gauge or not.\n   */\n  private void setCSEGauge() {\n    IOStatisticsStore ioStatisticsStore =\n        (IOStatisticsStore) getIOStatistics();\n    if (isCSEEnabled) {\n      ioStatisticsStore\n          .setGauge(CLIENT_SIDE_ENCRYPTION_ENABLED.getSymbol(), 1L);\n    } else {\n      ioStatisticsStore\n          .setGauge(CLIENT_SIDE_ENCRYPTION_ENABLED.getSymbol(), 0L);\n    }\n  }\n\n  /**\n   * Test bucket existence in S3.\n   * When the value of {@link Constants#S3A_BUCKET_PROBE} is set to 0,\n   * bucket existence check is not done to improve performance of\n   * S3AFileSystem initialization. When set to 1 or 2, bucket existence check\n   * will be performed which is potentially slow.\n   * If 3 or higher: warn and use the v2 check.\n   * Also logging DNS address of the s3 endpoint if the bucket probe value is\n   * greater than 0 else skipping it for increased performance.\n   * @throws UnknownStoreException the bucket is absent\n   * @throws IOException any other problem talking to S3\n   */\n  @Retries.RetryTranslated\n  private void doBucketProbing() throws IOException {\n    int bucketProbe = getConf()\n            .getInt(S3A_BUCKET_PROBE, S3A_BUCKET_PROBE_DEFAULT);\n    Preconditions.checkArgument(bucketProbe >= 0,\n            \"Value of \" + S3A_BUCKET_PROBE + \" should be >= 0\");\n    switch (bucketProbe) {\n    case 0:\n      LOG.debug(\"skipping check for bucket existence\");\n      break;\n    case 1:\n      logDnsLookup(getConf());\n      verifyBucketExists();\n      break;\n    case 2:\n      logDnsLookup(getConf());\n      verifyBucketExistsV2();\n      break;\n    default:\n      // we have no idea what this is, assume it is from a later release.\n      LOG.warn(\"Unknown bucket probe option {}: {}; falling back to check #2\",\n          S3A_BUCKET_PROBE, bucketProbe);\n      verifyBucketExistsV2();\n      break;\n    }\n  }\n\n  /**\n   * Initialize the statistics binding.\n   * This is done by creating an {@code IntegratedS3AStatisticsContext}\n   * with callbacks to get the FS's instrumentation and FileSystem.statistics\n   * field; the latter may change after {@link #initialize(URI, Configuration)},\n   * so needs to be dynamically adapted.\n   * Protected so that (mock) subclasses can replace it with a\n   * different statistics binding, if desired.\n   */\n  protected void initializeStatisticsBinding() {\n    storageStatistics = createStorageStatistics(\n        requireNonNull(getIOStatistics()));\n    statisticsContext = new BondedS3AStatisticsContext(\n        new BondedS3AStatisticsContext.S3AFSStatisticsSource() {\n\n          @Override\n          public S3AInstrumentation getInstrumentation() {\n            return S3AFileSystem.this.getInstrumentation();\n          }\n\n          @Override\n          public Statistics getInstanceStatistics() {\n            return S3AFileSystem.this.statistics;\n          }\n        });\n  }\n\n  /**\n   * Initialize the thread pool.\n   * This must be re-invoked after replacing the S3Client during test\n   * runs.\n   * @param conf configuration.\n   */\n  private void initThreadPools(Configuration conf) {\n    final String name = \"s3a-transfer-\" + getBucket();\n    int maxThreads = conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n    if (maxThreads < 2) {\n      LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n      maxThreads = 2;\n    }\n    int totalTasks = intOption(conf,\n        MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n    long keepAliveTime = longOption(conf, KEEPALIVE_TIME,\n        DEFAULT_KEEPALIVE_TIME, 0);\n    int numPrefetchThreads = this.prefetchEnabled ? this.prefetchBlockCount : 0;\n\n    int activeTasksForBoundedThreadPool = maxThreads;\n    int waitingTasksForBoundedThreadPool = maxThreads + totalTasks + numPrefetchThreads;\n    boundedThreadPool = BlockingThreadPoolExecutorService.newInstance(\n        activeTasksForBoundedThreadPool,\n        waitingTasksForBoundedThreadPool,\n        keepAliveTime, TimeUnit.SECONDS,\n        name + \"-bounded\");\n    unboundedThreadPool = new ThreadPoolExecutor(\n        maxThreads, Integer.MAX_VALUE,\n        keepAliveTime, TimeUnit.SECONDS,\n        new LinkedBlockingQueue<>(),\n        BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n            name + \"-unbounded\"));\n    unboundedThreadPool.allowCoreThreadTimeOut(true);\n    executorCapacity = intOption(conf,\n        EXECUTOR_CAPACITY, DEFAULT_EXECUTOR_CAPACITY, 1);\n    if (prefetchEnabled) {\n      final S3AInputStreamStatistics s3AInputStreamStatistics =\n          statisticsContext.newInputStreamStatistics();\n      futurePool = new ExecutorServiceFuturePool(\n          new SemaphoredDelegatingExecutor(\n              boundedThreadPool,\n              activeTasksForBoundedThreadPool + waitingTasksForBoundedThreadPool,\n              true,\n              s3AInputStreamStatistics));\n    }\n  }\n\n  /**\n   * Create the storage statistics or bind to an existing one.\n   * @param ioStatistics IOStatistics to build the storage statistics from.\n   * @return a storage statistics instance; expected to be that of the FS.\n   */\n  protected static S3AStorageStatistics createStorageStatistics(\n      final IOStatistics ioStatistics) {\n    return (S3AStorageStatistics)\n        GlobalStorageStatistics.INSTANCE\n            .put(S3AStorageStatistics.NAME,\n                () -> new S3AStorageStatistics(ioStatistics));\n  }\n\n  /**\n   * Verify that the bucket exists. This does not check permissions,\n   * not even read access.\n   * Retry policy: retrying, translated.\n   * @throws UnknownStoreException the bucket is absent\n   * @throws IOException any other problem talking to S3\n   */\n  @Retries.RetryTranslated\n  protected void verifyBucketExists()\n      throws UnknownStoreException, IOException {\n    if (!invoker.retry(\"doesBucketExist\", bucket, true,\n        trackDurationOfOperation(getDurationTrackerFactory(),\n            STORE_EXISTS_PROBE.getSymbol(),\n            () -> s3.doesBucketExist(bucket)))) {\n      throw new UnknownStoreException(\"s3a://\" + bucket + \"/\", \" Bucket does \"\n          + \"not exist\");\n    }\n  }\n\n  /**\n   * Verify that the bucket exists. This will correctly throw an exception\n   * when credentials are invalid.\n   * Retry policy: retrying, translated.\n   * @throws UnknownStoreException the bucket is absent\n   * @throws IOException any other problem talking to S3\n   */\n  @Retries.RetryTranslated\n  protected void verifyBucketExistsV2()\n      throws UnknownStoreException, IOException {\n    if (!invoker.retry(\"doesBucketExistV2\", bucket, true,\n        trackDurationOfOperation(getDurationTrackerFactory(),\n            STORE_EXISTS_PROBE.getSymbol(),\n            () -> {\n              // Bug in SDK always returns `true` for AccessPoint ARNs with `doesBucketExistV2()`\n              // expanding implementation to use ARNs and buckets correctly\n              try {\n                s3.getBucketAcl(bucket);\n              } catch (AmazonServiceException ex) {\n                int statusCode = ex.getStatusCode();\n                if (statusCode == SC_404 ||\n                    (statusCode == SC_403 && ex.getMessage().contains(AP_INACCESSIBLE))) {\n                  return false;\n                }\n              }\n\n              return true;\n            }))) {\n      throw new UnknownStoreException(\"s3a://\" + bucket + \"/\", \" Bucket does \"\n          + \"not exist\");\n    }\n  }\n\n  /**\n   * Get S3A Instrumentation. For test purposes.\n   * @return this instance's instrumentation.\n   */\n  @VisibleForTesting\n  public S3AInstrumentation getInstrumentation() {\n    return instrumentation;\n  }\n\n  /**\n   * Get FS Statistic for this S3AFS instance.\n   *\n   * @return FS statistic instance.\n   */\n  @VisibleForTesting\n  public FileSystem.Statistics getFsStatistics() {\n    return statistics;\n  }\n\n  /**\n   * Get current listing instance.\n   * @return this instance's listing.\n   */\n  public Listing getListing() {\n    return listing;\n  }\n\n  /**\n   * Set up the client bindings.\n   * If delegation tokens are enabled, the FS first looks for a DT\n   * ahead of any other bindings;.\n   * If there is a DT it uses that to do the auth\n   * and switches to the DT authenticator automatically (and exclusively)\n   * @param name URI of the FS\n   * @param dtEnabled are delegation tokens enabled?\n   * @throws IOException failure.\n   */\n  @SuppressWarnings(\"deprecation\")\n  private void bindAWSClient(URI name, boolean dtEnabled) throws IOException {\n    Configuration conf = getConf();\n    credentials = null;\n    String uaSuffix = \"\";\n\n    if (dtEnabled) {\n      // Delegation support.\n      // Create and start the DT integration.\n      // Then look for an existing DT for this bucket, switch to authenticating\n      // with it if so.\n\n      LOG.debug(\"Using delegation tokens\");\n      V2Migration.v1DelegationTokenCredentialProvidersUsed();\n      S3ADelegationTokens tokens = new S3ADelegationTokens();\n      this.delegationTokens = Optional.of(tokens);\n      tokens.bindToFileSystem(getCanonicalUri(),\n          createStoreContext(),\n          createDelegationOperations());\n      tokens.init(conf);\n      tokens.start();\n      // switch to the DT provider and bypass all other configured\n      // providers.\n      if (tokens.isBoundToDT()) {\n        // A DT was retrieved.\n        LOG.debug(\"Using existing delegation token\");\n        // and use the encryption settings from that client, whatever they were\n      } else {\n        LOG.debug(\"No delegation token for this instance\");\n      }\n      // Get new credential chain\n      credentials = tokens.getCredentialProviders();\n      // and any encryption secrets which came from a DT\n      tokens.getEncryptionSecrets()\n          .ifPresent(this::setEncryptionSecrets);\n      // and update the UA field with any diagnostics provided by\n      // the DT binding.\n      uaSuffix = tokens.getUserAgentField();\n    } else {\n      // DT support is disabled, so create the normal credential chain\n      credentials = createAWSCredentialProviderSet(name, conf);\n    }\n    LOG.debug(\"Using credential provider {}\", credentials);\n    Class<? extends S3ClientFactory> s3ClientFactoryClass = conf.getClass(\n        S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n        S3ClientFactory.class);\n\n    String endpoint = accessPoint == null\n        ? conf.getTrimmed(ENDPOINT, DEFAULT_ENDPOINT)\n        : accessPoint.getEndpoint();\n\n    S3ClientFactory.S3ClientCreationParameters parameters = null;\n    parameters = new S3ClientFactory.S3ClientCreationParameters()\n        .withCredentialSet(credentials)\n        .withPathUri(name)\n        .withEndpoint(endpoint)\n        .withMetrics(statisticsContext.newStatisticsFromAwsSdk())\n        .withPathStyleAccess(conf.getBoolean(PATH_STYLE_ACCESS, false))\n        .withUserAgentSuffix(uaSuffix)\n        .withRequesterPays(conf.getBoolean(ALLOW_REQUESTER_PAYS, DEFAULT_ALLOW_REQUESTER_PAYS))\n        .withRequestHandlers(auditManager.createRequestHandlers());\n\n    s3 = ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n        .createS3Client(getUri(),\n            parameters);\n  }\n\n  /**\n   * Initialize and launch the audit manager and service.\n   * As this takes the FS IOStatistics store, it must be invoked\n   * after instrumentation is initialized.\n   * @throws IOException failure to instantiate/initialize.\n   */\n  protected void initializeAuditService() throws IOException {\n    auditManager = AuditIntegration.createAndStartAuditManager(\n        getConf(),\n        instrumentation.createMetricsUpdatingStore());\n  }\n\n  /**\n   * The audit manager.\n   * @return the audit manager\n   */\n  @InterfaceAudience.Private\n  public AuditManagerS3A getAuditManager() {\n    return auditManager;\n  }\n\n  /**\n   * Get the auditor; valid once initialized.\n   * @return the auditor.\n   */\n  @InterfaceAudience.Private\n  public OperationAuditor getAuditor() {\n    return getAuditManager().getAuditor();\n  }\n\n  /**\n   * Get the active audit span.\n   * @return the span.\n   */\n  @InterfaceAudience.Private\n  @Override\n  public AuditSpanS3A getActiveAuditSpan() {\n    return getAuditManager().getActiveAuditSpan();\n  }\n\n  /**\n   * Get the audit span source; allows for components like the committers\n   * to have a source of spans without being hard coded to the FS only.\n   * @return the source of spans -base implementation is this instance.\n   */\n  @InterfaceAudience.Private\n  public AuditSpanSource getAuditSpanSource() {\n    return this;\n  }\n\n  /**\n   * Start an operation; this informs the audit service of the event\n   * and then sets it as the active span.\n   * @param operation operation name.\n   * @param path1 first path of operation\n   * @param path2 second path of operation\n   * @return a span for the audit\n   * @throws IOException failure\n   */\n  public AuditSpanS3A createSpan(String operation,\n      @Nullable String path1,\n      @Nullable String path2)\n      throws IOException {\n\n    return getAuditManager().createSpan(operation, path1, path2);\n  }\n\n  /**\n   * Build the request factory.\n   * MUST be called after reading encryption secrets from settings/\n   * delegation token.\n   * Protected, in case test/mock classes want to implement their\n   * own variants.\n   * @return request factory.\n   */\n  protected RequestFactory createRequestFactory() {\n    long partCountLimit = longOption(getConf(),\n        UPLOAD_PART_COUNT_LIMIT,\n        DEFAULT_UPLOAD_PART_COUNT_LIMIT,\n        1);\n    if (partCountLimit != DEFAULT_UPLOAD_PART_COUNT_LIMIT) {\n      LOG.warn(\"Configuration property {} shouldn't be overridden by client\",\n          UPLOAD_PART_COUNT_LIMIT);\n    }\n\n    // ACLs; this is passed to the\n    // request factory.\n    initCannedAcls(getConf());\n\n    // Any encoding type\n    String contentEncoding = getConf().getTrimmed(CONTENT_ENCODING, null);\n\n    String storageClassConf = getConf()\n        .getTrimmed(STORAGE_CLASS, \"\")\n        .toUpperCase(Locale.US);\n    StorageClass storageClass = null;\n    if (!storageClassConf.isEmpty()) {\n      try {\n        storageClass = StorageClass.fromValue(storageClassConf);\n      } catch (IllegalArgumentException e) {\n        LOG.warn(\"Unknown storage class property {}: {}; falling back to default storage class\",\n            STORAGE_CLASS, storageClassConf);\n      }\n    } else {\n      LOG.debug(\"Unset storage class property {}; falling back to default storage class\",\n          STORAGE_CLASS);\n    }\n\n    return RequestFactoryImpl.builder()\n        .withBucket(requireNonNull(bucket))\n        .withCannedACL(getCannedACL())\n        .withEncryptionSecrets(requireNonNull(encryptionSecrets))\n        .withMultipartPartCountLimit(partCountLimit)\n        .withRequestPreparer(getAuditManager()::requestCreated)\n        .withContentEncoding(contentEncoding)\n        .withStorageClass(storageClass)\n        .withMultipartUploadEnabled(isMultipartUploadEnabled)\n        .build();\n  }\n\n  /**\n   * Get the request factory which uses this store's audit span.\n   * @return the request factory.\n   */\n  @VisibleForTesting\n  public RequestFactory getRequestFactory() {\n    return requestFactory;\n  }\n\n  /**\n   * Implementation of all operations used by delegation tokens.\n   */\n  private class DelegationOperationsImpl implements DelegationOperations {\n\n    @Override\n    public List<RoleModel.Statement> listAWSPolicyRules(final Set<AccessLevel> access) {\n      return S3AFileSystem.this.listAWSPolicyRules(access);\n    }\n  }\n\n  /**\n   * Create an instance of the delegation operations.\n   * @return callbacks for DT support.\n   */\n  @VisibleForTesting\n  public DelegationOperations createDelegationOperations() {\n    return new DelegationOperationsImpl();\n  }\n\n  /**\n   * Set the encryption secrets for requests.\n   * @param secrets secrets\n   */\n  protected void setEncryptionSecrets(final EncryptionSecrets secrets) {\n    this.encryptionSecrets = secrets;\n    if (requestFactory != null) {\n      requestFactory.setEncryptionSecrets(secrets);\n    }\n  }\n\n  /**\n   * Get the encryption secrets.\n   * This potentially sensitive information and must be treated with care.\n   * @return the current encryption secrets.\n   */\n  public EncryptionSecrets getEncryptionSecrets() {\n    return encryptionSecrets;\n  }\n\n  private void initTransferManager() {\n    TransferManagerConfiguration transferConfiguration =\n        new TransferManagerConfiguration();\n    transferConfiguration.setMinimumUploadPartSize(partSize);\n    transferConfiguration.setMultipartUploadThreshold(multiPartThreshold);\n    transferConfiguration.setMultipartCopyPartSize(partSize);\n    transferConfiguration.setMultipartCopyThreshold(multiPartThreshold);\n\n    transfers = new TransferManager(s3, unboundedThreadPool);\n    transfers.setConfiguration(transferConfiguration);\n  }\n\n  private void initCannedAcls(Configuration conf) {\n    String cannedACLName = conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n    if (!cannedACLName.isEmpty()) {\n      cannedACL = CannedAccessControlList.valueOf(cannedACLName);\n    } else {\n      cannedACL = null;\n    }\n  }\n\n  @Retries.RetryTranslated\n  private void initMultipartUploads(Configuration conf) throws IOException {\n    boolean purgeExistingMultipart = conf.getBoolean(PURGE_EXISTING_MULTIPART,\n        DEFAULT_PURGE_EXISTING_MULTIPART);\n    long purgeExistingMultipartAge = longOption(conf,\n        PURGE_EXISTING_MULTIPART_AGE, DEFAULT_PURGE_EXISTING_MULTIPART_AGE, 0);\n\n    if (purgeExistingMultipart) {\n      try {\n        abortOutstandingMultipartUploads(purgeExistingMultipartAge);\n      } catch (AccessDeniedException e) {\n        instrumentation.errorIgnored();\n        LOG.debug(\"Failed to purge multipart uploads against {},\" +\n            \" FS may be read only\", bucket);\n      }\n    }\n  }\n\n  /**\n   * Abort all outstanding MPUs older than a given age.\n   * @param seconds time in seconds\n   * @throws IOException on any failure, other than 403 \"permission denied\"\n   */\n  @Retries.RetryTranslated\n  public void abortOutstandingMultipartUploads(long seconds)\n      throws IOException {\n    Preconditions.checkArgument(seconds >= 0);\n    Date purgeBefore =\n        new Date(new Date().getTime() - seconds * 1000);\n    LOG.debug(\"Purging outstanding multipart uploads older than {}\",\n        purgeBefore);\n    invoker.retry(\"Purging multipart uploads\", bucket, true,\n        () -> transfers.abortMultipartUploads(bucket, purgeBefore));\n  }\n\n  /**\n   * Return the protocol scheme for the FileSystem.\n   *\n   * @return \"s3a\"\n   */\n  @Override\n  public String getScheme() {\n    return this.scheme;\n  }\n\n  /**\n   * Returns a URI whose scheme and authority identify this FileSystem.\n   */\n  @Override\n  public URI getUri() {\n    return uri;\n  }\n\n  /**\n   * Set the URI field through {@link S3xLoginHelper} and\n   * optionally {@link #canonicalizeUri(URI)}\n   * Exported for testing.\n   * @param fsUri filesystem URI.\n   * @param canonicalize true if the URI should be canonicalized.\n   */\n  @VisibleForTesting\n  protected void setUri(URI fsUri, boolean canonicalize) {\n    URI u = S3xLoginHelper.buildFSURI(fsUri);\n    this.uri = canonicalize ? u : canonicalizeUri(u);\n  }\n\n  /**\n   * Get the canonical URI.\n   * @return the canonical URI of this FS.\n   */\n  public URI getCanonicalUri() {\n    return uri;\n  }\n\n  @VisibleForTesting\n  @Override\n  public int getDefaultPort() {\n    return 0;\n  }\n\n  /**\n   * Returns the S3 client used by this filesystem.\n   * This is for internal use within the S3A code itself.\n   * @return AmazonS3Client\n   */\n  private AmazonS3 getAmazonS3Client() {\n    return s3;\n  }\n\n  /**\n   * Returns the S3 client used by this filesystem.\n   * <i>Warning: this must only be used for testing, as it bypasses core\n   * S3A operations. </i>\n   * @param reason a justification for requesting access.\n   * @return AmazonS3Client\n   */\n  @VisibleForTesting\n  public AmazonS3 getAmazonS3ClientForTesting(String reason) {\n    LOG.warn(\"Access to S3A client requested, reason {}\", reason);\n    V2Migration.v1S3ClientRequested();\n    return s3;\n  }\n\n  /**\n   * Set the client -used in mocking tests to force in a different client.\n   * @param client client.\n   */\n  protected void setAmazonS3Client(AmazonS3 client) {\n    Preconditions.checkNotNull(client, \"client\");\n    LOG.debug(\"Setting S3 client to {}\", client);\n    s3 = client;\n\n    // Need to use a new TransferManager that uses the new client.\n    // Also, using a new TransferManager requires a new threadpool as the old\n    // TransferManager will shut the thread pool down when it is garbage\n    // collected.\n    initThreadPools(getConf());\n    initTransferManager();\n  }\n\n  /**\n   * Get the region of a bucket.\n   * Invoked from StoreContext; consider an entry point.\n   * @return the region in which a bucket is located\n   * @throws AccessDeniedException if the caller lacks permission.\n   * @throws IOException on any failure.\n   */\n  @Retries.RetryTranslated\n  @InterfaceAudience.LimitedPrivate(\"diagnostics\")\n  public String getBucketLocation() throws IOException {\n    return getBucketLocation(bucket);\n  }\n\n  /**\n   * Get the region of a bucket; fixing up the region so it can be used\n   * in the builders of other AWS clients.\n   * Requires the caller to have the AWS role permission\n   * {@code s3:GetBucketLocation}.\n   * Retry policy: retrying, translated.\n   * @param bucketName the name of the bucket\n   * @return the region in which a bucket is located\n   * @throws AccessDeniedException if the caller lacks permission.\n   * @throws IOException on any failure.\n   */\n  @VisibleForTesting\n  @AuditEntryPoint\n  @Retries.RetryTranslated\n  public String getBucketLocation(String bucketName) throws IOException {\n    final String region = trackDurationAndSpan(\n        STORE_EXISTS_PROBE, bucketName, null, () ->\n            invoker.retry(\"getBucketLocation()\", bucketName, true, () ->\n                // If accessPoint then region is known from Arn\n                accessPoint != null\n                    ? accessPoint.getRegion()\n                    : s3.getBucketLocation(bucketName)));\n    return fixBucketRegion(region);\n  }\n\n  /**\n   * Get the input policy for this FS instance.\n   * @return the input policy\n   */\n  @InterfaceStability.Unstable\n  public S3AInputPolicy getInputPolicy() {\n    return inputPolicy;\n  }\n\n  /**\n   * Get the change detection policy for this FS instance.\n   * Only public to allow access in tests in other packages.\n   * @return the change detection policy\n   */\n  @VisibleForTesting\n  public ChangeDetectionPolicy getChangeDetectionPolicy() {\n    return changeDetectionPolicy;\n  }\n\n  /**\n   * Get the encryption algorithm of this endpoint.\n   * @return the encryption algorithm.\n   */\n  public S3AEncryptionMethods getS3EncryptionAlgorithm() {\n    return encryptionSecrets.getEncryptionMethod();\n  }\n\n  /**\n   * Demand create the directory allocator, then create a temporary file.\n   * This does not mark the file for deletion when a process exits.\n   * {@link LocalDirAllocator#createTmpFileForWrite(String, long, Configuration)}.\n   * @param pathStr prefix for the temporary file\n   * @param size the size of the file that is going to be written\n   * @param conf the Configuration object\n   * @return a unique temporary file\n   * @throws IOException IO problems\n   */\n  File createTmpFileForWrite(String pathStr, long size,\n      Configuration conf) throws IOException {\n    initLocalDirAllocatorIfNotInitialized(conf);\n    Path path = directoryAllocator.getLocalPathForWrite(pathStr,\n        size, conf);\n    File dir = new File(path.getParent().toUri().getPath());\n    String prefix = path.getName();\n    // create a temp file on this directory\n    return File.createTempFile(prefix, null, dir);\n  }\n\n  /**\n   * Initialize dir allocator if not already initialized.\n   *\n   * @param conf The Configuration object.\n   */\n  private void initLocalDirAllocatorIfNotInitialized(Configuration conf) {\n    if (directoryAllocator == null) {\n      synchronized (this) {\n        String bufferDir = conf.get(BUFFER_DIR) != null\n            ? BUFFER_DIR : HADOOP_TMP_DIR;\n        directoryAllocator = new LocalDirAllocator(bufferDir);\n      }\n    }\n  }\n\n  /**\n   * Get the bucket of this filesystem.\n   * @return the bucket\n   */\n  public String getBucket() {\n    return bucket;\n  }\n\n  /**\n   * Set the bucket.\n   * @param bucket the bucket\n   */\n  @VisibleForTesting\n  protected void setBucket(String bucket) {\n    this.bucket = bucket;\n  }\n\n  /**\n   * Get the canned ACL of this FS.\n   * @return an ACL, if any\n   */\n  CannedAccessControlList getCannedACL() {\n    return cannedACL;\n  }\n\n  /**\n   * Change the input policy for this FS.\n   * This is now a no-op, retained in case some application\n   * or external test invokes it.\n   *\n   * @deprecated use openFile() options\n   * @param inputPolicy new policy\n   */\n  @InterfaceStability.Unstable\n  @Deprecated\n  public void setInputPolicy(S3AInputPolicy inputPolicy) {\n    LOG.warn(\"setInputPolicy is no longer supported\");\n  }\n\n  /**\n   * Turns a path (relative or otherwise) into an S3 key.\n   *\n   * @param path input path, may be relative to the working dir\n   * @return a key excluding the leading \"/\", or, if it is the root path, \"\"\n   */\n  @VisibleForTesting\n  public String pathToKey(Path path) {\n    if (!path.isAbsolute()) {\n      path = new Path(workingDir, path);\n    }\n\n    if (path.toUri().getScheme() != null && path.toUri().getPath().isEmpty()) {\n      return \"\";\n    }\n\n    return path.toUri().getPath().substring(1);\n  }\n\n  /**\n   * Turns a path (relative or otherwise) into an S3 key, adding a trailing\n   * \"/\" if the path is not the root <i>and</i> does not already have a \"/\"\n   * at the end.\n   *\n   * @param key s3 key or \"\"\n   * @return the with a trailing \"/\", or, if it is the root key, \"\",\n   */\n  @InterfaceAudience.Private\n  public String maybeAddTrailingSlash(String key) {\n    return S3AUtils.maybeAddTrailingSlash(key);\n  }\n\n  /**\n   * Convert a path back to a key.\n   * @param key input key\n   * @return the path from this key\n   */\n  Path keyToPath(String key) {\n    return new Path(\"/\" + key);\n  }\n\n  /**\n   * Convert a key to a fully qualified path.\n   * This includes fixing up the URI so that if it ends with a trailing slash,\n   * that is corrected, similar to {@code Path.normalizePath()}.\n   * @param key input key\n   * @return the fully qualified path including URI scheme and bucket name.\n   */\n  public Path keyToQualifiedPath(String key) {\n    return qualify(keyToPath(key));\n  }\n\n  @Override\n  public Path makeQualified(final Path path) {\n    Path q = super.makeQualified(path);\n    if (!q.isRoot()) {\n      String urlString = q.toUri().toString();\n      if (urlString.endsWith(Path.SEPARATOR)) {\n        // this is a path which needs root stripping off to avoid\n        // confusion, See HADOOP-15430\n        LOG.debug(\"Stripping trailing '/' from {}\", q);\n        // deal with an empty \"/\" at the end by mapping to the parent and\n        // creating a new path from it\n        q = new Path(urlString.substring(0, urlString.length() - 1));\n      }\n    }\n    if (!q.isRoot() && q.getName().isEmpty()) {\n      q = q.getParent();\n    }\n    return q;\n  }\n\n  /**\n   * Qualify a path.\n   * This includes fixing up the URI so that if it ends with a trailing slash,\n   * that is corrected, similar to {@code Path.normalizePath()}.\n   * @param path path to qualify\n   * @return a qualified path.\n   */\n  public Path qualify(Path path) {\n    return makeQualified(path);\n  }\n\n  /**\n   * Check that a Path belongs to this FileSystem.\n   * Unlike the superclass, this version does not look at authority,\n   * only hostnames.\n   * @param path to check\n   * @throws IllegalArgumentException if there is an FS mismatch\n   */\n  @Override\n  public void checkPath(Path path) {\n    S3xLoginHelper.checkPath(getConf(), getUri(), path, getDefaultPort());\n  }\n\n  /**\n   * Override the base canonicalization logic and relay to\n   * {@link S3xLoginHelper#canonicalizeUri(URI, int)}.\n   * This allows for the option of changing this logic for better DT handling.\n   * @param rawUri raw URI.\n   * @return the canonical URI to use in delegation tokens and file context.\n   */\n  @Override\n  protected URI canonicalizeUri(URI rawUri) {\n    return S3xLoginHelper.canonicalizeUri(rawUri, getDefaultPort());\n  }\n\n  /**\n   * Opens an FSDataInputStream at the indicated Path.\n   * @param f the file name to open\n   * @param bufferSize the size of the buffer to be used.\n   */\n  @Retries.RetryTranslated\n  public FSDataInputStream open(Path f, int bufferSize)\n      throws IOException {\n    return executeOpen(qualify(f),\n        openFileHelper.openSimpleFile(bufferSize));\n  }\n\n  /**\n   * Opens an FSDataInputStream at the indicated Path.\n   * The {@code fileInformation} parameter controls how the file\n   * is opened, whether it is normal vs. an S3 select call,\n   * can a HEAD be skipped, etc.\n   * @param path the file to open\n   * @param fileInformation information about the file to open\n   * @throws IOException IO failure.\n   */\n  @AuditEntryPoint\n  @Retries.RetryTranslated\n  private FSDataInputStream executeOpen(\n      final Path path,\n      final OpenFileSupport.OpenFileInformation fileInformation)\n      throws IOException {\n    // create the input stream statistics before opening\n    // the file so that the time to prepare to open the file is included.\n    S3AInputStreamStatistics inputStreamStats =\n        statisticsContext.newInputStreamStatistics();\n    // this span is passed into the stream.\n    final AuditSpan auditSpan = entryPoint(INVOCATION_OPEN, path);\n    final S3AFileStatus fileStatus =\n        trackDuration(inputStreamStats,\n            ACTION_FILE_OPENED.getSymbol(), () ->\n            extractOrFetchSimpleFileStatus(path, fileInformation));\n    S3AReadOpContext readContext = createReadContext(\n        fileStatus,\n        auditSpan);\n    fileInformation.applyOptions(readContext);\n    LOG.debug(\"Opening '{}'\", readContext);\n\n    if (this.prefetchEnabled) {\n      Configuration configuration = getConf();\n      initLocalDirAllocatorIfNotInitialized(configuration);\n      return new FSDataInputStream(\n          new S3APrefetchingInputStream(\n              readContext.build(),\n              createObjectAttributes(path, fileStatus),\n              createInputStreamCallbacks(auditSpan),\n              inputStreamStats,\n              configuration,\n              directoryAllocator));\n    } else {\n      return new FSDataInputStream(\n          new S3AInputStream(\n              readContext.build(),\n              createObjectAttributes(path, fileStatus),\n              createInputStreamCallbacks(auditSpan),\n                  inputStreamStats,\n                  new SemaphoredDelegatingExecutor(\n                          boundedThreadPool,\n                          vectoredActiveRangeReads,\n                          true,\n                          inputStreamStats)));\n    }\n  }\n\n  /**\n   * Override point: create the callbacks for S3AInputStream.\n   * @return an implementation of the InputStreamCallbacks,\n   */\n  private S3AInputStream.InputStreamCallbacks createInputStreamCallbacks(\n      final AuditSpan auditSpan) {\n    return new InputStreamCallbacksImpl(auditSpan);\n  }\n\n  /**\n   * Operations needed by S3AInputStream to read data.\n   */\n  private final class InputStreamCallbacksImpl implements\n      S3AInputStream.InputStreamCallbacks {\n\n    /**\n     * Audit span to activate before each call.\n     */\n    private final AuditSpan auditSpan;\n\n    /**\n     * Create.\n     * @param auditSpan Audit span to activate before each call.\n     */\n    private InputStreamCallbacksImpl(final AuditSpan auditSpan) {\n      this.auditSpan = requireNonNull(auditSpan);\n    }\n\n    /**\n     * Closes the audit span.\n     */\n    @Override\n    public void close()  {\n      auditSpan.close();\n    }\n\n    @Override\n    public GetObjectRequest newGetRequest(final String key) {\n      // active the audit span used for the operation\n      try (AuditSpan span = auditSpan.activate()) {\n        return getRequestFactory().newGetObjectRequest(key);\n      }\n    }\n\n    @Override\n    public S3Object getObject(GetObjectRequest request) {\n      // active the audit span used for the operation\n      try (AuditSpan span = auditSpan.activate()) {\n        return s3.getObject(request);\n      }\n    }\n\n    @Override\n    public <T> CompletableFuture<T> submit(final CallableRaisingIOE<T> operation) {\n      CompletableFuture<T> result = new CompletableFuture<>();\n      unboundedThreadPool.submit(() ->\n          LambdaUtils.eval(result, () -> {\n            LOG.debug(\"Starting submitted operation in {}\", auditSpan.getSpanId());\n            try (AuditSpan span = auditSpan.activate()) {\n              return operation.apply();\n            } finally {\n              LOG.debug(\"Completed submitted operation in {}\", auditSpan.getSpanId());\n            }\n          }));\n      return result;\n    }\n  }\n\n  /**\n   * Callbacks for WriteOperationHelper.\n   */\n  private final class WriteOperationHelperCallbacksImpl\n      implements WriteOperationHelper.WriteOperationHelperCallbacks {\n\n    @Override\n    public SelectObjectContentResult selectObjectContent(SelectObjectContentRequest request) {\n      return s3.selectObjectContent(request);\n    }\n\n    @Override\n    public CompleteMultipartUploadResult completeMultipartUpload(\n        CompleteMultipartUploadRequest request) {\n      return s3.completeMultipartUpload(request);\n    }\n  }\n\n\n  /**\n   * Create the read context for reading from the referenced file,\n   * using FS state as well as the status.\n   * @param fileStatus file status.\n   * @param auditSpan audit span.\n   * @return a context for read and select operations.\n   */\n  @VisibleForTesting\n  protected S3AReadOpContext createReadContext(\n      final FileStatus fileStatus,\n      final AuditSpan auditSpan) {\n    final S3AReadOpContext roc = new S3AReadOpContext(\n        fileStatus.getPath(),\n        invoker,\n        statistics,\n        statisticsContext,\n        fileStatus,\n        vectoredIOContext,\n        IOStatisticsContext.getCurrentIOStatisticsContext().getAggregator(),\n        futurePool,\n        prefetchBlockSize,\n        prefetchBlockCount)\n        .withAuditSpan(auditSpan);\n    openFileHelper.applyDefaultOptions(roc);\n    return roc.build();\n  }\n\n  /**\n   * Create the attributes of an object for subsequent use.\n   * @param f path path of the request.\n   * @param eTag the eTag of the S3 object\n   * @param versionId S3 object version ID\n   * @param len length of the file\n   * @return attributes to use when building the query.\n   */\n  private S3ObjectAttributes createObjectAttributes(\n      final Path f,\n      final String eTag,\n      final String versionId,\n      final long len) {\n    return new S3ObjectAttributes(bucket,\n        f,\n        pathToKey(f),\n        getS3EncryptionAlgorithm(),\n        encryptionSecrets.getEncryptionKey(),\n        eTag,\n        versionId,\n        len);\n  }\n\n  /**\n   * Create the attributes of an object for subsequent use.\n   * @param path path -this is used over the file status path.\n   * @param fileStatus file status to build from.\n   * @return attributes to use when building the query.\n   */\n  private S3ObjectAttributes createObjectAttributes(\n      final Path path,\n      final S3AFileStatus fileStatus) {\n    return createObjectAttributes(\n        path,\n        fileStatus.getEtag(),\n        fileStatus.getVersionId(),\n        fileStatus.getLen());\n  }\n\n  /**\n   * Create an FSDataOutputStream at the indicated Path with write-progress\n   * reporting.\n   * Retry policy: retrying, translated on the getFileStatus() probe.\n   * No data is uploaded to S3 in this call, so retry issues related to that.\n   * @param f the file name to open\n   * @param permission the permission to set.\n   * @param overwrite if a file with this name already exists, then if true,\n   *   the file will be overwritten, and if false an error will be thrown.\n   * @param bufferSize the size of the buffer to be used.\n   * @param replication required block replication for the file.\n   * @param blockSize the requested block size.\n   * @param progress the progress reporter.\n   * @throws IOException in the event of IO related errors.\n   * @see #setPermission(Path, FsPermission)\n   */\n  @Override\n  @AuditEntryPoint\n  @SuppressWarnings(\"IOResourceOpenedButNotSafelyClosed\")\n  public FSDataOutputStream create(Path f, FsPermission permission,\n      boolean overwrite, int bufferSize, short replication, long blockSize,\n      Progressable progress) throws IOException {\n    final Path path = qualify(f);\n\n    // the span will be picked up inside the output stream\n    return trackDurationAndSpan(INVOCATION_CREATE, path, () ->\n        innerCreateFile(path,\n            progress,\n            getActiveAuditSpan(),\n            overwrite\n                ? OPTIONS_CREATE_FILE_OVERWRITE\n                : OPTIONS_CREATE_FILE_NO_OVERWRITE));\n  }\n\n  /**\n   * Create an FSDataOutputStream at the indicated Path with write-progress\n   * reporting; in the active span.\n   * Retry policy: retrying, translated on the getFileStatus() probe.\n   * No data is uploaded to S3 in this call, so no retry issues related to that.\n   * The \"performance\" flag disables safety checks for the path being a file,\n   * parent directory existing, and doesn't attempt to delete\n   * dir markers, irrespective of FS settings.\n   * If true, this method call does no IO at all.\n   * @param path the file name to open\n   * @param progress the progress reporter.\n   * @param auditSpan audit span\n   * @param options options for the file\n   * @throws IOException in the event of IO related errors.\n   */\n  @SuppressWarnings(\"IOResourceOpenedButNotSafelyClosed\")\n  @Retries.RetryTranslated\n  private FSDataOutputStream innerCreateFile(\n      final Path path,\n      final Progressable progress,\n      final AuditSpan auditSpan,\n      final CreateFileBuilder.CreateFileOptions options) throws IOException {\n    auditSpan.activate();\n    String key = pathToKey(path);\n    EnumSet<CreateFlag> flags = options.getFlags();\n    boolean overwrite = flags.contains(CreateFlag.OVERWRITE);\n    boolean performance = options.isPerformance();\n    boolean skipProbes = performance || isUnderMagicCommitPath(path);\n    if (skipProbes) {\n      LOG.debug(\"Skipping existence/overwrite checks\");\n    } else {\n      try {\n        // get the status or throw an FNFE.\n        // when overwriting, there is no need to look for any existing file,\n        // just a directory (for safety)\n        FileStatus status = innerGetFileStatus(path, false,\n            overwrite\n                ? StatusProbeEnum.DIRECTORIES\n                : StatusProbeEnum.ALL);\n\n        // if the thread reaches here, there is something at the path\n        if (status.isDirectory()) {\n          // path references a directory: automatic error\n          throw new FileAlreadyExistsException(path + \" is a directory\");\n        }\n        if (!overwrite) {\n          // path references a file and overwrite is disabled\n          throw new FileAlreadyExistsException(path + \" already exists\");\n        }\n        LOG.debug(\"Overwriting file {}\", path);\n      } catch (FileNotFoundException e) {\n        // this means there is nothing at the path; all good.\n      }\n    }\n    instrumentation.fileCreated();\n    final BlockOutputStreamStatistics outputStreamStatistics\n        = statisticsContext.newOutputStreamStatistics();\n    PutTracker putTracker =\n        committerIntegration.createTracker(path, key, outputStreamStatistics);\n    String destKey = putTracker.getDestKey();\n\n    // put options are derived from the path and the\n    // option builder.\n    boolean keep = performance || keepDirectoryMarkers(path);\n    final PutObjectOptions putOptions =\n        new PutObjectOptions(keep, null, options.getHeaders());\n\n    validateOutputStreamConfiguration(path, getConf());\n\n    final S3ABlockOutputStream.BlockOutputStreamBuilder builder =\n        S3ABlockOutputStream.builder()\n        .withKey(destKey)\n        .withBlockFactory(blockFactory)\n        .withBlockSize(partSize)\n        .withStatistics(outputStreamStatistics)\n        .withProgress(progress)\n        .withPutTracker(putTracker)\n        .withWriteOperations(\n            createWriteOperationHelper(auditSpan))\n        .withExecutorService(\n            new SemaphoredDelegatingExecutor(\n                boundedThreadPool,\n                blockOutputActiveBlocks,\n                true,\n                outputStreamStatistics))\n        .withDowngradeSyncableExceptions(\n            getConf().getBoolean(\n                DOWNGRADE_SYNCABLE_EXCEPTIONS,\n                DOWNGRADE_SYNCABLE_EXCEPTIONS_DEFAULT))\n        .withCSEEnabled(isCSEEnabled)\n        .withPutOptions(putOptions)\n        .withIOStatisticsAggregator(\n            IOStatisticsContext.getCurrentIOStatisticsContext().getAggregator())\n        .withMultipartEnabled(isMultipartUploadEnabled);\n    return new FSDataOutputStream(\n        new S3ABlockOutputStream(builder),\n        null);\n  }\n  /**\n   * Create a Write Operation Helper with the current active span.\n   * All operations made through this helper will activate the\n   * span before execution.\n   *\n   * This class permits other low-level operations against the store.\n   * It is unstable and\n   * only intended for code with intimate knowledge of the object store.\n   * If using this, be prepared for changes even on minor point releases.\n   * @return a new helper.\n   */\n  @InterfaceAudience.Private\n  public WriteOperationHelper getWriteOperationHelper() {\n    return createWriteOperationHelper(getActiveAuditSpan());\n  }\n\n  /**\n   * Create a Write Operation Helper with the given span.\n   * All operations made through this helper will activate the\n   * span before execution.\n   * @param auditSpan audit span\n   * @return a new helper.\n   */\n  @InterfaceAudience.Private\n  public WriteOperationHelper createWriteOperationHelper(AuditSpan auditSpan) {\n    return new WriteOperationHelper(this,\n        getConf(),\n        statisticsContext,\n        getAuditSpanSource(),\n        auditSpan,\n        new WriteOperationHelperCallbacksImpl());\n  }\n\n  /**\n   * Create instance of an FSDataOutputStreamBuilder for\n   * creating a file at the given path.\n   * @param path path to create\n   * @return a builder.\n   * @throws UncheckedIOException for problems creating the audit span\n   */\n  @Override\n  @AuditEntryPoint\n  public FSDataOutputStreamBuilder createFile(final Path path) {\n    try {\n      final Path qualified = qualify(path);\n      final AuditSpan span = entryPoint(INVOCATION_CREATE_FILE,\n          pathToKey(qualified),\n          null);\n      return new CreateFileBuilder(this,\n          qualified,\n          new CreateFileBuilderCallbacksImpl(INVOCATION_CREATE_FILE, span))\n            .create()\n            .overwrite(true);\n    } catch (IOException e) {\n      // catch any IOEs raised in span creation and convert to\n      // an UncheckedIOException\n      throw new UncheckedIOException(e);\n    }\n  }\n\n  /**\n   * Callback for create file operations.\n   */\n  private final class CreateFileBuilderCallbacksImpl implements\n      CreateFileBuilder.CreateFileBuilderCallbacks {\n\n    private final Statistic statistic;\n    /** span for operations. */\n    private final AuditSpan span;\n\n    private CreateFileBuilderCallbacksImpl(\n        final Statistic statistic,\n        final AuditSpan span) {\n      this.statistic = statistic;\n      this.span = span;\n    }\n\n    @Override\n    public FSDataOutputStream createFileFromBuilder(\n        final Path path,\n        final Progressable progress,\n        final CreateFileBuilder.CreateFileOptions options) throws IOException {\n      // the span will be picked up inside the output stream\n      return trackDuration(getDurationTrackerFactory(), statistic.getSymbol(), () ->\n          innerCreateFile(path, progress, span, options));\n    }\n  }\n\n  /**\n   * {@inheritDoc}\n   * The S3A implementations downgrades to the recursive creation, to avoid\n   * any race conditions with parent entries \"disappearing\".\n   */\n  @Override\n  @AuditEntryPoint\n  public FSDataOutputStream createNonRecursive(Path p,\n      FsPermission permission,\n      EnumSet<CreateFlag> flags,\n      int bufferSize,\n      short replication,\n      long blockSize,\n      Progressable progress) throws IOException {\n    final Path path = makeQualified(p);\n\n    // span is created and passed in to the callbacks.\n    final AuditSpan span = entryPoint(INVOCATION_CREATE_NON_RECURSIVE,\n        pathToKey(path),\n        null);\n    // uses the CreateFileBuilder, filling it in with the relevant arguments.\n    final CreateFileBuilder builder = new CreateFileBuilder(this,\n        path,\n        new CreateFileBuilderCallbacksImpl(INVOCATION_CREATE_NON_RECURSIVE, span))\n        .create()\n        .withFlags(flags)\n        .blockSize(blockSize)\n        .bufferSize(bufferSize);\n    if (progress != null) {\n      builder.progress(progress);\n    }\n    return builder.build();\n  }\n\n  /**\n   * Append to an existing file (optional operation).\n   * @param f the existing file to be appended.\n   * @param bufferSize the size of the buffer to be used.\n   * @param progress for reporting progress if it is not null.\n   * @throws IOException indicating that append is not supported.\n   */\n  public FSDataOutputStream append(Path f, int bufferSize,\n      Progressable progress) throws IOException {\n    throw new UnsupportedOperationException(\"Append is not supported \"\n        + \"by S3AFileSystem\");\n  }\n\n\n  /**\n   * Renames Path src to Path dst.  Can take place on local fs\n   * or remote DFS.\n   *\n   * Warning: S3 does not support renames. This method does a copy which can\n   * take S3 some time to execute with large files and directories. Since\n   * there is no Progressable passed in, this can time out jobs.\n   *\n   * Note: This implementation differs with other S3 drivers. Specifically:\n   * <pre>\n   *       Fails if src is a file and dst is a directory.\n   *       Fails if src is a directory and dst is a file.\n   *       Fails if the parent of dst does not exist or is a file.\n   *       Fails if dst is a directory that is not empty.\n   * </pre>\n   *\n   * @param src path to be renamed\n   * @param dst new path after rename\n   * @throws IOException on IO failure\n   * @return true if rename is successful\n   */\n  @AuditEntryPoint\n  @Retries.RetryTranslated\n  public boolean rename(Path src, Path dst) throws IOException {\n    try {\n      long bytesCopied = trackDurationAndSpan(\n          INVOCATION_RENAME, src.toString(), dst.toString(), () ->\n          innerRename(src, dst));\n      LOG.debug(\"Copied {} bytes\", bytesCopied);\n      return true;\n    } catch (AmazonClientException e) {\n      throw translateException(\"rename(\" + src +\", \" + dst + \")\", src, e);\n    } catch (RenameFailedException e) {\n      LOG.info(\"{}\", e.getMessage());\n      LOG.debug(\"rename failure\", e);\n      return e.getExitCode();\n    }\n  }\n\n  /**\n   * Validate the rename parameters and status of the filesystem;\n   * returns the source and any destination File Status.\n   * @param src qualified path to be renamed\n   * @param dst qualified path after rename\n   * @return the source and (possibly null) destination status entries.\n   * @throws RenameFailedException if some criteria for a state changing\n   * rename was not met. This means work didn't happen; it's not something\n   * which is reported upstream to the FileSystem APIs, for which the semantics\n   * of \"false\" are pretty vague.\n   * @throws FileNotFoundException there's no source file.\n   * @throws IOException on IO failure.\n   */\n  @Retries.RetryTranslated\n  private Pair<S3AFileStatus, S3AFileStatus> initiateRename(\n      final Path src,\n      final Path dst) throws IOException {\n    String srcKey = pathToKey(src);\n    String dstKey = pathToKey(dst);\n\n    if (srcKey.isEmpty()) {\n      throw new RenameFailedException(src, dst, \"source is root directory\");\n    }\n    if (dstKey.isEmpty()) {\n      throw new RenameFailedException(src, dst, \"dest is root directory\");\n    }\n\n    // get the source file status; this raises a FNFE if there is no source\n    // file.\n    S3AFileStatus srcStatus = innerGetFileStatus(src, true,\n        StatusProbeEnum.ALL);\n\n    if (srcKey.equals(dstKey)) {\n      LOG.debug(\"rename: src and dest refer to the same file or directory: {}\",\n          dst);\n      throw new RenameFailedException(src, dst,\n          \"source and dest refer to the same file or directory\")\n          .withExitCode(srcStatus.isFile());\n    }\n\n    S3AFileStatus dstStatus = null;\n    try {\n      dstStatus = innerGetFileStatus(dst, true, StatusProbeEnum.ALL);\n      // if there is no destination entry, an exception is raised.\n      // hence this code sequence can assume that there is something\n      // at the end of the path; the only detail being what it is and\n      // whether or not it can be the destination of the rename.\n      if (srcStatus.isDirectory()) {\n        if (dstStatus.isFile()) {\n          throw new FileAlreadyExistsException(\n              \"Failed to rename \" + src + \" to \" + dst\n               +\"; source is a directory and dest is a file\");\n        } else if (dstStatus.isEmptyDirectory() != Tristate.TRUE) {\n          throw new RenameFailedException(src, dst,\n              \"Destination is a non-empty directory\")\n              .withExitCode(false);\n        }\n        // at this point the destination is an empty directory\n      } else {\n        // source is a file. The destination must be a directory,\n        // empty or not\n        if (dstStatus.isFile()) {\n          throw new FileAlreadyExistsException(\n              \"Failed to rename \" + src + \" to \" + dst\n                  + \"; destination file exists\");\n        }\n      }\n\n    } catch (FileNotFoundException e) {\n      LOG.debug(\"rename: destination path {} not found\", dst);\n      // Parent must exist\n      Path parent = dst.getParent();\n      if (!pathToKey(parent).isEmpty()\n          && !parent.equals(src.getParent())) {\n        try {\n          // make sure parent isn't a file.\n          // don't look for parent being a dir as there is a risk\n          // of a race between dest dir cleanup and rename in different\n          // threads.\n          S3AFileStatus dstParentStatus = innerGetFileStatus(parent,\n              false, StatusProbeEnum.FILE);\n          // if this doesn't raise an exception then\n          // the parent is a file or a dir.\n          if (!dstParentStatus.isDirectory()) {\n            throw new RenameFailedException(src, dst,\n                \"destination parent is not a directory\");\n          }\n        } catch (FileNotFoundException expected) {\n          // nothing was found. Don't worry about it;\n          // expect rename to implicitly create the parent dir\n        }\n      }\n    }\n    return Pair.of(srcStatus, dstStatus);\n  }\n\n  /**\n   * The inner rename operation. See {@link #rename(Path, Path)} for\n   * the description of the operation.\n   * This operation throws an exception on any failure which needs to be\n   * reported and downgraded to a failure.\n   * Retries: retry translated, assuming all operations it is called do\n   * so. For safely, consider catch and handle AmazonClientException\n   * because this is such a complex method there's a risk it could surface.\n   * @param source path to be renamed\n   * @param dest new path after rename\n   * @throws RenameFailedException if some criteria for a state changing\n   * rename was not met. This means work didn't happen; it's not something\n   * which is reported upstream to the FileSystem APIs, for which the semantics\n   * of \"false\" are pretty vague.\n   * @return the number of bytes copied.\n   * @throws FileNotFoundException there's no source file.\n   * @throws IOException on IO failure.\n   * @throws AmazonClientException on failures inside the AWS SDK\n   */\n  @Retries.RetryMixed\n  private long innerRename(Path source, Path dest)\n      throws RenameFailedException, FileNotFoundException, IOException,\n        AmazonClientException {\n    Path src = qualify(source);\n    Path dst = qualify(dest);\n\n    LOG.debug(\"Rename path {} to {}\", src, dst);\n\n    String srcKey = pathToKey(src);\n    String dstKey = pathToKey(dst);\n\n    Pair<S3AFileStatus, S3AFileStatus> p = initiateRename(src, dst);\n\n    // Initiate the rename.\n    // this will call back into this class via the rename callbacks\n    RenameOperation renameOperation = new RenameOperation(\n        createStoreContext(),\n        src, srcKey, p.getLeft(),\n        dst, dstKey, p.getRight(),\n        new OperationCallbacksImpl(),\n        pageSize);\n    return renameOperation.execute();\n  }\n\n  @Override public Token<? extends TokenIdentifier> getFsDelegationToken()\n      throws IOException {\n    return getDelegationToken(null);\n  }\n\n  /**\n   * The callbacks made by the rename and delete operations.\n   * This separation allows the operation to be factored out and\n   * still avoid knowledge of the S3AFilesystem implementation.\n   * The Audit span active at the time of creation is cached and activated\n   * before every call.\n   */\n  private final class OperationCallbacksImpl implements OperationCallbacks {\n\n    /** Audit Span at time of creation. */\n    private final AuditSpan auditSpan;\n\n    private OperationCallbacksImpl() {\n      auditSpan = getActiveAuditSpan();\n    }\n\n    @Override\n    public S3ObjectAttributes createObjectAttributes(final Path path,\n        final String eTag,\n        final String versionId,\n        final long len) {\n      return S3AFileSystem.this.createObjectAttributes(path, eTag, versionId,\n          len);\n    }\n\n    @Override\n    public S3ObjectAttributes createObjectAttributes(\n        final S3AFileStatus fileStatus) {\n      return S3AFileSystem.this.createObjectAttributes(\n          fileStatus.getPath(),\n          fileStatus);\n    }\n\n    @Override\n    public S3AReadOpContext createReadContext(final FileStatus fileStatus) {\n      return S3AFileSystem.this.createReadContext(fileStatus,\n          auditSpan);\n    }\n\n    @Override\n    @Retries.RetryTranslated\n    public void deleteObjectAtPath(final Path path,\n        final String key,\n        final boolean isFile)\n        throws IOException {\n      auditSpan.activate();\n      once(\"delete\", path.toString(), () ->\n          S3AFileSystem.this.deleteObjectAtPath(path, key, isFile));\n    }\n\n    @Override\n    @Retries.RetryTranslated\n    public RemoteIterator<S3ALocatedFileStatus> listFilesAndDirectoryMarkers(\n        final Path path,\n        final S3AFileStatus status,\n        final boolean includeSelf) throws IOException {\n      auditSpan.activate();\n      return innerListFiles(\n          path,\n          true,\n          includeSelf\n              ? Listing.ACCEPT_ALL_BUT_S3N\n              : new Listing.AcceptAllButSelfAndS3nDirs(path),\n          status\n      );\n    }\n\n    @Override\n    public CopyResult copyFile(final String srcKey,\n        final String destKey,\n        final S3ObjectAttributes srcAttributes,\n        final S3AReadOpContext readContext) throws IOException {\n      auditSpan.activate();\n      return S3AFileSystem.this.copyFile(srcKey, destKey,\n          srcAttributes.getLen(), srcAttributes, readContext);\n    }\n\n    @Override\n    public void removeKeys(\n            final List<DeleteObjectsRequest.KeyVersion> keysToDelete,\n            final boolean deleteFakeDir)\n        throws MultiObjectDeleteException, AmazonClientException, IOException {\n      auditSpan.activate();\n      S3AFileSystem.this.removeKeys(keysToDelete, deleteFakeDir);\n    }\n\n    @Override\n    public void finishRename(final Path sourceRenamed, final Path destCreated)\n        throws IOException {\n      auditSpan.activate();\n      Path destParent = destCreated.getParent();\n      if (!sourceRenamed.getParent().equals(destParent)) {\n        LOG.debug(\"source & dest parents are different; fix up dir markers\");\n        if (!keepDirectoryMarkers(destParent)) {\n          deleteUnnecessaryFakeDirectories(destParent);\n        }\n        maybeCreateFakeParentDirectory(sourceRenamed);\n      }\n    }\n\n    @Override\n    @Retries.RetryTranslated\n    public RemoteIterator<S3AFileStatus> listObjects(\n        final Path path,\n        final String key)\n        throws IOException {\n      return once(\"listObjects\", key, () ->\n          listing.createFileStatusListingIterator(path,\n              createListObjectsRequest(key, null),\n              ACCEPT_ALL,\n              Listing.ACCEPT_ALL_BUT_S3N,\n              auditSpan));\n    }\n  }\n\n  /**\n   * Callbacks from {@link Listing}.\n   * Auditing: the listing object is long-lived; the audit span\n   * for a single listing is passed in from the listing\n   * method calls and then down to the callbacks.\n   */\n  protected class ListingOperationCallbacksImpl implements\n          ListingOperationCallbacks {\n\n    @Override\n    public CompletableFuture<S3ListResult> listObjectsAsync(\n        S3ListRequest request,\n        DurationTrackerFactory trackerFactory,\n        AuditSpan span) {\n      return submit(unboundedThreadPool, span, () ->\n          listObjects(request,\n              pairedTrackerFactory(trackerFactory,\n                  getDurationTrackerFactory())));\n    }\n\n    @Override\n    @Retries.RetryRaw\n    public CompletableFuture<S3ListResult> continueListObjectsAsync(\n        S3ListRequest request,\n        S3ListResult prevResult,\n        DurationTrackerFactory trackerFactory,\n        AuditSpan span) {\n      return submit(unboundedThreadPool, span,\n          () -> continueListObjects(request, prevResult,\n              pairedTrackerFactory(trackerFactory,\n                  getDurationTrackerFactory())));\n    }\n\n    @Override\n    public S3ALocatedFileStatus toLocatedFileStatus(\n            S3AFileStatus status)\n            throws IOException {\n      return S3AFileSystem.this.toLocatedFileStatus(status);\n    }\n\n    @Override\n    public S3ListRequest createListObjectsRequest(\n        String key,\n        String delimiter,\n        AuditSpan span) {\n      span.activate();\n      return S3AFileSystem.this.createListObjectsRequest(key, delimiter);\n    }\n\n    @Override\n    public long getDefaultBlockSize(Path path) {\n      return S3AFileSystem.this.getDefaultBlockSize(path);\n    }\n\n    @Override\n    public int getMaxKeys() {\n      return S3AFileSystem.this.getMaxKeys();\n    }\n\n  }\n\n  /**\n   * Low-level call to get at the object metadata.\n   * This method is used in some external applications and so\n   * must be viewed as a public entry point.\n   * Auditing: An audit entry point.\n   * @param path path to the object. This will be qualified.\n   * @return metadata\n   * @throws IOException IO and object access problems.\n   */\n  @VisibleForTesting\n  @AuditEntryPoint\n  @InterfaceAudience.LimitedPrivate(\"utilities\")\n  @Retries.RetryTranslated\n  @InterfaceStability.Evolving\n  public ObjectMetadata getObjectMetadata(Path path) throws IOException {\n    V2Migration.v1GetObjectMetadataCalled();\n    return trackDurationAndSpan(INVOCATION_GET_FILE_STATUS, path, () ->\n        getObjectMetadata(makeQualified(path), null, invoker,\n            \"getObjectMetadata\"));\n  }\n\n  /**\n   * Low-level call to get at the object metadata.\n   * @param path path to the object\n   * @param changeTracker the change tracker to detect version inconsistencies\n   * @param changeInvoker the invoker providing the retry policy\n   * @param operation the operation being performed (e.g. \"read\" or \"copy\")\n   * @return metadata\n   * @throws IOException IO and object access problems.\n   */\n  @Retries.RetryTranslated\n  private ObjectMetadata getObjectMetadata(Path path,\n      ChangeTracker changeTracker, Invoker changeInvoker, String operation)\n      throws IOException {\n    String key = pathToKey(path);\n    return once(operation, path.toString(), () ->\n            // HEAD against the object\n            getObjectMetadata(\n                key, changeTracker, changeInvoker, operation));\n  }\n\n  /**\n   * Entry point to an operation.\n   * Increments the statistic; verifies the FS is active.\n   * @param operation The operation being invoked\n   * @param path first path of operation\n   * @return a span for the audit\n   * @throws IOException failure of audit service\n   */\n  protected AuditSpan entryPoint(Statistic operation,\n      Path path) throws IOException {\n    return entryPoint(operation,\n        (path != null ? pathToKey(path): null),\n        null);\n  }\n\n  /**\n   * Entry point to an operation.\n   * Increments the statistic; verifies the FS is active.\n   * @param operation The operation being invoked\n   * @param path1 first path of operation\n   * @param path2 second path of operation\n   * @return a span for the audit\n   * @throws IOException failure of audit service\n   */\n  protected AuditSpan entryPoint(Statistic operation,\n      @Nullable String path1,\n      @Nullable String path2) throws IOException {\n    checkNotClosed();\n    incrementStatistic(operation);\n    return createSpan(operation.getSymbol(),\n        path1, path2);\n  }\n\n  /**\n   * Given an IOException raising callable/lambda expression,\n   * execute it and update the relevant statistic within a span\n   * of the same statistic.\n   * @param statistic statistic key\n   * @param path first path for span (nullable)\n   * @param path2 second path for span\n   * @param input input callable.\n   * @param <B> return type.\n   * @return the result of the operation.\n   * @throws IOException if raised in the callable\n   */\n  private <B> B trackDurationAndSpan(\n      Statistic statistic, String path, String path2,\n      CallableRaisingIOE<B> input) throws IOException {\n    checkNotClosed();\n    try (AuditSpan span = createSpan(statistic.getSymbol(),\n        path, path2)) {\n      return trackDuration(getDurationTrackerFactory(),\n          statistic.getSymbol(), input);\n    }\n  }\n\n  /**\n   * Overloaded version of {@code trackDurationAndSpan()}.\n   * Takes a single nullable path as the path param,\n   * @param statistic statistic key\n   * @param path path for span (nullable)\n   * @param input input callable.\n   * @param <B> return type.\n   * @return the result of the operation.\n   * @throws IOException if raised in the callable\n   */\n  private <B> B trackDurationAndSpan(\n      Statistic statistic,\n      @Nullable Path path,\n      CallableRaisingIOE<B> input) throws IOException {\n    return trackDurationAndSpan(statistic,\n        path != null ? pathToKey(path): null,\n        null, input);\n  }\n\n  /**\n   * Increment a statistic by 1.\n   * This increments both the instrumentation and storage statistics.\n   * @param statistic The operation to increment\n   */\n  protected void incrementStatistic(Statistic statistic) {\n    incrementStatistic(statistic, 1);\n  }\n\n  /**\n   * Increment a statistic by a specific value.\n   * This increments both the instrumentation and storage statistics.\n   * @param statistic The operation to increment\n   * @param count the count to increment\n   */\n  protected void incrementStatistic(Statistic statistic, long count) {\n    statisticsContext.incrementCounter(statistic, count);\n  }\n\n  /**\n   * Decrement a gauge by a specific value.\n   * @param statistic The operation to decrement\n   * @param count the count to decrement\n   */\n  protected void decrementGauge(Statistic statistic, long count) {\n    statisticsContext.decrementGauge(statistic, count);\n  }\n\n  /**\n   * Increment a gauge by a specific value.\n   * @param statistic The operation to increment\n   * @param count the count to increment\n   */\n  protected void incrementGauge(Statistic statistic, long count) {\n    statisticsContext.incrementGauge(statistic, count);\n  }\n\n  /**\n   * Callback when an operation was retried.\n   * Increments the statistics of ignored errors or throttled requests,\n   * depending up on the exception class.\n   * @param ex exception.\n   */\n  public void operationRetried(Exception ex) {\n    if (isThrottleException(ex)) {\n      LOG.debug(\"Request throttled\");\n      incrementStatistic(STORE_IO_THROTTLED);\n      statisticsContext.addValueToQuantiles(STORE_IO_THROTTLE_RATE, 1);\n    } else {\n      incrementStatistic(STORE_IO_RETRY);\n      incrementStatistic(IGNORED_ERRORS);\n    }\n  }\n\n  /**\n   * Callback from {@link Invoker} when an operation is retried.\n   * @param text text of the operation\n   * @param ex exception\n   * @param retries number of retries\n   * @param idempotent is the method idempotent\n   */\n  public void operationRetried(\n      String text,\n      Exception ex,\n      int retries,\n      boolean idempotent) {\n    operationRetried(ex);\n  }\n\n  /**\n   * Get the storage statistics of this filesystem.\n   * @return the storage statistics\n   */\n  @Override\n  public S3AStorageStatistics getStorageStatistics() {\n    return storageStatistics;\n  }\n\n  /**\n   * Get the instrumentation's IOStatistics.\n   * @return statistics\n   */\n  @Override\n  public IOStatistics getIOStatistics() {\n    return instrumentation != null\n        ? instrumentation.getIOStatistics()\n        : null;\n  }\n\n  /**\n   * Get the factory for duration tracking.\n   * @return a factory from the instrumentation.\n   */\n  protected DurationTrackerFactory getDurationTrackerFactory() {\n    return instrumentation != null ?\n        instrumentation.getDurationTrackerFactory()\n        : null;\n  }\n\n  /**\n   * Given a possibly null duration tracker factory, return a non-null\n   * one for use in tracking durations -either that or the FS tracker\n   * itself.\n   *\n   * @param factory factory.\n   * @return a non-null factory.\n   */\n  protected DurationTrackerFactory nonNullDurationTrackerFactory(\n      DurationTrackerFactory factory) {\n    return factory != null\n        ? factory\n        : getDurationTrackerFactory();\n  }\n\n  /**\n   * Request object metadata; increments counters in the process.\n   * Retry policy: retry untranslated.\n   * This method is used in some external applications and so\n   * must be viewed as a public entry point.\n   * Auditing: this call does NOT initiate a new AuditSpan; the expectation\n   * is that there is already an active span.\n   * @param key key\n   * @return the metadata\n   * @throws IOException if the retry invocation raises one (it shouldn't).\n   */\n  @Retries.RetryRaw\n  @VisibleForTesting\n  @InterfaceAudience.LimitedPrivate(\"external utilities\")\n  ObjectMetadata getObjectMetadata(String key) throws IOException {\n    return getObjectMetadata(key, null, invoker, \"getObjectMetadata\");\n  }\n\n  /**\n   * Request object metadata; increments counters in the process.\n   * Retry policy: retry untranslated.\n   * Uses changeTracker to detect an unexpected file version (eTag or versionId)\n   * @param key key\n   * @param changeTracker the change tracker to detect unexpected object version\n   * @param changeInvoker the invoker providing the retry policy\n   * @param operation the operation (e.g. \"read\" or \"copy\") triggering this call\n   * @return the metadata\n   * @throws IOException if the retry invocation raises one (it shouldn't).\n   * @throws RemoteFileChangedException if an unexpected version is detected\n   */\n  @Retries.RetryRaw\n  protected ObjectMetadata getObjectMetadata(String key,\n      ChangeTracker changeTracker,\n      Invoker changeInvoker,\n      String operation) throws IOException {\n    ObjectMetadata meta = changeInvoker.retryUntranslated(\"GET \" + key, true,\n        () -> {\n          GetObjectMetadataRequest request\n              = getRequestFactory().newGetObjectMetadataRequest(key);\n          incrementStatistic(OBJECT_METADATA_REQUESTS);\n          DurationTracker duration = getDurationTrackerFactory()\n              .trackDuration(ACTION_HTTP_HEAD_REQUEST.getSymbol());\n          try {\n            LOG.debug(\"HEAD {} with change tracker {}\", key, changeTracker);\n            if (changeTracker != null) {\n              changeTracker.maybeApplyConstraint(request);\n            }\n            ObjectMetadata objectMetadata = s3.getObjectMetadata(request);\n            if (changeTracker != null) {\n              changeTracker.processMetadata(objectMetadata, operation);\n            }\n            return objectMetadata;\n          } catch(AmazonServiceException ase) {\n            if (!isObjectNotFound(ase)) {\n              // file not found is not considered a failure of the call,\n              // so only switch the duration tracker to update failure\n              // metrics on other exception outcomes.\n              duration.failed();\n            }\n            throw ase;\n          } finally {\n            // update the tracker.\n            duration.close();\n          }\n        });\n    incrementReadOperations();\n    return meta;\n  }\n\n  /**\n   * Initiate a {@code listObjects} operation, incrementing metrics\n   * in the process.\n   *\n   * Retry policy: retry untranslated.\n   * @param request request to initiate\n   * @param trackerFactory duration tracking\n   * @return the results\n   * @throws IOException if the retry invocation raises one (it shouldn't).\n   */\n  @Retries.RetryRaw\n  protected S3ListResult listObjects(S3ListRequest request,\n      @Nullable final DurationTrackerFactory trackerFactory)\n      throws IOException {\n    incrementReadOperations();\n    LOG.debug(\"LIST {}\", request);\n    validateListArguments(request);\n    try(DurationInfo ignored =\n            new DurationInfo(LOG, false, \"LIST\")) {\n      return invoker.retryUntranslated(\n          request.toString(),\n          true,\n          trackDurationOfOperation(trackerFactory,\n              OBJECT_LIST_REQUEST,\n              () -> {\n                if (useListV1) {\n                  return S3ListResult.v1(s3.listObjects(request.getV1()));\n                } else {\n                  return S3ListResult.v2(s3.listObjectsV2(request.getV2()));\n                }\n              }));\n    }\n  }\n\n  /**\n   * Validate the list arguments with this bucket's settings.\n   * @param request the request to validate\n   */\n  private void validateListArguments(S3ListRequest request) {\n    if (useListV1) {\n      Preconditions.checkArgument(request.isV1());\n    } else {\n      Preconditions.checkArgument(!request.isV1());\n    }\n  }\n\n  /**\n   * List the next set of objects.\n   * Retry policy: retry untranslated.\n   * @param request last list objects request to continue\n   * @param prevResult last paged result to continue from\n   * @param trackerFactory duration tracking\n   * @return the next result object\n   * @throws IOException none, just there for retryUntranslated.\n   */\n  @Retries.RetryRaw\n  protected S3ListResult continueListObjects(S3ListRequest request,\n      S3ListResult prevResult,\n      final DurationTrackerFactory trackerFactory) throws IOException {\n    incrementReadOperations();\n    validateListArguments(request);\n    try(DurationInfo ignored =\n            new DurationInfo(LOG, false, \"LIST (continued)\")) {\n      return invoker.retryUntranslated(\n          request.toString(),\n          true,\n          trackDurationOfOperation(\n              trackerFactory,\n              OBJECT_CONTINUE_LIST_REQUEST,\n              () -> {\n                if (useListV1) {\n                  return S3ListResult.v1(\n                      s3.listNextBatchOfObjects(\n                          getRequestFactory()\n                              .newListNextBatchOfObjectsRequest(\n                                  prevResult.getV1())));\n                } else {\n                  request.getV2().setContinuationToken(prevResult.getV2()\n                      .getNextContinuationToken());\n                  return S3ListResult.v2(s3.listObjectsV2(request.getV2()));\n                }\n              }));\n    }\n  }\n\n  /**\n   * Increment read operations.\n   */\n  public void incrementReadOperations() {\n    statistics.incrementReadOps(1);\n  }\n\n  /**\n   * Increment the write operation counter.\n   * This is somewhat inaccurate, as it appears to be invoked more\n   * often than needed in progress callbacks.\n   */\n  public void incrementWriteOperations() {\n    statistics.incrementWriteOps(1);\n  }\n\n  /**\n   * Delete an object.\n   * Increments the {@code OBJECT_DELETE_REQUESTS} and write\n   * operation statistics.\n   * This call does <i>not</i> create any mock parent entries.\n   *\n   * Retry policy: retry untranslated; delete considered idempotent.\n   * @param key key to blob to delete.\n   * @throws AmazonClientException problems working with S3\n   * @throws InvalidRequestException if the request was rejected due to\n   * a mistaken attempt to delete the root directory.\n   */\n  @VisibleForTesting\n  @Retries.RetryRaw\n  protected void deleteObject(String key)\n      throws AmazonClientException, IOException {\n    blockRootDelete(key);\n    incrementWriteOperations();\n    try (DurationInfo ignored =\n             new DurationInfo(LOG, false,\n                 \"deleting %s\", key)) {\n      invoker.retryUntranslated(String.format(\"Delete %s:/%s\", bucket, key),\n          DELETE_CONSIDERED_IDEMPOTENT,\n          ()-> {\n            incrementStatistic(OBJECT_DELETE_OBJECTS);\n            trackDurationOfInvocation(getDurationTrackerFactory(),\n                OBJECT_DELETE_REQUEST.getSymbol(),\n                () -> s3.deleteObject(getRequestFactory()\n                    .newDeleteObjectRequest(key)));\n            return null;\n          });\n    }\n  }\n\n  /**\n   * Delete an object.\n   * This call does <i>not</i> create any mock parent entries.\n   * Retry policy: retry untranslated; delete considered idempotent.\n   * @param f path path to delete\n   * @param key key of entry\n   * @param isFile is the path a file (used for instrumentation only)\n   * @throws AmazonClientException problems working with S3\n   * @throws IOException from invoker signature only -should not be raised.\n   */\n  @Retries.RetryRaw\n  void deleteObjectAtPath(Path f,\n      String key,\n      boolean isFile)\n      throws AmazonClientException, IOException {\n    if (isFile) {\n      instrumentation.fileDeleted(1);\n    } else {\n      instrumentation.directoryDeleted();\n    }\n    deleteObject(key);\n  }\n\n  /**\n   * Reject any request to delete an object where the key is root.\n   * @param key key to validate\n   * @throws InvalidRequestException if the request was rejected due to\n   * a mistaken attempt to delete the root directory.\n   */\n  private void blockRootDelete(String key) throws InvalidRequestException {\n    if (key.isEmpty() || \"/\".equals(key)) {\n      throw new InvalidRequestException(\"Bucket \"+ bucket\n          +\" cannot be deleted\");\n    }\n  }\n\n  /**\n   * Perform a bulk object delete operation against S3.\n   * Increments the {@code OBJECT_DELETE_REQUESTS} and write\n   * operation statistics\n   * <p></p>\n   * {@code OBJECT_DELETE_OBJECTS} is updated with the actual number\n   * of objects deleted in the request.\n   * <p></p>\n   * Retry policy: retry untranslated; delete considered idempotent.\n   * If the request is throttled, this is logged in the throttle statistics,\n   * with the counter set to the number of keys, rather than the number\n   * of invocations of the delete operation.\n   * This is because S3 considers each key as one mutating operation on\n   * the store when updating its load counters on a specific partition\n   * of an S3 bucket.\n   * If only the request was measured, this operation would under-report.\n   * @param deleteRequest keys to delete on the s3-backend\n   * @return the AWS response\n   * @throws MultiObjectDeleteException one or more of the keys could not\n   * be deleted.\n   * @throws AmazonClientException amazon-layer failure.\n   */\n  @Retries.RetryRaw\n  private DeleteObjectsResult deleteObjects(DeleteObjectsRequest deleteRequest)\n      throws MultiObjectDeleteException, AmazonClientException, IOException {\n    incrementWriteOperations();\n    BulkDeleteRetryHandler retryHandler =\n        new BulkDeleteRetryHandler(createStoreContext());\n    int keyCount = deleteRequest.getKeys().size();\n    try(DurationInfo ignored =\n            new DurationInfo(LOG, false, \"DELETE %d keys\",\n                keyCount)) {\n      return invoker.retryUntranslated(\"delete\",\n          DELETE_CONSIDERED_IDEMPOTENT,\n          (text, e, r, i) -> {\n            // handle the failure\n            retryHandler.bulkDeleteRetried(deleteRequest, e);\n          },\n          // duration is tracked in the bulk delete counters\n          trackDurationOfOperation(getDurationTrackerFactory(),\n              OBJECT_BULK_DELETE_REQUEST.getSymbol(), () -> {\n                incrementStatistic(OBJECT_DELETE_OBJECTS, keyCount);\n                return s3.deleteObjects(deleteRequest);\n            }));\n    } catch (MultiObjectDeleteException e) {\n      // one or more of the keys could not be deleted.\n      // log and rethrow\n      List<MultiObjectDeleteException.DeleteError> errors = e.getErrors();\n      LOG.debug(\"Partial failure of delete, {} errors\", errors.size(), e);\n      for (MultiObjectDeleteException.DeleteError error : errors) {\n        LOG.debug(\"{}: \\\"{}\\\" - {}\",\n            error.getKey(), error.getCode(), error.getMessage());\n      }\n      throw e;\n    }\n  }\n\n  /**\n   * Create a putObject request.\n   * Adds the ACL and metadata\n   * @param key key of object\n   * @param metadata metadata header\n   * @param srcfile source file\n   * @return the request\n   */\n  public PutObjectRequest newPutObjectRequest(String key,\n      ObjectMetadata metadata, File srcfile) {\n    return requestFactory.newPutObjectRequest(key, metadata, null, srcfile);\n  }\n\n  /**\n   * Create a new object metadata instance.\n   * Any standard metadata headers are added here, for example:\n   * encryption.\n   *\n   * @param length length of data to set in header.\n   * @return a new metadata instance\n   */\n  public ObjectMetadata newObjectMetadata(long length) {\n    return requestFactory.newObjectMetadata(length);\n  }\n\n  /**\n   * Start a transfer-manager managed async PUT of an object,\n   * incrementing the put requests and put bytes\n   * counters.\n   * It does not update the other counters,\n   * as existing code does that as progress callbacks come in.\n   * Byte length is calculated from the file length, or, if there is no\n   * file, from the content length of the header.\n   * Because the operation is async, any stream supplied in the request\n   * must reference data (files, buffers) which stay valid until the upload\n   * completes.\n   * Retry policy: N/A: the transfer manager is performing the upload.\n   * Auditing: must be inside an audit span.\n   * @param putObjectRequest the request\n   * @return the upload initiated\n   */\n  @Retries.OnceRaw\n  public UploadInfo putObject(PutObjectRequest putObjectRequest) {\n    long len = getPutRequestLength(putObjectRequest);\n    LOG.debug(\"PUT {} bytes to {} via transfer manager \",\n        len, putObjectRequest.getKey());\n    incrementPutStartStatistics(len);\n    Upload upload = transfers.upload(putObjectRequest);\n    return new UploadInfo(upload, len);\n  }\n\n  /**\n   * PUT an object directly (i.e. not via the transfer manager).\n   * Byte length is calculated from the file length, or, if there is no\n   * file, from the content length of the header.\n   *\n   * Retry Policy: none.\n   * Auditing: must be inside an audit span.\n   * <i>Important: this call will close any input stream in the request.</i>\n   * @param putObjectRequest the request\n   * @param putOptions put object options\n   * @param durationTrackerFactory factory for duration tracking\n   * @return the upload initiated\n   * @throws AmazonClientException on problems\n   */\n  @VisibleForTesting\n  @Retries.OnceRaw(\"For PUT; post-PUT actions are RetryExceptionsSwallowed\")\n  PutObjectResult putObjectDirect(PutObjectRequest putObjectRequest,\n      PutObjectOptions putOptions,\n      DurationTrackerFactory durationTrackerFactory)\n      throws AmazonClientException {\n    long len = getPutRequestLength(putObjectRequest);\n    LOG.debug(\"PUT {} bytes to {}\", len, putObjectRequest.getKey());\n    incrementPutStartStatistics(len);\n    try {\n      PutObjectResult result = trackDurationOfSupplier(\n          nonNullDurationTrackerFactory(durationTrackerFactory),\n          OBJECT_PUT_REQUESTS.getSymbol(), () ->\n              s3.putObject(putObjectRequest));\n      incrementPutCompletedStatistics(true, len);\n      // apply any post-write actions.\n      finishedWrite(putObjectRequest.getKey(), len,\n          result.getETag(), result.getVersionId(),\n          putOptions);\n      return result;\n    } catch (SdkBaseException e) {\n      incrementPutCompletedStatistics(false, len);\n      throw e;\n    }\n  }\n\n  /**\n   * Get the length of the PUT, verifying that the length is known.\n   * @param putObjectRequest a request bound to a file or a stream.\n   * @return the request length\n   * @throws IllegalArgumentException if the length is negative\n   */\n  private long getPutRequestLength(PutObjectRequest putObjectRequest) {\n    long len;\n    if (putObjectRequest.getFile() != null) {\n      len = putObjectRequest.getFile().length();\n    } else {\n      len = putObjectRequest.getMetadata().getContentLength();\n    }\n    Preconditions.checkState(len >= 0, \"Cannot PUT object of unknown length\");\n    return len;\n  }\n\n  /**\n   * Upload part of a multi-partition file.\n   * Increments the write and put counters.\n   * <i>Important: this call does not close any input stream in the request.</i>\n   *\n   * Retry Policy: none.\n   * @param request request\n   * @param durationTrackerFactory duration tracker factory for operation\n   * @return the result of the operation.\n   * @throws AmazonClientException on problems\n   */\n  @Retries.OnceRaw\n  UploadPartResult uploadPart(UploadPartRequest request,\n      final DurationTrackerFactory durationTrackerFactory)\n      throws AmazonClientException {\n    long len = request.getPartSize();\n    incrementPutStartStatistics(len);\n    try {\n      UploadPartResult uploadPartResult = trackDurationOfSupplier(\n          nonNullDurationTrackerFactory(durationTrackerFactory),\n          MULTIPART_UPLOAD_PART_PUT.getSymbol(), () ->\n              s3.uploadPart(request));\n      incrementPutCompletedStatistics(true, len);\n      return uploadPartResult;\n    } catch (AmazonClientException e) {\n      incrementPutCompletedStatistics(false, len);\n      throw e;\n    }\n  }\n\n  /**\n   * At the start of a put/multipart upload operation, update the\n   * relevant counters.\n   *\n   * @param bytes bytes in the request.\n   */\n  public void incrementPutStartStatistics(long bytes) {\n    LOG.debug(\"PUT start {} bytes\", bytes);\n    incrementWriteOperations();\n    incrementGauge(OBJECT_PUT_REQUESTS_ACTIVE, 1);\n    if (bytes > 0) {\n      incrementGauge(OBJECT_PUT_BYTES_PENDING, bytes);\n    }\n  }\n\n  /**\n   * At the end of a put/multipart upload operation, update the\n   * relevant counters and gauges.\n   *\n   * @param success did the operation succeed?\n   * @param bytes bytes in the request.\n   */\n  public void incrementPutCompletedStatistics(boolean success, long bytes) {\n    LOG.debug(\"PUT completed success={}; {} bytes\", success, bytes);\n    if (bytes > 0) {\n      incrementStatistic(OBJECT_PUT_BYTES, bytes);\n      decrementGauge(OBJECT_PUT_BYTES_PENDING, bytes);\n    }\n    incrementStatistic(OBJECT_PUT_REQUESTS_COMPLETED);\n    decrementGauge(OBJECT_PUT_REQUESTS_ACTIVE, 1);\n  }\n\n  /**\n   * Callback for use in progress callbacks from put/multipart upload events.\n   * Increments those statistics which are expected to be updated during\n   * the ongoing upload operation.\n   * @param key key to file that is being written (for logging)\n   * @param bytes bytes successfully uploaded.\n   */\n  public void incrementPutProgressStatistics(String key, long bytes) {\n    PROGRESS.debug(\"PUT {}: {} bytes\", key, bytes);\n    incrementWriteOperations();\n    if (bytes > 0) {\n      statistics.incrementBytesWritten(bytes);\n    }\n  }\n\n  /**\n   * Delete a list of keys on a s3-backend.\n   * Retry policy: retry untranslated; delete considered idempotent.\n   * @param keysToDelete collection of keys to delete on the s3-backend.\n   *        if empty, no request is made of the object store.\n   * @param deleteFakeDir indicates whether this is for deleting fake dirs\n   * @throws InvalidRequestException if the request was rejected due to\n   * a mistaken attempt to delete the root directory.\n   * @throws MultiObjectDeleteException one or more of the keys could not\n   * be deleted in a multiple object delete operation.\n   * The number of rejected objects will be added to the metric\n   * {@link Statistic#FILES_DELETE_REJECTED}.\n   * @throws AmazonClientException other amazon-layer failure.\n   */\n  @Retries.RetryRaw\n  private void removeKeysS3(\n          List<DeleteObjectsRequest.KeyVersion> keysToDelete,\n          boolean deleteFakeDir)\n      throws MultiObjectDeleteException, AmazonClientException,\n      IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Initiating delete operation for {} objects\",\n          keysToDelete.size());\n      for (DeleteObjectsRequest.KeyVersion key : keysToDelete) {\n        LOG.debug(\" {} {}\", key.getKey(),\n            key.getVersion() != null ? key.getVersion() : \"\");\n      }\n    }\n    if (keysToDelete.isEmpty()) {\n      // exit fast if there are no keys to delete\n      return;\n    }\n    for (DeleteObjectsRequest.KeyVersion keyVersion : keysToDelete) {\n      blockRootDelete(keyVersion.getKey());\n    }\n    try {\n      if (enableMultiObjectsDelete) {\n        if (keysToDelete.size() <= pageSize) {\n          deleteObjects(getRequestFactory()\n                  .newBulkDeleteRequest(keysToDelete));\n        } else {\n          // Multi object deletion of more than 1000 keys is not supported\n          // by s3. So we are paging the keys by page size.\n          LOG.debug(\"Partitioning the keys to delete as it is more than \" +\n                  \"page size. Number of keys: {}, Page size: {}\",\n                  keysToDelete.size(), pageSize);\n          for (List<DeleteObjectsRequest.KeyVersion> batchOfKeysToDelete :\n                  Lists.partition(keysToDelete, pageSize)) {\n            deleteObjects(getRequestFactory()\n                    .newBulkDeleteRequest(batchOfKeysToDelete));\n          }\n        }\n      } else {\n        for (DeleteObjectsRequest.KeyVersion keyVersion : keysToDelete) {\n          deleteObject(keyVersion.getKey());\n        }\n      }\n    } catch (MultiObjectDeleteException ex) {\n      // partial delete.\n      // Update the stats with the count of the actual number of successful\n      // deletions.\n      int rejected = ex.getErrors().size();\n      noteDeleted(keysToDelete.size() - rejected, deleteFakeDir);\n      incrementStatistic(FILES_DELETE_REJECTED, rejected);\n      throw ex;\n    }\n    noteDeleted(keysToDelete.size(), deleteFakeDir);\n  }\n\n  /**\n   * Note the deletion of files or fake directories deleted.\n   * @param count count of keys deleted.\n   * @param deleteFakeDir are the deletions fake directories?\n   */\n  private void noteDeleted(final int count, final boolean deleteFakeDir) {\n    if (!deleteFakeDir) {\n      instrumentation.fileDeleted(count);\n    } else {\n      instrumentation.fakeDirsDeleted(count);\n    }\n  }\n\n  /**\n   * Invoke {@link #removeKeysS3(List, boolean)}.\n   * If a {@code MultiObjectDeleteException} is raised, the\n   * relevant statistics are updated.\n   *\n   * @param keysToDelete collection of keys to delete on the s3-backend.\n   *        if empty, no request is made of the object store.\n   * @param deleteFakeDir indicates whether this is for deleting fake dirs\n   * @throws InvalidRequestException if the request was rejected due to\n   * a mistaken attempt to delete the root directory.\n   * @throws MultiObjectDeleteException one or more of the keys could not\n   * be deleted in a multiple object delete operation.\n   * @throws AmazonClientException amazon-layer failure.\n   * @throws IOException other IO Exception.\n   */\n  @VisibleForTesting\n  @Retries.RetryRaw\n  public void removeKeys(\n      final List<DeleteObjectsRequest.KeyVersion> keysToDelete,\n      final boolean deleteFakeDir)\n      throws MultiObjectDeleteException, AmazonClientException,\n      IOException {\n    try (DurationInfo ignored = new DurationInfo(LOG, false,\n            \"Deleting %d keys\", keysToDelete.size())) {\n      removeKeysS3(keysToDelete, deleteFakeDir);\n    }\n  }\n\n  /**\n   * Delete a Path. This operation is at least {@code O(files)}, with\n   * added overheads to enumerate the path. It is also not atomic.\n   *\n   * @param f the path to delete.\n   * @param recursive if path is a directory and set to\n   * true, the directory is deleted else throws an exception. In\n   * case of a file the recursive can be set to either true or false.\n   * @return true if the path existed and then was deleted; false if there\n   * was no path in the first place, or the corner cases of root path deletion\n   * have surfaced.\n   * @throws IOException due to inability to delete a directory or file.\n   */\n  @Override\n  @Retries.RetryTranslated\n  @AuditEntryPoint\n  public boolean delete(Path f, boolean recursive) throws IOException {\n    checkNotClosed();\n    return deleteWithoutCloseCheck(f, recursive);\n  }\n\n  /**\n   * Same as delete(), except that it does not check if fs is closed.\n   *\n   * @param f the path to delete.\n   * @param recursive if path is a directory and set to\n   * true, the directory is deleted else throws an exception. In\n   * case of a file the recursive can be set to either true or false.\n   * @return true if the path existed and then was deleted; false if there\n   * was no path in the first place, or the corner cases of root path deletion\n   * have surfaced.\n   * @throws IOException due to inability to delete a directory or file.\n   */\n\n  @VisibleForTesting\n  protected boolean deleteWithoutCloseCheck(Path f, boolean recursive) throws IOException {\n    final Path path = qualify(f);\n    // span covers delete, getFileStatus, fake directory operations.\n    try (AuditSpan span = createSpan(INVOCATION_DELETE.getSymbol(),\n        path.toString(), null)) {\n      boolean outcome = trackDuration(getDurationTrackerFactory(),\n          INVOCATION_DELETE.getSymbol(),\n          new DeleteOperation(\n              createStoreContext(),\n              innerGetFileStatus(path, true, StatusProbeEnum.ALL),\n              recursive,\n              new OperationCallbacksImpl(),\n              pageSize));\n      if (outcome) {\n        try {\n          maybeCreateFakeParentDirectory(path);\n        } catch (AccessDeniedException e) {\n          LOG.warn(\"Cannot create directory marker at {}: {}\",\n              f.getParent(), e.toString());\n          LOG.debug(\"Failed to create fake dir above {}\", path, e);\n        }\n      }\n      return outcome;\n    } catch (FileNotFoundException e) {\n      LOG.debug(\"Couldn't delete {} - does not exist: {}\", path, e.toString());\n      instrumentation.errorIgnored();\n      return false;\n    } catch (AmazonClientException e) {\n      throw translateException(\"delete\", path, e);\n    }\n  }\n\n  /**\n   * Create a fake directory if required.\n   * That is: it is not the root path and the path does not exist.\n   * Retry policy: retrying; untranslated.\n   * @param f path to create\n   * @throws IOException IO problem\n   */\n  @Retries.RetryTranslated\n  private void createFakeDirectoryIfNecessary(Path f)\n      throws IOException, AmazonClientException {\n    String key = pathToKey(f);\n    // we only make the LIST call; the codepaths to get here should not\n    // be reached if there is an empty dir marker -and if they do, it\n    // is mostly harmless to create a new one.\n    if (!key.isEmpty() && !s3Exists(f, StatusProbeEnum.DIRECTORIES)) {\n      LOG.debug(\"Creating new fake directory at {}\", f);\n      createFakeDirectory(key, putOptionsForPath(f));\n    }\n  }\n\n  /**\n   * Create a fake parent directory if required.\n   * That is: it parent is not the root path and does not yet exist.\n   * @param path whose parent is created if needed.\n   * @throws IOException IO problem\n   */\n  @Retries.RetryTranslated\n  @VisibleForTesting\n  protected void maybeCreateFakeParentDirectory(Path path)\n      throws IOException, AmazonClientException {\n    Path parent = path.getParent();\n    if (parent != null && !parent.isRoot() && !isUnderMagicCommitPath(parent)) {\n      createFakeDirectoryIfNecessary(parent);\n    }\n  }\n\n  /**\n   * Override subclass such that we benefit for async listing done\n   * in {@code S3AFileSystem}. See {@code Listing#ObjectListingIterator}.\n   * {@inheritDoc}\n   *\n   */\n  @Override\n  @AuditEntryPoint\n  public RemoteIterator<FileStatus> listStatusIterator(Path p)\n          throws FileNotFoundException, IOException {\n    Path path = qualify(p);\n    return typeCastingRemoteIterator(trackDurationAndSpan(\n        INVOCATION_LIST_STATUS, path, () ->\n            once(\"listStatus\", path.toString(), () ->\n                innerListStatus(p))));\n  }\n\n  /**\n   * List the statuses of the files/directories in the given path if the path is\n   * a directory.\n   *\n   * @param f given path\n   * @return the statuses of the files/directories in the given patch\n   * @throws FileNotFoundException when the path does not exist;\n   *         IOException see specific implementation\n   */\n  @Override\n  @AuditEntryPoint\n  public FileStatus[] listStatus(Path f) throws FileNotFoundException,\n      IOException {\n    Path path = qualify(f);\n    return trackDurationAndSpan(INVOCATION_LIST_STATUS, path, () ->\n        once(\"listStatus\", path.toString(),\n            () -> iteratorToStatuses(innerListStatus(path))));\n  }\n\n  /**\n   * List the statuses of the files/directories in the given path if the path is\n   * a directory. The returned iterator is within the current active span.\n   *\n   * Auditing: This method MUST be called within a span.\n   * The span is attached to the iterator. All further S3 calls\n   * made by the iterator will be within the span.\n   * @param f qualified path\n   * @return the statuses of the files/directories in the given patch\n   * @throws FileNotFoundException when the path does not exist;\n   * @throws IOException due to an IO problem.\n   * @throws AmazonClientException on failures inside the AWS SDK\n   */\n  private RemoteIterator<S3AFileStatus> innerListStatus(Path f)\n          throws FileNotFoundException,\n          IOException, AmazonClientException {\n    Path path = qualify(f);\n    LOG.debug(\"List status for path: {}\", path);\n\n    final RemoteIterator<S3AFileStatus> statusIt = listing\n        .getFileStatusesAssumingNonEmptyDir(path, getActiveAuditSpan());\n    if (!statusIt.hasNext()) {\n      // We may have an empty dir, or may have file or may have nothing.\n      // So we call innerGetFileStatus to get the status, this may throw\n      // FileNotFoundException if we have nothing.\n      // So We are guaranteed to have either a dir marker or a file.\n      final S3AFileStatus fileStatus = innerGetFileStatus(path, false,\n              StatusProbeEnum.ALL);\n      // If it is a file return directly.\n      if (fileStatus.isFile()) {\n        LOG.debug(\"Adding: rd (not a dir): {}\", path);\n        S3AFileStatus[] stats = new S3AFileStatus[1];\n        stats[0] = fileStatus;\n        return listing.createProvidedFileStatusIterator(\n                stats,\n                ACCEPT_ALL,\n                Listing.ACCEPT_ALL_BUT_S3N);\n      }\n    }\n    // Here we have a directory which may or may not be empty.\n    return statusIt;\n  }\n\n  /**\n   * Is a path to be considered as authoritative?\n   * is a  store with the supplied path under\n   * one of the paths declared as authoritative.\n   * @param path path\n   * @return true if the path is auth\n   */\n  public boolean allowAuthoritative(final Path path) {\n    return S3Guard.allowAuthoritative(path, this,\n        allowAuthoritativePaths);\n  }\n\n  /**\n   * Create a {@code ListObjectsRequest} request against this bucket,\n   * with the maximum keys returned in a query set by {@link #maxKeys}.\n   * @param key key for request\n   * @param delimiter any delimiter\n   * @return the request\n   */\n  @VisibleForTesting\n  public S3ListRequest createListObjectsRequest(String key,\n      String delimiter) {\n    return createListObjectsRequest(key, delimiter, maxKeys);\n  }\n\n  /**\n   * Create the List objects request appropriate for the\n   * active list request option.\n   * @param key key for request\n   * @param delimiter any delimiter\n   * @param limit limit of keys\n   * @return the request\n   */\n  private S3ListRequest createListObjectsRequest(String key,\n      String delimiter, int limit) {\n    if (!useListV1) {\n      ListObjectsV2Request request =\n          getRequestFactory().newListObjectsV2Request(\n              key, delimiter, limit);\n      return S3ListRequest.v2(request);\n    } else {\n      ListObjectsRequest request =\n          getRequestFactory().newListObjectsV1Request(\n              key, delimiter, limit);\n      return S3ListRequest.v1(request);\n    }\n  }\n\n  /**\n   * Set the current working directory for the given file system. All relative\n   * paths will be resolved relative to it.\n   *\n   * @param newDir the current working directory.\n   */\n  public void setWorkingDirectory(Path newDir) {\n    workingDir = makeQualified(newDir);\n  }\n\n  /**\n   * Get the current working directory for the given file system.\n   * @return the directory pathname\n   */\n  public Path getWorkingDirectory() {\n    return workingDir;\n  }\n\n  /**\n   * Get the username of the FS.\n   * @return the short name of the user who instantiated the FS\n   */\n  public String getUsername() {\n    return username;\n  }\n\n  /**\n   * Get the owner of this FS: who created it?\n   * @return the owner of the FS.\n   */\n  public UserGroupInformation getOwner() {\n    return owner;\n  }\n\n  /**\n   *\n   * Make the given path and all non-existent parents into\n   * directories. Has the semantics of Unix {@code 'mkdir -p'}.\n   * Existence of the directory hierarchy is not an error.\n   * Parent elements are scanned to see if any are a file,\n   * <i>except under __magic</i> paths.\n   * There the FS assumes that the destination directory creation\n   * did that scan and that paths in job/task attempts are all\n   * \"well formed\"\n   * @param p path to create\n   * @param permission to apply to path\n   * @return true if a directory was created or already existed\n   * @throws FileAlreadyExistsException there is a file at the path specified\n   * or is discovered on one of its ancestors.\n   * @throws IOException other IO problems\n   */\n  @Override\n  @AuditEntryPoint\n  public boolean mkdirs(Path p, FsPermission permission) throws IOException,\n      FileAlreadyExistsException {\n    Path path = qualify(p);\n    return trackDurationAndSpan(\n        INVOCATION_MKDIRS, path,\n        new MkdirOperation(\n            createStoreContext(),\n            path,\n            createMkdirOperationCallbacks(),\n            isMagicCommitPath(path)));\n  }\n\n  /**\n   * Override point: create the callbacks for Mkdir.\n   * This does not create a new span; caller must be in one.\n   * @return an implementation of the MkdirCallbacks,\n   */\n  @VisibleForTesting\n  public MkdirOperation.MkdirCallbacks createMkdirOperationCallbacks() {\n    return new MkdirOperationCallbacksImpl();\n  }\n\n  /**\n   * Callbacks from the {@link MkdirOperation}.\n   */\n  protected class MkdirOperationCallbacksImpl implements\n      MkdirOperation.MkdirCallbacks {\n\n    @Override\n    public S3AFileStatus probePathStatus(final Path path,\n        final Set<StatusProbeEnum> probes) throws IOException {\n      return S3AFileSystem.this.innerGetFileStatus(path, false, probes);\n    }\n\n    @Override\n    public void createFakeDirectory(final Path dir, final boolean keepMarkers)\n        throws IOException {\n      S3AFileSystem.this.createFakeDirectory(\n          pathToKey(dir),\n          keepMarkers\n              ? PutObjectOptions.keepingDirs()\n              : putOptionsForPath(dir));\n    }\n  }\n\n  /**\n   * This is a very slow operation against object storage.\n   * Execute it as a single span with whatever optimizations\n   * have been implemented.\n   * {@inheritDoc}\n   */\n  @Override\n  @Retries.RetryTranslated\n  @AuditEntryPoint\n  public ContentSummary getContentSummary(final Path f) throws IOException {\n    final Path path = qualify(f);\n    return trackDurationAndSpan(\n        INVOCATION_GET_CONTENT_SUMMARY, path,\n        new GetContentSummaryOperation(\n            createStoreContext(),\n            path,\n            createGetContentSummaryCallbacks()));\n  }\n\n  /**\n   * Override point: create the callbacks for getContentSummary.\n   * This does not create a new span; caller must be in one.\n   * @return an implementation of the GetContentSummaryCallbacksImpl\n   */\n  protected GetContentSummaryOperation.GetContentSummaryCallbacks\n      createGetContentSummaryCallbacks() {\n    return new GetContentSummaryCallbacksImpl();\n  }\n\n  /**\n   * Callbacks from the {@link GetContentSummaryOperation}.\n   */\n  protected class GetContentSummaryCallbacksImpl implements\n      GetContentSummaryOperation.GetContentSummaryCallbacks {\n\n    @Override\n    public S3AFileStatus probePathStatus(final Path path,\n        final Set<StatusProbeEnum> probes) throws IOException {\n      return S3AFileSystem.this.innerGetFileStatus(path, false, probes);\n    }\n\n    @Override\n    public RemoteIterator<S3ALocatedFileStatus> listFilesIterator(final Path path,\n        final boolean recursive) throws IOException {\n      return S3AFileSystem.this.innerListFiles(path, recursive, Listing.ACCEPT_ALL_BUT_S3N, null);\n    }\n  }\n\n  /**\n   * Soft check of access by forwarding to the audit manager\n   * and so on to the auditor.\n   * {@inheritDoc}\n   */\n  @Override\n  @AuditEntryPoint\n  public void access(final Path f, final FsAction mode)\n      throws AccessControlException, FileNotFoundException, IOException {\n    Path path = qualify(f);\n    LOG.debug(\"check access mode {} for {}\", path, mode);\n    trackDurationAndSpan(\n        INVOCATION_ACCESS, path, () -> {\n          final S3AFileStatus stat = innerGetFileStatus(path, false,\n              StatusProbeEnum.ALL);\n          if (!getAuditManager().checkAccess(path, stat, mode)) {\n            incrementStatistic(AUDIT_ACCESS_CHECK_FAILURE);\n            throw new AccessControlException(String.format(\n                \"Permission denied: user=%s, path=\\\"%s\\\":%s:%s:%s%s\",\n                getOwner().getUserName(),\n                stat.getPath(),\n                stat.getOwner(), stat.getGroup(),\n                stat.isDirectory() ? \"d\" : \"-\", mode));\n          }\n          // simply for the API binding.\n          return true;\n        });\n  }\n\n  /**\n   * Return a file status object that represents the path.\n   * @param f The path we want information from\n   * @return a FileStatus object\n   * @throws FileNotFoundException when the path does not exist\n   * @throws IOException on other problems.\n   */\n  @Override\n  @AuditEntryPoint\n  @Retries.RetryTranslated\n  public FileStatus getFileStatus(final Path f) throws IOException {\n    Path path = qualify(f);\n    return trackDurationAndSpan(\n        INVOCATION_GET_FILE_STATUS, path, () ->\n            innerGetFileStatus(path, false, StatusProbeEnum.ALL));\n  }\n\n  /**\n   * Get the status of a file or directory.\n   * Internal version of {@link #getFileStatus(Path)}.\n   * @param f The path we want information from\n   * @param needEmptyDirectoryFlag if true, implementation will calculate\n   *        a TRUE or FALSE value for {@link S3AFileStatus#isEmptyDirectory()}\n   * @param probes probes to make.\n   * @return a S3AFileStatus object\n   * @throws FileNotFoundException when the path does not exist\n   * @throws IOException on other problems.\n   */\n  @VisibleForTesting\n  @Retries.RetryTranslated\n  S3AFileStatus innerGetFileStatus(final Path f,\n      final boolean needEmptyDirectoryFlag,\n      final Set<StatusProbeEnum> probes) throws IOException {\n    final Path path = qualify(f);\n    String key = pathToKey(path);\n    LOG.debug(\"Getting path status for {}  ({}); needEmptyDirectory={}\",\n        path, key, needEmptyDirectoryFlag);\n    return s3GetFileStatus(path,\n        key,\n        probes,\n        needEmptyDirectoryFlag);\n\n  }\n\n  /**\n   * Probe store for file status with control of which probes are issued..\n   * Used to implement {@link #innerGetFileStatus(Path, boolean, Set)},\n   * and for direct management of empty directory blobs.\n   *\n   * Checks made, in order:\n   * <ol>\n   *   <li>\n   *     Head: look for an object at the given key, provided that\n   *     the key doesn't end in \"/\"\n   *   </li>\n   *   <li>\n   *     DirMarker/List: issue a LIST on the key (with / if needed), require one\n   *     entry to be found for the path to be considered a non-empty directory.\n   *   </li>\n   * </ol>\n   *\n   * Notes:\n   * <ul>\n   *   <li>\n   *     Objects ending in / are treated as directory markers,\n   *     irrespective of length.\n   *   </li>\n   *   <li>\n   *     The HEAD requests require the permissions to read an object,\n   *     including (we believe) the ability to decrypt the file.\n   *     At the very least, for SSE-C markers, you need the same key on\n   *     the client for the HEAD to work.\n   *   </li>\n   *   <li>\n   *     The List probe needs list permission.\n   *   </li>\n   * </ul>\n   *\n   * Retry policy: retry translated.\n   * @param path Qualified path\n   * @param key  Key string for the path\n   * @param probes probes to make\n   * @param needEmptyDirectoryFlag if true, implementation will calculate\n   *        a TRUE or FALSE value for {@link S3AFileStatus#isEmptyDirectory()}\n   * @return Status\n   * @throws FileNotFoundException the supplied probes failed.\n   * @throws IOException on other problems.\n   */\n  @VisibleForTesting\n  @Retries.RetryTranslated\n  S3AFileStatus s3GetFileStatus(final Path path,\n      final String key,\n      final Set<StatusProbeEnum> probes,\n      final boolean needEmptyDirectoryFlag) throws IOException {\n    LOG.debug(\"S3GetFileStatus {}\", path);\n    // either you aren't looking for the directory flag, or you are,\n    // and if you are, the probe list must contain list.\n    Preconditions.checkArgument(!needEmptyDirectoryFlag\n        || probes.contains(StatusProbeEnum.List),\n        \"s3GetFileStatus(%s) wants to know if a directory is empty but\"\n            + \" does not request a list probe\", path);\n    if (key.isEmpty() && !needEmptyDirectoryFlag) {\n      return new S3AFileStatus(Tristate.UNKNOWN, path, username);\n    }\n\n    if (!key.isEmpty() && !key.endsWith(\"/\")\n        && probes.contains(StatusProbeEnum.Head)) {\n      try {\n        // look for the simple file\n        ObjectMetadata meta = getObjectMetadata(key);\n        LOG.debug(\"Found exact file: normal file {}\", key);\n        long contentLength = meta.getContentLength();\n        // check if CSE is enabled, then strip padded length.\n        if (isCSEEnabled\n            && meta.getUserMetaDataOf(Headers.CRYPTO_CEK_ALGORITHM) != null\n            && contentLength >= CSE_PADDING_LENGTH) {\n          contentLength -= CSE_PADDING_LENGTH;\n        }\n        return new S3AFileStatus(contentLength,\n            dateToLong(meta.getLastModified()),\n            path,\n            getDefaultBlockSize(path),\n            username,\n            meta.getETag(),\n            meta.getVersionId());\n      } catch (AmazonServiceException e) {\n        // if the response is a 404 error, it just means that there is\n        // no file at that path...the remaining checks will be needed.\n        // But: an empty bucket is also a 404, so check for that\n        // and fail.\n        if (e.getStatusCode() != SC_404 || isUnknownBucket(e)) {\n          throw translateException(\"getFileStatus\", path, e);\n        }\n      } catch (AmazonClientException e) {\n        throw translateException(\"getFileStatus\", path, e);\n      }\n    }\n\n    // execute the list\n    if (probes.contains(StatusProbeEnum.List)) {\n      try {\n        // this will find a marker dir / as well as an entry.\n        // When making a simple \"is this a dir check\" all is good.\n        // but when looking for an empty dir, we need to verify there are no\n        // children, so ask for two entries, so as to find\n        // a child\n        String dirKey = maybeAddTrailingSlash(key);\n        // list size is dir marker + at least one entry\n\n        final int listSize = 2;\n        S3ListRequest request = createListObjectsRequest(dirKey, \"/\",\n            listSize);\n        // execute the request\n        S3ListResult listResult = listObjects(request,\n            getDurationTrackerFactory());\n\n        if (listResult.hasPrefixesOrObjects()) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Found path as directory (with /)\");\n            listResult.logAtDebug(LOG);\n          }\n          // At least one entry has been found.\n          // If looking for an empty directory, the marker must exist but no\n          // children.\n          // So the listing must contain the marker entry only.\n          if (needEmptyDirectoryFlag\n              && listResult.representsEmptyDirectory(dirKey)) {\n            return new S3AFileStatus(Tristate.TRUE, path, username);\n          }\n          // either an empty directory is not needed, or the\n          // listing does not meet the requirements.\n          return new S3AFileStatus(Tristate.FALSE, path, username);\n        } else if (key.isEmpty()) {\n          LOG.debug(\"Found root directory\");\n          return new S3AFileStatus(Tristate.TRUE, path, username);\n        }\n      } catch (AmazonServiceException e) {\n        if (e.getStatusCode() != SC_404 || isUnknownBucket(e)) {\n          throw translateException(\"getFileStatus\", path, e);\n        }\n      } catch (AmazonClientException e) {\n        throw translateException(\"getFileStatus\", path, e);\n      }\n    }\n\n    LOG.debug(\"Not Found: {}\", path);\n    throw new FileNotFoundException(\"No such file or directory: \" + path);\n  }\n\n  /**\n   * Probe S3 for a file or dir existing, with the given probe set.\n   * Retry policy: retrying; translated.\n   * @param path qualified path to look for\n   * @param probes probes to make\n   * @return true if path exists in S3\n   * @throws IOException IO failure\n   */\n  @Retries.RetryTranslated\n  private boolean s3Exists(final Path path, final Set<StatusProbeEnum> probes)\n      throws IOException {\n    String key = pathToKey(path);\n    try {\n      s3GetFileStatus(path, key, probes, false);\n      return true;\n    } catch (FileNotFoundException e) {\n      return false;\n    }\n  }\n\n  /**\n   * The src file is on the local disk.  Add it to FS at\n   * the given dst name.\n   *\n   * This version doesn't need to create a temporary file to calculate the md5.\n   * Sadly this doesn't seem to be used by the shell cp :(\n   *\n   * delSrc indicates if the source should be removed\n   * @param delSrc whether to delete the src\n   * @param overwrite whether to overwrite an existing file\n   * @param src path\n   * @param dst path\n   * @throws IOException IO problem\n   * @throws FileAlreadyExistsException the destination file exists and\n   * overwrite==false\n   * @throws AmazonClientException failure in the AWS SDK\n   */\n  @Override\n  @AuditEntryPoint\n  public void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src,\n                                Path dst) throws IOException {\n    checkNotClosed();\n    LOG.debug(\"Copying local file from {} to {}\", src, dst);\n    trackDurationAndSpan(INVOCATION_COPY_FROM_LOCAL_FILE, dst,\n        () -> new CopyFromLocalOperation(\n            createStoreContext(),\n            src,\n            dst,\n            delSrc,\n            overwrite,\n            createCopyFromLocalCallbacks()).execute());\n  }\n\n  protected CopyFromLocalOperation.CopyFromLocalOperationCallbacks\n      createCopyFromLocalCallbacks() throws IOException {\n    LocalFileSystem local = getLocal(getConf());\n    return new CopyFromLocalCallbacksImpl(local);\n  }\n\n  protected final class CopyFromLocalCallbacksImpl implements\n      CopyFromLocalOperation.CopyFromLocalOperationCallbacks {\n    private final LocalFileSystem local;\n\n    private CopyFromLocalCallbacksImpl(LocalFileSystem local) {\n      this.local = local;\n    }\n\n    @Override\n    public RemoteIterator<LocatedFileStatus> listLocalStatusIterator(\n        final Path path) throws IOException {\n      return local.listLocatedStatus(path);\n    }\n\n    @Override\n    public File pathToLocalFile(Path path) {\n      return local.pathToFile(path);\n    }\n\n    @Override\n    public boolean deleteLocal(Path path, boolean recursive) throws IOException {\n      return local.delete(path, recursive);\n    }\n\n    @Override\n    public void copyLocalFileFromTo(File file, Path from, Path to) throws IOException {\n      trackDurationAndSpan(\n          OBJECT_PUT_REQUESTS,\n          to,\n          () -> {\n            final String key = pathToKey(to);\n            final ObjectMetadata om = newObjectMetadata(file.length());\n            Progressable progress = null;\n            PutObjectRequest putObjectRequest = newPutObjectRequest(key, om, file);\n            S3AFileSystem.this.invoker.retry(\n                \"putObject(\" + \"\" + \")\", to.toString(),\n                true,\n                () -> executePut(putObjectRequest, progress, putOptionsForPath(to)));\n\n            return null;\n          });\n    }\n\n    @Override\n    public FileStatus getFileStatus(Path f) throws IOException {\n      return S3AFileSystem.this.getFileStatus(f);\n    }\n\n    @Override\n    public boolean createEmptyDir(Path path, StoreContext storeContext)\n        throws IOException {\n      return trackDuration(getDurationTrackerFactory(),\n          INVOCATION_MKDIRS.getSymbol(),\n          new MkdirOperation(\n              storeContext,\n              path,\n              createMkdirOperationCallbacks(), false));\n    }\n  }\n\n  /**\n   * Execute a PUT via the transfer manager, blocking for completion.\n   * If the waiting for completion is interrupted, the upload will be\n   * aborted before an {@code InterruptedIOException} is thrown.\n   * @param putObjectRequest request\n   * @param progress optional progress callback\n   * @param putOptions put object options\n   * @return the upload result\n   * @throws InterruptedIOException if the blocking was interrupted.\n   */\n  @Retries.OnceRaw(\"For PUT; post-PUT actions are RetrySwallowed\")\n  UploadResult executePut(\n      final PutObjectRequest putObjectRequest,\n      final Progressable progress,\n      final PutObjectOptions putOptions)\n      throws InterruptedIOException {\n    String key = putObjectRequest.getKey();\n    long len = getPutRequestLength(putObjectRequest);\n    UploadInfo info = putObject(putObjectRequest);\n    Upload upload = info.getUpload();\n    ProgressableProgressListener listener = new ProgressableProgressListener(\n        this, key, upload, progress);\n    upload.addProgressListener(listener);\n    UploadResult result = waitForUploadCompletion(key, info);\n    listener.uploadCompleted();\n\n    // post-write actions\n    finishedWrite(key, len,\n        result.getETag(), result.getVersionId(), putOptions);\n    return result;\n  }\n\n  /**\n   * Wait for an upload to complete.\n   * If the waiting for completion is interrupted, the upload will be\n   * aborted before an {@code InterruptedIOException} is thrown.\n   * If the upload (or its result collection) failed, this is where\n   * the failure is raised as an AWS exception.\n   * Calls {@link #incrementPutCompletedStatistics(boolean, long)}\n   * to update the statistics.\n   * @param key destination key\n   * @param uploadInfo upload to wait for\n   * @return the upload result\n   * @throws InterruptedIOException if the blocking was interrupted.\n   */\n  @Retries.OnceRaw\n  UploadResult waitForUploadCompletion(String key, UploadInfo uploadInfo)\n      throws InterruptedIOException {\n    Upload upload = uploadInfo.getUpload();\n    try {\n      UploadResult result = upload.waitForUploadResult();\n      incrementPutCompletedStatistics(true, uploadInfo.getLength());\n      return result;\n    } catch (InterruptedException e) {\n      LOG.info(\"Interrupted: aborting upload\");\n      incrementPutCompletedStatistics(false, uploadInfo.getLength());\n      upload.abort();\n      throw (InterruptedIOException)\n          new InterruptedIOException(\"Interrupted in PUT to \"\n              + keyToQualifiedPath(key))\n          .initCause(e);\n    }\n  }\n\n  /**\n   * This override bypasses checking for existence.\n   *\n   * @param f the path to delete; this may be unqualified.\n   * @return true, always.   * @param f the path to delete.\n   * @return  true if deleteOnExit is successful, otherwise false.\n   * @throws IOException IO failure\n   */\n  @Override\n  public boolean deleteOnExit(Path f) throws IOException {\n    Path qualifedPath = makeQualified(f);\n    synchronized (deleteOnExit) {\n      deleteOnExit.add(qualifedPath);\n    }\n    return true;\n  }\n\n  /**\n   * Cancel the scheduled deletion of the path when the FileSystem is closed.\n   * @param f the path to cancel deletion\n   * @return true if the path was found in the delete-on-exit list.\n   */\n  @Override\n  public boolean cancelDeleteOnExit(Path f) {\n    Path qualifedPath = makeQualified(f);\n    synchronized (deleteOnExit) {\n      return deleteOnExit.remove(qualifedPath);\n    }\n  }\n\n  /**\n   * Delete all paths that were marked as delete-on-exit. This recursively\n   * deletes all files and directories in the specified paths. It does not\n   * check if file exists and filesystem is closed.\n   *\n   * The time to process this operation is {@code O(paths)}, with the actual\n   * time dependent on the time for existence and deletion operations to\n   * complete, successfully or not.\n   */\n  @Override\n  protected void processDeleteOnExit() {\n    synchronized (deleteOnExit) {\n      for (Iterator<Path> iter = deleteOnExit.iterator(); iter.hasNext();) {\n        Path path = iter.next();\n        try {\n          deleteWithoutCloseCheck(path, true);\n        } catch (IOException e) {\n          LOG.info(\"Ignoring failure to deleteOnExit for path {}\", path);\n          LOG.debug(\"The exception for deleteOnExit is {}\", e);\n        }\n        iter.remove();\n      }\n    }\n  }\n\n  /**\n   * Close the filesystem. This shuts down all transfers.\n   * @throws IOException IO problem\n   */\n  @Override\n  public void close() throws IOException {\n    if (closed.getAndSet(true)) {\n      // already closed\n      return;\n    }\n    isClosed = true;\n    LOG.debug(\"Filesystem {} is closed\", uri);\n    try {\n      super.close();\n    } finally {\n      stopAllServices();\n      // log IO statistics, including of any file deletion during\n      // superclass close\n      if (getConf() != null) {\n        String iostatisticsLoggingLevel =\n            getConf().getTrimmed(IOSTATISTICS_LOGGING_LEVEL,\n                IOSTATISTICS_LOGGING_LEVEL_DEFAULT);\n        logIOStatisticsAtLevel(LOG, iostatisticsLoggingLevel, getIOStatistics());\n      }\n    }\n  }\n\n  /**\n   * Stop all services.\n   * This is invoked in close() and during failures of initialize()\n   * -make sure that all operations here are robust to failures in\n   * both the expected state of this FS and of failures while being stopped.\n   */\n  protected synchronized void stopAllServices() {\n    // shutting down the transfer manager also shuts\n    // down the S3 client it is bonded to.\n    if (transfers != null) {\n      try {\n        transfers.shutdownNow(true);\n      } catch (RuntimeException e) {\n        // catch and swallow for resilience.\n        LOG.debug(\"When shutting down\", e);\n      }\n      transfers = null;\n    }\n    // At this point the S3A client is shut down,\n    // now the executor pools are closed\n    HadoopExecutors.shutdown(boundedThreadPool, LOG,\n        THREAD_POOL_SHUTDOWN_DELAY_SECONDS, TimeUnit.SECONDS);\n    boundedThreadPool = null;\n    HadoopExecutors.shutdown(unboundedThreadPool, LOG,\n        THREAD_POOL_SHUTDOWN_DELAY_SECONDS, TimeUnit.SECONDS);\n    unboundedThreadPool = null;\n    if (futurePool != null) {\n      futurePool.shutdown(LOG, THREAD_POOL_SHUTDOWN_DELAY_SECONDS, TimeUnit.SECONDS);\n      futurePool = null;\n    }\n    // other services are shutdown.\n    cleanupWithLogger(LOG,\n        instrumentation,\n        delegationTokens.orElse(null),\n        signerManager,\n        auditManager);\n    closeAutocloseables(LOG, credentials);\n    delegationTokens = Optional.empty();\n    signerManager = null;\n    credentials = null;\n  }\n\n  /**\n   * Verify that the input stream is open. Non blocking; this gives\n   * the last state of the volatile {@link #closed} field.\n   * @throws IOException if the connection is closed.\n   */\n  private void checkNotClosed() throws IOException {\n    if (isClosed) {\n      throw new IOException(uri + \": \" + E_FS_CLOSED);\n    }\n  }\n\n  /**\n   * Get the delegation token support for this filesystem;\n   * not null iff delegation support is enabled.\n   * @return the token support, or an empty option.\n   */\n  @VisibleForTesting\n  public Optional<S3ADelegationTokens> getDelegationTokens() {\n    return delegationTokens;\n  }\n\n  /**\n   * Return a service name iff delegation tokens are enabled and the\n   * token binding is issuing delegation tokens.\n   * @return the canonical service name or null\n   */\n  @Override\n  public String getCanonicalServiceName() {\n    // this could all be done in map statements, but it'd be harder to\n    // understand and maintain.\n    // Essentially: no DTs, no canonical service name.\n    if (!delegationTokens.isPresent()) {\n      return null;\n    }\n    // DTs present: ask the binding if it is willing to\n    // serve tokens (or fail noisily).\n    S3ADelegationTokens dt = delegationTokens.get();\n    return dt.getTokenIssuingPolicy() != NoTokensAvailable\n        ? dt.getCanonicalServiceName()\n        : null;\n  }\n\n  /**\n   * Get a delegation token if the FS is set up for them.\n   * If the user already has a token, it is returned,\n   * <i>even if it has expired</i>.\n   * @param renewer the account name that is allowed to renew the token.\n   * @return the delegation token or null\n   * @throws IOException IO failure\n   */\n  @Override\n  @AuditEntryPoint\n  public Token<AbstractS3ATokenIdentifier> getDelegationToken(String renewer)\n      throws IOException {\n    checkNotClosed();\n    LOG.debug(\"Delegation token requested\");\n    if (delegationTokens.isPresent()) {\n      return trackDurationAndSpan(\n          INVOCATION_GET_DELEGATION_TOKEN, null, () ->\n              delegationTokens.get().getBoundOrNewDT(\n                  encryptionSecrets,\n                  (renewer != null ? new Text(renewer) : new Text())));\n    } else {\n      // Delegation token support is not set up\n      LOG.debug(\"Token support is not enabled\");\n      return null;\n    }\n  }\n\n  /**\n   * Ask any DT plugin for any extra token issuers.\n   * These do not get told of the encryption secrets and can\n   * return any type of token.\n   * This allows DT plugins to issue extra tokens for\n   * ancillary services.\n   */\n  @Override\n  public DelegationTokenIssuer[] getAdditionalTokenIssuers()\n      throws IOException {\n    checkNotClosed();\n    if (delegationTokens.isPresent()) {\n      return delegationTokens.get().getAdditionalTokenIssuers();\n    } else {\n      // Delegation token support is not set up\n      LOG.debug(\"Token support is not enabled\");\n      return null;\n    }\n  }\n\n  /**\n   * Build the AWS policy for restricted access to the resources needed\n   * by this bucket.\n   * if needed, and KMS operations.\n   * @param access access level desired.\n   * @return a policy for use in roles\n   */\n  @Override\n  @InterfaceAudience.Private\n  public List<RoleModel.Statement> listAWSPolicyRules(\n      final Set<AccessLevel> access) {\n    if (access.isEmpty()) {\n      return Collections.emptyList();\n    }\n    List<RoleModel.Statement> statements = new ArrayList<>(\n        allowS3Operations(bucket,\n            access.contains(AccessLevel.WRITE)\n                || access.contains(AccessLevel.ADMIN)));\n\n    // no attempt is made to qualify KMS access; there's no\n    // way to predict read keys, and not worried about granting\n    // too much encryption access.\n    statements.add(STATEMENT_ALLOW_SSE_KMS_RW);\n\n    return statements;\n  }\n\n  /**\n   * Copy a single object in the bucket via a COPY operation.\n   * There's no update of metadata, directory markers, etc.\n   * Callers must implement.\n   * @param srcKey source object path\n   * @param dstKey destination object path\n   * @param size object size\n   * @param srcAttributes S3 attributes of the source object\n   * @param readContext the read context\n   * @return the result of the copy\n   * @throws InterruptedIOException the operation was interrupted\n   * @throws IOException Other IO problems\n   */\n  @Retries.RetryTranslated\n  private CopyResult copyFile(String srcKey, String dstKey, long size,\n      S3ObjectAttributes srcAttributes, S3AReadOpContext readContext)\n      throws IOException, InterruptedIOException  {\n    LOG.debug(\"copyFile {} -> {} \", srcKey, dstKey);\n\n    ProgressListener progressListener = progressEvent -> {\n      switch (progressEvent.getEventType()) {\n      case TRANSFER_PART_COMPLETED_EVENT:\n        incrementWriteOperations();\n        break;\n      default:\n        break;\n      }\n    };\n\n    ChangeTracker changeTracker = new ChangeTracker(\n        keyToQualifiedPath(srcKey).toString(),\n        changeDetectionPolicy,\n        readContext.getS3AStatisticsContext()\n            .newInputStreamStatistics()\n            .getChangeTrackerStatistics(),\n        srcAttributes);\n\n    String action = \"copyFile(\" + srcKey + \", \" + dstKey + \")\";\n    Invoker readInvoker = readContext.getReadInvoker();\n\n    ObjectMetadata srcom;\n    try {\n      srcom = once(action, srcKey,\n          () ->\n              getObjectMetadata(srcKey, changeTracker, readInvoker, \"copy\"));\n    } catch (FileNotFoundException e) {\n      // if rename fails at this point it means that the expected file was not\n      // found.\n      // This means the File was deleted since LIST enumerated it.\n      LOG.debug(\"getObjectMetadata({}) failed to find an expected file\",\n          srcKey, e);\n      // We create an exception, but the text depends on the S3Guard state\n      throw new RemoteFileChangedException(\n          keyToQualifiedPath(srcKey).toString(),\n          action,\n          RemoteFileChangedException.FILE_NOT_FOUND_SINGLE_ATTEMPT,\n          e);\n    }\n\n    return readInvoker.retry(\n        action, srcKey,\n        true,\n        () -> {\n          CopyObjectRequest copyObjectRequest =\n              getRequestFactory().newCopyObjectRequest(srcKey, dstKey, srcom);\n          changeTracker.maybeApplyConstraint(copyObjectRequest);\n          incrementStatistic(OBJECT_COPY_REQUESTS);\n          Copy copy = transfers.copy(copyObjectRequest,\n              getAuditManager().createStateChangeListener());\n          copy.addProgressListener(progressListener);\n          CopyOutcome copyOutcome = CopyOutcome.waitForCopy(copy);\n          InterruptedException interruptedException =\n              copyOutcome.getInterruptedException();\n          if (interruptedException != null) {\n            // copy interrupted: convert to an IOException.\n            throw (IOException)new InterruptedIOException(\n                \"Interrupted copying \" + srcKey\n                    + \" to \" + dstKey + \", cancelling\")\n                .initCause(interruptedException);\n          }\n          SdkBaseException awsException = copyOutcome.getAwsException();\n          if (awsException != null) {\n            changeTracker.processException(awsException, \"copy\");\n            throw awsException;\n          }\n          CopyResult result = copyOutcome.getCopyResult();\n          changeTracker.processResponse(result);\n          incrementWriteOperations();\n          instrumentation.filesCopied(1, size);\n          return result;\n        });\n  }\n\n  /**\n   * Initiate a multipart upload from the preconfigured request.\n   * Retry policy: none + untranslated.\n   * @param request request to initiate\n   * @return the result of the call\n   * @throws AmazonClientException on failures inside the AWS SDK\n   * @throws IOException Other IO problems\n   */\n  @Retries.OnceRaw\n  InitiateMultipartUploadResult initiateMultipartUpload(\n      InitiateMultipartUploadRequest request) throws IOException {\n    LOG.debug(\"Initiate multipart upload to {}\", request.getKey());\n    return trackDurationOfSupplier(getDurationTrackerFactory(),\n        OBJECT_MULTIPART_UPLOAD_INITIATED.getSymbol(),\n        () -> getAmazonS3Client().initiateMultipartUpload(request));\n  }\n\n  /**\n   * Perform post-write actions.\n   * <p>\n   * This operation MUST be called after any PUT/multipart PUT completes\n   * successfully.\n   * <p>\n   * The actions include calling\n   * {@link #deleteUnnecessaryFakeDirectories(Path)}\n   * if directory markers are not being retained.\n   * @param key key written to\n   * @param length  total length of file written\n   * @param eTag eTag of the written object\n   * @param versionId S3 object versionId of the written object\n   * @param putOptions put object options\n   */\n  @InterfaceAudience.Private\n  @Retries.RetryExceptionsSwallowed\n  void finishedWrite(\n      String key,\n      long length,\n      String eTag,\n      String versionId,\n      PutObjectOptions putOptions) {\n    LOG.debug(\"Finished write to {}, len {}. etag {}, version {}\",\n        key, length, eTag, versionId);\n    Preconditions.checkArgument(length >= 0, \"content length is negative\");\n    if (!putOptions.isKeepMarkers()) {\n      Path p = keyToQualifiedPath(key);\n      deleteUnnecessaryFakeDirectories(p.getParent());\n    }\n  }\n\n  /**\n   * Should we keep directory markers under the path being created\n   * by mkdir/file creation/rename?\n   * This is done if marker retention is enabled for the path,\n   * or it is under a magic path where we are saving IOPs\n   * knowing that all committers are on the same code version and\n   * therefore marker aware.\n   * @param path path to probe\n   * @return true if the markers MAY be retained,\n   * false if they MUST be deleted\n   */\n  private boolean keepDirectoryMarkers(Path path) {\n    return directoryPolicy.keepDirectoryMarkers(path)\n        || isUnderMagicCommitPath(path);\n  }\n\n  /**\n   * Should we keep directory markers under the path being created\n   * by mkdir/file creation/rename?\n   * See {@link #keepDirectoryMarkers(Path)} for the policy.\n   *\n   * @param path path to probe\n   * @return the options to use with the put request\n   */\n  private PutObjectOptions putOptionsForPath(Path path) {\n    return keepDirectoryMarkers(path)\n        ? PutObjectOptions.keepingDirs()\n        : PutObjectOptions.deletingDirs();\n  }\n\n  /**\n   * Delete mock parent directories which are no longer needed.\n   * Retry policy: retrying; exceptions swallowed.\n   * @param path path\n   *\n   */\n  @Retries.RetryExceptionsSwallowed\n  private void deleteUnnecessaryFakeDirectories(Path path) {\n    List<DeleteObjectsRequest.KeyVersion> keysToRemove = new ArrayList<>();\n    while (!path.isRoot()) {\n      String key = pathToKey(path);\n      key = (key.endsWith(\"/\")) ? key : (key + \"/\");\n      LOG.trace(\"To delete unnecessary fake directory {} for {}\", key, path);\n      keysToRemove.add(new DeleteObjectsRequest.KeyVersion(key));\n      path = path.getParent();\n    }\n    try {\n      removeKeys(keysToRemove, true);\n    } catch(AmazonClientException | IOException e) {\n      instrumentation.errorIgnored();\n      if (LOG.isDebugEnabled()) {\n        StringBuilder sb = new StringBuilder();\n        for(DeleteObjectsRequest.KeyVersion kv : keysToRemove) {\n          sb.append(kv.getKey()).append(\",\");\n        }\n        LOG.debug(\"While deleting keys {} \", sb.toString(), e);\n      }\n    }\n  }\n\n  /**\n   * Create a fake directory, always ending in \"/\".\n   * Retry policy: retrying; translated.\n   * @param objectName name of directory object.\n   * @param putOptions put object options\n   * @throws IOException IO failure\n   */\n  @Retries.RetryTranslated\n  private void createFakeDirectory(final String objectName,\n      final PutObjectOptions putOptions)\n      throws IOException {\n    createEmptyObject(objectName, putOptions);\n  }\n\n  /**\n   * Used to create an empty file that represents an empty directory.\n   * The policy for deleting parent dirs depends on the path, dir\n   * status and the putOptions value.\n   * Retry policy: retrying; translated.\n   * @param objectName object to create\n   * @param putOptions put object options\n   * @throws IOException IO failure\n   */\n  @Retries.RetryTranslated\n  private void createEmptyObject(final String objectName, PutObjectOptions putOptions)\n      throws IOException {\n    invoker.retry(\"PUT 0-byte object \", objectName,\n         true, () ->\n            putObjectDirect(getRequestFactory().newDirectoryMarkerRequest(objectName),\n                putOptions,\n                getDurationTrackerFactory()));\n    incrementPutProgressStatistics(objectName, 0);\n    instrumentation.directoryCreated();\n  }\n\n  /**\n   * Return the number of bytes that large input files should be optimally\n   * be split into to minimize I/O time.\n   */\n  public long getDefaultBlockSize() {\n    return getConf().getLongBytes(FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE);\n  }\n\n  /**\n   * Get the directory marker policy of this filesystem.\n   * @return the marker policy.\n   */\n  public DirectoryPolicy getDirectoryMarkerPolicy() {\n    return directoryPolicy;\n  }\n\n  @Override\n  public String toString() {\n    final StringBuilder sb = new StringBuilder(\n        \"S3AFileSystem{\");\n    sb.append(\"uri=\").append(uri);\n    sb.append(\", workingDir=\").append(workingDir);\n    sb.append(\", partSize=\").append(partSize);\n    sb.append(\", enableMultiObjectsDelete=\").append(enableMultiObjectsDelete);\n    sb.append(\", maxKeys=\").append(maxKeys);\n    if (cannedACL != null) {\n      sb.append(\", cannedACL=\").append(cannedACL);\n    }\n    if (openFileHelper != null) {\n      sb.append(\", \").append(openFileHelper);\n    }\n    if (getConf() != null) {\n      sb.append(\", blockSize=\").append(getDefaultBlockSize());\n    }\n    sb.append(\", multiPartThreshold=\").append(multiPartThreshold);\n    if (getS3EncryptionAlgorithm() != null) {\n      sb.append(\", s3EncryptionAlgorithm='\")\n          .append(getS3EncryptionAlgorithm())\n          .append('\\'');\n    }\n    if (blockFactory != null) {\n      sb.append(\", blockFactory=\").append(blockFactory);\n    }\n    sb.append(\", auditManager=\").append(auditManager);\n    sb.append(\", authoritativePath=\").append(allowAuthoritativePaths);\n    sb.append(\", useListV1=\").append(useListV1);\n    if (committerIntegration != null) {\n      sb.append(\", magicCommitter=\").append(isMagicCommitEnabled());\n    }\n    sb.append(\", boundedExecutor=\").append(boundedThreadPool);\n    sb.append(\", unboundedExecutor=\").append(unboundedThreadPool);\n    sb.append(\", credentials=\").append(credentials);\n    sb.append(\", delegation tokens=\")\n        .append(delegationTokens.map(Objects::toString).orElse(\"disabled\"));\n    sb.append(\", \").append(directoryPolicy);\n    // if logging at debug, toString returns the entire IOStatistics set.\n    if (getInstrumentation() != null) {\n      sb.append(\", instrumentation {\")\n          .append(getInstrumentation().toString())\n          .append(\"}\");\n    }\n    sb.append(\", ClientSideEncryption=\").append(isCSEEnabled);\n\n    if (accessPoint != null) {\n      sb.append(\", arnForBucket=\").append(accessPoint.getFullArn());\n    }\n    sb.append('}');\n    return sb.toString();\n  }\n\n  /**\n   * Get the partition size for multipart operations.\n   * @return the value as set during initialization\n   */\n  public long getPartitionSize() {\n    return partSize;\n  }\n\n  /**\n   * Get the threshold for multipart files.\n   * @return the value as set during initialization\n   */\n  public long getMultiPartThreshold() {\n    return multiPartThreshold;\n  }\n\n  /**\n   * Get the maximum key count.\n   * @return a value, valid after initialization\n   */\n  int getMaxKeys() {\n    return maxKeys;\n  }\n\n  /**\n   * Is magic commit enabled?\n   * @return true if magic commit support is turned on.\n   */\n  public boolean isMagicCommitEnabled() {\n    return committerIntegration.isMagicCommitEnabled();\n  }\n\n  /**\n   * Predicate: is a path a magic commit path?\n   * True if magic commit is enabled and the path qualifies as special,\n   * and is not a a .pending or .pendingset file,\n   * @param path path to examine\n   * @return true if writing a file to the path triggers a \"magic\" write.\n   */\n  public boolean isMagicCommitPath(Path path) {\n    return committerIntegration.isMagicCommitPath(path);\n  }\n\n  /**\n   * Predicate: is a path under a magic commit path?\n   * True if magic commit is enabled and the path is under __magic,\n   * irrespective of file type.\n   * @param path path to examine\n   * @return true if the path is in a magic dir and the FS has magic writes enabled.\n   */\n  private boolean isUnderMagicCommitPath(Path path) {\n    return committerIntegration.isUnderMagicPath(path);\n  }\n\n  /**\n   * Increments the statistic {@link Statistic#INVOCATION_GLOB_STATUS}.\n   * Override superclass so as to disable symlink resolution as symlinks\n   * are not supported by S3A.\n   * {@inheritDoc}\n   */\n  @Override\n  public FileStatus[] globStatus(Path pathPattern) throws IOException {\n    return globStatus(pathPattern, ACCEPT_ALL);\n  }\n\n  /**\n   * Increments the statistic {@link Statistic#INVOCATION_GLOB_STATUS}.\n   * Override superclass so as to disable symlink resolution as symlinks\n   * are not supported by S3A.\n   *\n   * Although an AuditEntryPoint, the globber itself will talk do\n   * the filesystem through the filesystem API, so its operations will\n   * all appear part of separate operations.\n   * {@inheritDoc}\n   */\n  @Override\n  @AuditEntryPoint\n  public FileStatus[] globStatus(\n      final Path pathPattern,\n      final PathFilter filter)\n      throws IOException {\n    return trackDurationAndSpan(\n        INVOCATION_GLOB_STATUS, pathPattern, () ->\n            Globber.createGlobber(this)\n                .withPathPattern(pathPattern)\n                .withPathFiltern(filter)\n                .withResolveSymlinks(false)\n                .build()\n                .glob());\n  }\n\n  /**\n   * Override superclass so as to add statistic collection.\n   * {@inheritDoc}\n   */\n  @Override\n  @AuditEntryPoint\n  public boolean exists(Path f) throws IOException {\n    final Path path = qualify(f);\n    try {\n      trackDurationAndSpan(\n          INVOCATION_EXISTS, path, () ->\n              innerGetFileStatus(path, false, StatusProbeEnum.ALL));\n      return true;\n    } catch (FileNotFoundException e) {\n      return false;\n    }\n  }\n\n  /**\n   * Optimized probe for a path referencing a dir.\n   * Even though it is optimized to a single HEAD, applications\n   * should not over-use this method...it is all too common.\n   * {@inheritDoc}\n   */\n  @Override\n  @AuditEntryPoint\n  @SuppressWarnings(\"deprecation\")\n  public boolean isDirectory(Path f) throws IOException {\n    final Path path = qualify(f);\n    try {\n      return trackDurationAndSpan(\n          INVOCATION_IS_DIRECTORY, path, () ->\n              innerGetFileStatus(path, false, StatusProbeEnum.DIRECTORIES)\n                  .isDirectory());\n    } catch (FileNotFoundException e) {\n      // not found or it is a file.\n      return false;\n    }\n  }\n\n  /**\n   * Optimized probe for a path referencing a file.\n   * Even though it is optimized to a single HEAD, applications\n   * should not over-use this method...it is all too common.\n   * {@inheritDoc}\n   */\n  @Override\n  @AuditEntryPoint\n  @SuppressWarnings(\"deprecation\")\n  public boolean isFile(Path f) throws IOException {\n    final Path path = qualify(f);\n    try {\n      return trackDurationAndSpan(INVOCATION_IS_FILE, path, () ->\n          innerGetFileStatus(path, false, StatusProbeEnum.HEAD_ONLY)\n              .isFile());\n    } catch (FileNotFoundException e) {\n      // not found or it is a dir.\n      return false;\n    }\n  }\n\n  /**\n   * When enabled, get the etag of a object at the path via HEAD request and\n   * return it as a checksum object.\n   * <ol>\n   *   <li>If a tag has not changed, consider the object unchanged.</li>\n   *   <li>Two tags being different does not imply the data is different.</li>\n   * </ol>\n   * Different S3 implementations may offer different guarantees.\n   *\n   * This check is (currently) only made if\n   * {@link Constants#ETAG_CHECKSUM_ENABLED} is set; turning it on\n   * has caused problems with Distcp (HADOOP-15273).\n   *\n   * @param f The file path\n   * @param length The length of the file range for checksum calculation\n   * @return The EtagChecksum or null if checksums are not enabled or supported.\n   * @throws IOException IO failure\n   * @see <a href=\"http://docs.aws.amazon.com/AmazonS3/latest/API/RESTCommonResponseHeaders.html\">Common Response Headers</a>\n   */\n  @Override\n  @Retries.RetryTranslated\n  @AuditEntryPoint\n  public EtagChecksum getFileChecksum(Path f, final long length)\n      throws IOException {\n    Preconditions.checkArgument(length >= 0);\n    final Path path = qualify(f);\n    if (getConf().getBoolean(ETAG_CHECKSUM_ENABLED,\n        ETAG_CHECKSUM_ENABLED_DEFAULT)) {\n      return trackDurationAndSpan(INVOCATION_GET_FILE_CHECKSUM, path, () -> {\n        LOG.debug(\"getFileChecksum({})\", path);\n        ObjectMetadata headers = getObjectMetadata(path, null,\n            invoker,\n            \"getFileChecksum are\");\n        String eTag = headers.getETag();\n        return eTag != null ? new EtagChecksum(eTag) : null;\n      });\n    } else {\n      // disabled\n      return null;\n    }\n  }\n\n  /**\n   * Get header processing support.\n   * @return a new header processing instance.\n   */\n  private HeaderProcessing getHeaderProcessing() {\n    return new HeaderProcessing(createStoreContext(),\n        createHeaderProcessingCallbacks());\n  }\n\n  @Override\n  @AuditEntryPoint\n  public byte[] getXAttr(final Path path, final String name)\n      throws IOException {\n    checkNotClosed();\n    try (AuditSpan span = createSpan(\n        INVOCATION_XATTR_GET_NAMED.getSymbol(),\n        path.toString(), null)) {\n      return getHeaderProcessing().getXAttr(path, name);\n    }\n  }\n\n  @Override\n  @AuditEntryPoint\n  public Map<String, byte[]> getXAttrs(final Path path) throws IOException {\n    checkNotClosed();\n    try (AuditSpan span = createSpan(\n        INVOCATION_XATTR_GET_MAP.getSymbol(),\n        path.toString(), null)) {\n      return getHeaderProcessing().getXAttrs(path);\n    }\n  }\n\n  @Override\n  @AuditEntryPoint\n  public Map<String, byte[]> getXAttrs(final Path path,\n      final List<String> names)\n      throws IOException {\n    checkNotClosed();\n    try (AuditSpan span = createSpan(\n        INVOCATION_XATTR_GET_NAMED_MAP.getSymbol(),\n        path.toString(), null)) {\n      return getHeaderProcessing().getXAttrs(path, names);\n    }\n  }\n\n  @Override\n  @AuditEntryPoint\n  public List<String> listXAttrs(final Path path) throws IOException {\n    checkNotClosed();\n    try (AuditSpan span = createSpan(\n        INVOCATION_OP_XATTR_LIST.getSymbol(),\n        path.toString(), null)) {\n      return getHeaderProcessing().listXAttrs(path);\n    }\n  }\n\n  /**\n   * Create the callbacks.\n   * @return An implementation of the header processing\n   * callbacks.\n   */\n  protected HeaderProcessing.HeaderProcessingCallbacks\n      createHeaderProcessingCallbacks() {\n    return new HeaderProcessingCallbacksImpl();\n  }\n\n  /**\n   * Operations needed for Header Processing.\n   */\n  protected final class HeaderProcessingCallbacksImpl implements\n      HeaderProcessing.HeaderProcessingCallbacks {\n\n    @Override\n    public ObjectMetadata getObjectMetadata(final String key)\n        throws IOException {\n      return once(\"getObjectMetadata\", key, () ->\n          S3AFileSystem.this.getObjectMetadata(key));\n    }\n  }\n  /**\n   * {@inheritDoc}.\n   *\n   * This implementation is optimized for S3, which can do a bulk listing\n   * off all entries under a path in one single operation. Thus there is\n   * no need to recursively walk the directory tree.\n   *\n   * Instead a {@link ListObjectsRequest} is created requesting a (windowed)\n   * listing of all entries under the given path. This is used to construct\n   * an {@code ObjectListingIterator} instance, iteratively returning the\n   * sequence of lists of elements under the path. This is then iterated\n   * over in a {@code FileStatusListingIterator}, which generates\n   * {@link S3AFileStatus} instances, one per listing entry.\n   * These are then translated into {@link LocatedFileStatus} instances.\n   *\n   * This is essentially a nested and wrapped set of iterators, with some\n   * generator classes.\n   * @param f a path\n   * @param recursive if the subdirectories need to be traversed recursively\n   *\n   * @return an iterator that traverses statuses of the files/directories\n   *         in the given path\n   * @throws FileNotFoundException if {@code path} does not exist\n   * @throws IOException if any I/O error occurred\n   */\n  @Override\n  @Retries.RetryTranslated\n  @AuditEntryPoint\n  public RemoteIterator<LocatedFileStatus> listFiles(Path f,\n      boolean recursive) throws FileNotFoundException, IOException {\n    final Path path = qualify(f);\n    return toLocatedFileStatusIterator(\n        trackDurationAndSpan(INVOCATION_LIST_FILES, path, () ->\n            innerListFiles(path, recursive,\n                new Listing.AcceptFilesOnly(path), null)));\n  }\n\n  /**\n   * Recursive List of files and empty directories.\n   * @param f path to list from\n   * @param recursive recursive?\n   * @return an iterator.\n   * @throws IOException failure\n   */\n  @InterfaceAudience.Private\n  @Retries.RetryTranslated\n  @AuditEntryPoint\n  public RemoteIterator<S3ALocatedFileStatus> listFilesAndEmptyDirectories(\n      Path f, boolean recursive) throws IOException {\n    final Path path = qualify(f);\n    return trackDurationAndSpan(INVOCATION_LIST_FILES, path, () ->\n        innerListFiles(path, recursive,\n            Listing.ACCEPT_ALL_BUT_S3N,\n            null));\n  }\n\n  /**\n   * List files under the path.\n   * <ol>\n   *   <li>\n   *     The optional {@code status} parameter will be used to skip the\n   *     initial getFileStatus call.\n   *   </li>\n   * </ol>\n   *\n   * @param f path\n   * @param recursive recursive listing?\n   * @param acceptor file status filter\n   * @param status optional status of path to list.\n   * @return an iterator over the listing.\n   * @throws IOException failure\n   */\n  @Retries.RetryTranslated\n  private RemoteIterator<S3ALocatedFileStatus> innerListFiles(\n      final Path f,\n      final boolean recursive,\n      final Listing.FileStatusAcceptor acceptor,\n      final S3AFileStatus status) throws IOException {\n    Path path = qualify(f);\n    LOG.debug(\"listFiles({}, {})\", path, recursive);\n    try {\n      // if a status was given and it is a file.\n      if (status != null && status.isFile()) {\n        // simple case: File\n        LOG.debug(\"Path is a file: {}\", path);\n        return listing.createSingleStatusIterator(\n            toLocatedFileStatus(status));\n      }\n      // Assuming the path to be a directory\n      // do a bulk operation.\n      RemoteIterator<S3ALocatedFileStatus> listFilesAssumingDir =\n              listing.getListFilesAssumingDir(path,\n                      recursive,\n                      acceptor,\n                  getActiveAuditSpan());\n      // If there are no list entries present, we\n      // fallback to file existence check as the path\n      // can be a file or empty directory.\n      if (!listFilesAssumingDir.hasNext()) {\n        // If file status was already passed, reuse it.\n        final S3AFileStatus fileStatus = status != null\n                ? status\n                : innerGetFileStatus(path, false, StatusProbeEnum.ALL);\n        if (fileStatus.isFile()) {\n          return listing.createSingleStatusIterator(\n                  toLocatedFileStatus(fileStatus));\n        }\n      }\n      // If we have reached here, it means either there are files\n      // in this directory or it is empty.\n      return listFilesAssumingDir;\n    } catch (AmazonClientException e) {\n      throw translateException(\"listFiles\", path, e);\n    }\n  }\n\n  /**\n   * Override superclass so as to add statistic collection.\n   * {@inheritDoc}\n   */\n  @Override\n  public RemoteIterator<LocatedFileStatus> listLocatedStatus(Path f)\n      throws FileNotFoundException, IOException {\n    return listLocatedStatus(f, ACCEPT_ALL);\n  }\n\n  /**\n   * {@inheritDoc}.\n   *\n   * S3 Optimized directory listing. The initial operation performs the\n   * first bulk listing; extra listings will take place\n   * when all the current set of results are used up.\n   * @param f a path\n   * @param filter a path filter\n   * @return an iterator that traverses statuses of the files/directories\n   *         in the given path\n   * @throws FileNotFoundException if {@code path} does not exist\n   * @throws IOException if any I/O error occurred\n   */\n  @Override\n  @Retries.OnceTranslated(\"s3guard not retrying\")\n  @AuditEntryPoint\n  public RemoteIterator<LocatedFileStatus> listLocatedStatus(final Path f,\n      final PathFilter filter)\n      throws FileNotFoundException, IOException {\n    Path path = qualify(f);\n    AuditSpan span = entryPoint(INVOCATION_LIST_LOCATED_STATUS, path);\n    LOG.debug(\"listLocatedStatus({}, {}\", path, filter);\n    RemoteIterator<? extends LocatedFileStatus> iterator =\n        once(\"listLocatedStatus\", path.toString(),\n          () -> {\n            // Assuming the path to be a directory,\n            // trigger a list call directly.\n            final RemoteIterator<S3ALocatedFileStatus>\n                    locatedFileStatusIteratorForDir =\n                    listing.getLocatedFileStatusIteratorForDir(path, filter,\n                        span);\n\n            // If no listing is present then path might be a file.\n            if (!locatedFileStatusIteratorForDir.hasNext()) {\n              final S3AFileStatus fileStatus =\n                  innerGetFileStatus(path, false, StatusProbeEnum.ALL);\n              if (fileStatus.isFile()) {\n                // simple case: File\n                LOG.debug(\"Path is a file\");\n                return listing.createSingleStatusIterator(\n                        filter.accept(path)\n                                ? toLocatedFileStatus(fileStatus)\n                                : null);\n              }\n            }\n            // Either empty or non-empty directory.\n            return locatedFileStatusIteratorForDir;\n          });\n    return toLocatedFileStatusIterator(iterator);\n  }\n\n  /**\n   * Build a {@link S3ALocatedFileStatus} from a {@link FileStatus} instance.\n   * @param status file status\n   * @return a located status with block locations set up from this FS.\n   * @throws IOException IO Problems.\n   */\n  S3ALocatedFileStatus toLocatedFileStatus(S3AFileStatus status)\n      throws IOException {\n    return new S3ALocatedFileStatus(status,\n        status.isFile() ?\n          getFileBlockLocations(status, 0, status.getLen())\n          : null);\n  }\n\n  /**\n   * List any pending multipart uploads whose keys begin with prefix, using\n   * an iterator that can handle an unlimited number of entries.\n   * See {@link #listMultipartUploads(String)} for a non-iterator version of\n   * this.\n   *\n   * @param prefix optional key prefix to search\n   * @return Iterator over multipart uploads.\n   * @throws IOException on failure\n   */\n  @InterfaceAudience.Private\n  @Retries.RetryTranslated\n  @AuditEntryPoint\n  public MultipartUtils.UploadIterator listUploads(@Nullable String prefix)\n      throws IOException {\n    // span is picked up retained in the listing.\n    return trackDurationAndSpan(MULTIPART_UPLOAD_LIST, prefix, null, () ->\n        MultipartUtils.listMultipartUploads(\n            createStoreContext(),\n            s3, prefix, maxKeys\n        ));\n  }\n\n  /**\n   * Listing all multipart uploads; limited to the first few hundred.\n   * See {@link #listUploads(String)} for an iterator-based version that does\n   * not limit the number of entries returned.\n   * Retry policy: retry, translated.\n   * @return a listing of multipart uploads.\n   * @param prefix prefix to scan for, \"\" for none\n   * @throws IOException IO failure, including any uprated AmazonClientException\n   */\n  @InterfaceAudience.Private\n  @Retries.RetryTranslated\n  public List<MultipartUpload> listMultipartUploads(String prefix)\n      throws IOException {\n    // add a trailing / if needed.\n    if (prefix != null && !prefix.isEmpty() && !prefix.endsWith(\"/\")) {\n      prefix = prefix + \"/\";\n    }\n    String p = prefix;\n    return invoker.retry(\"listMultipartUploads\", p, true, () -> {\n      ListMultipartUploadsRequest request = getRequestFactory()\n          .newListMultipartUploadsRequest(p);\n      return s3.listMultipartUploads(request).getMultipartUploads();\n    });\n  }\n\n  /**\n   * Abort a multipart upload.\n   * Retry policy: none.\n   * @param destKey destination key\n   * @param uploadId Upload ID\n   */\n  @Retries.OnceRaw\n  void abortMultipartUpload(String destKey, String uploadId) {\n    LOG.info(\"Aborting multipart upload {} to {}\", uploadId, destKey);\n    getAmazonS3Client().abortMultipartUpload(\n        getRequestFactory().newAbortMultipartUploadRequest(\n            destKey,\n            uploadId));\n  }\n\n  /**\n   * Abort a multipart upload.\n   * Retry policy: none.\n   * @param upload the listed upload to abort.\n   */\n  @Retries.OnceRaw\n  void abortMultipartUpload(MultipartUpload upload) {\n    String destKey;\n    String uploadId;\n    destKey = upload.getKey();\n    uploadId = upload.getUploadId();\n    if (LOG.isInfoEnabled()) {\n      DateFormat df = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n      LOG.debug(\"Aborting multipart upload {} to {} initiated by {} on {}\",\n          uploadId, destKey, upload.getInitiator(),\n          df.format(upload.getInitiated()));\n    }\n    getAmazonS3Client().abortMultipartUpload(\n        getRequestFactory().newAbortMultipartUploadRequest(\n            destKey,\n            uploadId));\n  }\n\n  /**\n   * Create a new instance of the committer statistics.\n   * @return a new committer statistics instance\n   */\n  public CommitterStatistics newCommitterStatistics() {\n    return statisticsContext.newCommitterStatistics();\n  }\n\n  @SuppressWarnings(\"deprecation\")\n  @Override\n  public boolean hasPathCapability(final Path path, final String capability)\n      throws IOException {\n    final Path p = makeQualified(path);\n    String cap = validatePathCapabilityArgs(p, capability);\n    switch (cap) {\n\n    case CommitConstants.STORE_CAPABILITY_MAGIC_COMMITTER:\n    case CommitConstants.STORE_CAPABILITY_MAGIC_COMMITTER_OLD:\n      // capability depends on FS configuration\n      return isMagicCommitEnabled();\n\n    case SelectConstants.S3_SELECT_CAPABILITY:\n      // select is only supported if enabled and client side encryption is\n      // disabled.\n      return !isCSEEnabled && SelectBinding.isSelectEnabled(getConf());\n\n    case CommonPathCapabilities.FS_CHECKSUMS:\n      // capability depends on FS configuration\n      return getConf().getBoolean(ETAG_CHECKSUM_ENABLED,\n          ETAG_CHECKSUM_ENABLED_DEFAULT);\n\n    case CommonPathCapabilities.ABORTABLE_STREAM:\n      return true;\n    case CommonPathCapabilities.FS_MULTIPART_UPLOADER:\n      // client side encryption doesn't support multipart uploader.\n      return !isCSEEnabled;\n\n    // this client is safe to use with buckets\n    // containing directory markers anywhere in\n    // the hierarchy\n    case STORE_CAPABILITY_DIRECTORY_MARKER_AWARE:\n      return true;\n\n    // etags are avaialable in listings, but they\n    // are not consistent across renames.\n    // therefore, only availability is declared\n    case CommonPathCapabilities.ETAGS_AVAILABLE:\n      return true;\n\n      /*\n     * Marker policy capabilities are handed off.\n     */\n    case STORE_CAPABILITY_DIRECTORY_MARKER_POLICY_KEEP:\n    case STORE_CAPABILITY_DIRECTORY_MARKER_POLICY_DELETE:\n    case STORE_CAPABILITY_DIRECTORY_MARKER_POLICY_AUTHORITATIVE:\n      return getDirectoryMarkerPolicy().hasPathCapability(path, cap);\n\n     // keep for a magic path or if the policy retains it\n    case STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_KEEP:\n      return keepDirectoryMarkers(path);\n    // delete is the opposite of keep\n    case STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_DELETE:\n      return !keepDirectoryMarkers(path);\n\n    case STORE_CAPABILITY_DIRECTORY_MARKER_MULTIPART_UPLOAD_ENABLED:\n      return isMultipartUploadEnabled();\n\n    // create file options\n    case FS_S3A_CREATE_PERFORMANCE:\n    case FS_S3A_CREATE_HEADER:\n      return true;\n\n    default:\n      return super.hasPathCapability(p, cap);\n    }\n  }\n\n  /**\n   * Return the capabilities of this filesystem instance.\n   *\n   * This has been supplanted by {@link #hasPathCapability(Path, String)}.\n   * @param capability string to query the stream support for.\n   * @return whether the FS instance has the capability.\n   */\n  @Deprecated\n  @Override\n  public boolean hasCapability(String capability) {\n    try {\n      return hasPathCapability(new Path(\"/\"), capability);\n    } catch (IOException ex) {\n      // should never happen, so log and downgrade.\n      LOG.debug(\"Ignoring exception on hasCapability({}})\", capability, ex);\n      return false;\n    }\n  }\n\n  /**\n   * Get a shared copy of the AWS credentials, with its reference\n   * counter updated.\n   * Caller is required to call {@code close()} on this after\n   * they have finished using it.\n   * @param purpose what is this for? This is initially for logging\n   * @return a reference to shared credentials.\n   */\n  public AWSCredentialProviderList shareCredentials(final String purpose) {\n    LOG.debug(\"Sharing credentials for: {}\", purpose);\n    return credentials.share();\n  }\n\n  /**\n   * This is a proof of concept of a select API.\n   * @param source path to source data\n   * @param options request configuration from the builder.\n   * @param fileInformation any passed in information.\n   * @return the stream of the results\n   * @throws IOException IO failure\n   */\n  @Retries.RetryTranslated\n  @AuditEntryPoint\n  private FSDataInputStream select(final Path source,\n      final Configuration options,\n      final OpenFileSupport.OpenFileInformation fileInformation)\n      throws IOException {\n    requireSelectSupport(source);\n    final AuditSpan auditSpan = entryPoint(OBJECT_SELECT_REQUESTS, source);\n    final Path path = makeQualified(source);\n    String expression = fileInformation.getSql();\n    final S3AFileStatus fileStatus = extractOrFetchSimpleFileStatus(path,\n        fileInformation);\n\n    // readahead range can be dynamically set\n    S3ObjectAttributes objectAttributes = createObjectAttributes(\n        path, fileStatus);\n    ChangeDetectionPolicy changePolicy = fileInformation.getChangePolicy();\n    S3AReadOpContext readContext = createReadContext(\n        fileStatus,\n        auditSpan);\n    fileInformation.applyOptions(readContext);\n\n    if (changePolicy.getSource() != ChangeDetectionPolicy.Source.None\n        && fileStatus.getEtag() != null) {\n      // if there is change detection, and the status includes at least an\n      // etag,\n      // check that the object metadata lines up with what is expected\n      // based on the object attributes (which may contain an eTag or\n      // versionId).\n      // This is because the select API doesn't offer this.\n      // (note: this is trouble for version checking as cannot force the old\n      // version in the final read; nor can we check the etag match)\n      ChangeTracker changeTracker =\n          new ChangeTracker(uri.toString(),\n              changePolicy,\n              readContext.getS3AStatisticsContext()\n                  .newInputStreamStatistics()\n                  .getChangeTrackerStatistics(),\n              objectAttributes);\n\n      // will retry internally if wrong version detected\n      Invoker readInvoker = readContext.getReadInvoker();\n      getObjectMetadata(path, changeTracker, readInvoker, \"select\");\n    }\n    // instantiate S3 Select support using the current span\n    // as the active span for operations.\n    SelectBinding selectBinding = new SelectBinding(\n        createWriteOperationHelper(auditSpan));\n\n    // build and execute the request\n    return selectBinding.select(\n        readContext,\n        expression,\n        options,\n        objectAttributes);\n  }\n\n  /**\n   * Verify the FS supports S3 Select.\n   * @param source source file.\n   * @throws UnsupportedOperationException if not.\n   */\n  private void requireSelectSupport(final Path source) throws\n      UnsupportedOperationException {\n    if (!isCSEEnabled && !SelectBinding.isSelectEnabled(getConf())) {\n\n      throw new UnsupportedOperationException(\n          SelectConstants.SELECT_UNSUPPORTED);\n    }\n  }\n\n  /**\n   * Get the file status of the source file.\n   * If in the fileInformation parameter return that\n   * if not found, issue a HEAD request, looking for a\n   * file only.\n   * @param path path of the file to open\n   * @param fileInformation information on the file to open\n   * @return a file status\n   * @throws FileNotFoundException if a HEAD request found no file\n   * @throws IOException IO failure\n   */\n  private S3AFileStatus extractOrFetchSimpleFileStatus(\n      final Path path,\n      final OpenFileSupport.OpenFileInformation fileInformation)\n      throws IOException {\n    S3AFileStatus fileStatus = fileInformation.getStatus();\n    if (fileStatus == null) {\n      // we check here for the passed in status\n      // being a directory\n      fileStatus = innerGetFileStatus(path, false,\n          StatusProbeEnum.HEAD_ONLY);\n    }\n    if (fileStatus.isDirectory()) {\n      throw new FileNotFoundException(path.toString() + \" is a directory\");\n    }\n\n    return fileStatus;\n  }\n\n  /**\n   * Initiate the open() or select() operation.\n   * This is invoked from both the FileSystem and FileContext APIs.\n   * It's declared as an audit entry point but the span creation is pushed\n   * down into the open/select methods it ultimately calls.\n   * @param rawPath path to the file\n   * @param parameters open file parameters from the builder.\n   * @return a future which will evaluate to the opened/selected file.\n   * @throws IOException failure to resolve the link.\n   * @throws PathIOException operation is a select request but S3 select is\n   * disabled\n   * @throws IllegalArgumentException unknown mandatory key\n   */\n  @Override\n  @Retries.RetryTranslated\n  @AuditEntryPoint\n  public CompletableFuture<FSDataInputStream> openFileWithOptions(\n      final Path rawPath,\n      final OpenFileParameters parameters) throws IOException {\n    final Path path = qualify(rawPath);\n    OpenFileSupport.OpenFileInformation fileInformation =\n        openFileHelper.prepareToOpenFile(\n            path,\n            parameters,\n            getDefaultBlockSize());\n    CompletableFuture<FSDataInputStream> result = new CompletableFuture<>();\n    if (!fileInformation.isS3Select()) {\n      // normal path.\n      unboundedThreadPool.submit(() ->\n          LambdaUtils.eval(result,\n              () -> executeOpen(path, fileInformation)));\n    } else {\n      // it is a select statement.\n      // fail fast if the operation is not available\n      requireSelectSupport(path);\n      // submit the query\n      unboundedThreadPool.submit(() ->\n          LambdaUtils.eval(result,\n              () -> select(path, parameters.getOptions(), fileInformation)));\n    }\n    return result;\n  }\n\n  @Override\n  @AuditEntryPoint\n  public S3AMultipartUploaderBuilder createMultipartUploader(\n      final Path basePath)\n      throws IOException {\n    if(isCSEEnabled) {\n      throw new UnsupportedOperationException(\"Multi-part uploader not \"\n          + \"supported for Client side encryption.\");\n    }\n    final Path path = makeQualified(basePath);\n    try (AuditSpan span = entryPoint(MULTIPART_UPLOAD_INSTANTIATED, path)) {\n      StoreContext ctx = createStoreContext();\n      return new S3AMultipartUploaderBuilder(this,\n          createWriteOperationHelper(span),\n          ctx,\n          path,\n          statisticsContext.createMultipartUploaderStatistics());\n    }\n  }\n\n  /**\n   * Build an immutable store context.\n   * If called while the FS is being initialized,\n   * some of the context will be incomplete.\n   * new store context instances should be created as appropriate.\n   * @return the store context of this FS.\n   */\n  @InterfaceAudience.Private\n  public StoreContext createStoreContext() {\n    return new StoreContextBuilder().setFsURI(getUri())\n        .setBucket(getBucket())\n        .setConfiguration(getConf())\n        .setUsername(getUsername())\n        .setOwner(owner)\n        .setExecutor(boundedThreadPool)\n        .setExecutorCapacity(executorCapacity)\n        .setInvoker(invoker)\n        .setInstrumentation(statisticsContext)\n        .setStorageStatistics(getStorageStatistics())\n        .setInputPolicy(getInputPolicy())\n        .setChangeDetectionPolicy(changeDetectionPolicy)\n        .setMultiObjectDeleteEnabled(enableMultiObjectsDelete)\n        .setUseListV1(useListV1)\n        .setContextAccessors(new ContextAccessorsImpl())\n        .setAuditor(getAuditor())\n        .setEnableCSE(isCSEEnabled)\n        .build();\n  }\n\n  /**\n   * Create a marker tools operations binding for this store.\n   * Auditing:\n   * @param target target path\n   * @return callbacks for operations.\n   * @throws IOException if raised during span creation\n   */\n  @AuditEntryPoint\n  @InterfaceAudience.Private\n  public MarkerToolOperations createMarkerToolOperations(final String target)\n      throws IOException {\n    createSpan(\"marker-tool-scan\", target,\n        null);\n    return new MarkerToolOperationsImpl(new OperationCallbacksImpl());\n  }\n\n  /**\n   * This is purely for testing, as it force initializes all static\n   * initializers. See HADOOP-17385 for details.\n   */\n  @InterfaceAudience.Private\n  public static void initializeClass() {\n    LOG.debug(\"Initialize S3A class\");\n  }\n\n  /**\n   * The implementation of context accessors.\n   */\n  private class ContextAccessorsImpl implements ContextAccessors {\n\n    @Override\n    public Path keyToPath(final String key) {\n      return keyToQualifiedPath(key);\n    }\n\n    @Override\n    public String pathToKey(final Path path) {\n      return S3AFileSystem.this.pathToKey(path);\n    }\n\n    @Override\n    public File createTempFile(final String prefix, final long size)\n        throws IOException {\n      return createTmpFileForWrite(prefix, size, getConf());\n    }\n\n    @Override\n    public String getBucketLocation() throws IOException {\n      return S3AFileSystem.this.getBucketLocation();\n    }\n\n    @Override\n    public Path makeQualified(final Path path) {\n      return S3AFileSystem.this.makeQualified(path);\n    }\n\n    @Override\n    public AuditSpan getActiveAuditSpan() {\n      return S3AFileSystem.this.getActiveAuditSpan();\n    }\n\n    @Override\n    public RequestFactory getRequestFactory() {\n      return S3AFileSystem.this.getRequestFactory();\n    }\n  }\n\n  /**\n   * a method to know if Client side encryption is enabled or not.\n   * @return a boolean stating if CSE is enabled.\n   */\n  public boolean isCSEEnabled() {\n    return isCSEEnabled;\n  }\n\n  public boolean isMultipartUploadEnabled() {\n    return isMultipartUploadEnabled;\n  }\n}\n"
    },
    {
      "file_path": "D:\\Disaster\\Codefield\\Code_Python\\Anti-patternRAG\\data\\AWD\\apache\\hadoop\\commit_1200\\1951\\before\\hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/statistics/DurationTrackerFactory.java",
      "chunk_type": "superType",
      "ast_subtree": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.hadoop.fs.statistics;\n\nimport static org.apache.hadoop.fs.statistics.IOStatisticsSupport.stubDurationTracker;\n\n/**\n * Interface for a source of duration tracking.\n *\n * This is intended for uses where it can be passed into classes\n * which update operation durations, without tying those\n * classes to internal implementation details.\n */\npublic interface DurationTrackerFactory {\n\n  /**\n   * Initiate a duration tracking operation by creating/returning\n   * an object whose {@code close()} call will\n   * update the statistics.\n   *\n   * The statistics counter with the key name will be incremented\n   * by the given count.\n   *\n   * The expected use is within a try-with-resources clause.\n   *\n   * The default implementation returns a stub duration tracker.\n   * @param key statistic key prefix\n   * @param count  #of times to increment the matching counter in this\n   * operation.\n   * @return an object to close after an operation completes.\n   */\n  default DurationTracker trackDuration(String key, long count) {\n    return stubDurationTracker();\n  }\n\n  /**\n   * Initiate a duration tracking operation by creating/returning\n   * an object whose {@code close()} call will\n   * update the statistics.\n   * The expected use is within a try-with-resources clause.\n   * @param key statistic key\n   * @return an object to close after an operation completes.\n   */\n  default DurationTracker trackDuration(String key) {\n    return trackDuration(key, 1);\n  }\n}\n"
    },
    {
      "file_path": "D:\\Disaster\\Codefield\\Code_Python\\Anti-patternRAG\\data\\AWD\\apache\\hadoop\\commit_1200\\1951\\before\\hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStore.java",
      "chunk_type": "subType",
      "ast_subtree": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.hadoop.fs.statistics.impl;\n\nimport java.time.Duration;\nimport java.util.concurrent.atomic.AtomicLong;\n\nimport org.apache.hadoop.fs.statistics.IOStatistics;\nimport org.apache.hadoop.fs.statistics.IOStatisticsAggregator;\nimport org.apache.hadoop.fs.statistics.DurationTrackerFactory;\nimport org.apache.hadoop.fs.statistics.MeanStatistic;\n\n/**\n * Interface of an IOStatistics store intended for\n * use in classes which track statistics for reporting.\n */\npublic interface IOStatisticsStore extends IOStatistics,\n    IOStatisticsAggregator,\n    DurationTrackerFactory {\n\n  /**\n   * Increment a counter by one.\n   *\n   * No-op if the counter is unknown.\n   * @param key statistics key\n   * @return old value or, if the counter is unknown: 0\n   */\n  default long incrementCounter(String key) {\n    return incrementCounter(key, 1);\n  }\n\n  /**\n   * Increment a counter.\n   *\n   * No-op if the counter is unknown.\n   * If the value is negative, it is ignored.\n   * @param key statistics key\n   * @param value value to increment\n   * @return the updated value or, if the counter is unknown: 0\n   */\n  long incrementCounter(String key, long value);\n\n  /**\n   * Set a counter.\n   *\n   * No-op if the counter is unknown.\n   * @param key statistics key\n   * @param value value to set\n   */\n  void setCounter(String key, long value);\n\n  /**\n   * Set a gauge.\n   *\n   * No-op if the gauge is unknown.\n   * @param key statistics key\n   * @param value value to set\n   */\n  void setGauge(String key, long value);\n\n  /**\n   * Increment a gauge.\n   * <p>\n   * No-op if the gauge is unknown.\n   * </p>\n   * @param key statistics key\n   * @param value value to increment\n   * @return new value or 0 if the key is unknown\n   */\n  long incrementGauge(String key, long value);\n\n  /**\n   * Set a maximum.\n   * No-op if the maximum is unknown.\n   * @param key statistics key\n   * @param value value to set\n   */\n  void setMaximum(String key, long value);\n\n  /**\n   * Increment a maximum.\n   * <p>\n   * No-op if the maximum is unknown.\n   * </p>\n   * @param key statistics key\n   * @param value value to increment\n   * @return new value or 0 if the key is unknown\n   */\n  long incrementMaximum(String key, long value);\n\n  /**\n   * Set a minimum.\n   * <p>\n   * No-op if the minimum is unknown.\n   * </p>\n   * @param key statistics key\n   * @param value value to set\n   */\n  void setMinimum(String key, long value);\n\n  /**\n   * Increment a minimum.\n   * <p>\n   * No-op if the minimum is unknown.\n   * </p>\n   * @param key statistics key\n   * @param value value to increment\n   * @return new value or 0 if the key is unknown\n   */\n  long incrementMinimum(String key, long value);\n\n  /**\n   * Add a minimum sample: if less than the current value,\n   * updates the value.\n   * <p>\n   * No-op if the minimum is unknown.\n   * </p>\n   * @param key statistics key\n   * @param value sample value\n   */\n  void addMinimumSample(String key, long value);\n\n  /**\n   * Add a maximum sample: if greater than the current value,\n   * updates the value.\n   * <p>\n   * No-op if the key is unknown.\n   * </p>\n   * @param key statistics key\n   * @param value sample value\n   */\n  void addMaximumSample(String key, long value);\n\n  /**\n   * Set a mean statistic to a given value.\n   * <p>\n   * No-op if the key is unknown.\n   * </p>\n   * @param key statistic key\n   * @param value new value.\n   */\n  void setMeanStatistic(String key, MeanStatistic value);\n\n  /**\n   * Add a sample to the mean statistics.\n   * <p>\n   * No-op if the key is unknown.\n   * </p>\n   * @param key key\n   * @param value sample value.\n   */\n  void addMeanStatisticSample(String key, long value);\n\n  /**\n   * Reset all statistics.\n   * Unsynchronized.\n   */\n  void reset();\n\n  /**\n   * Get a reference to the atomic instance providing the\n   * value for a specific counter. This is useful if\n   * the value is passed around.\n   * @param key statistic name\n   * @return the reference\n   * @throws NullPointerException if there is no entry of that name\n   */\n  AtomicLong getCounterReference(String key);\n\n  /**\n   * Get a reference to the atomic instance providing the\n   * value for a specific maximum. This is useful if\n   * the value is passed around.\n   * @param key statistic name\n   * @return the reference\n   * @throws NullPointerException if there is no entry of that name\n   */\n  AtomicLong getMaximumReference(String key);\n\n  /**\n   * Get a reference to the atomic instance providing the\n   * value for a specific minimum. This is useful if\n   * the value is passed around.\n   * @param key statistic name\n   * @return the reference\n   * @throws NullPointerException if there is no entry of that name\n   */\n  AtomicLong getMinimumReference(String key);\n\n  /**\n   * Get a reference to the atomic instance providing the\n   * value for a specific gauge. This is useful if\n   * the value is passed around.\n   * @param key statistic name\n   * @return the reference\n   * @throws NullPointerException if there is no entry of that name\n   */\n  AtomicLong getGaugeReference(String key);\n\n  /**\n   * Get a reference to the atomic instance providing the\n   * value for a specific meanStatistic. This is useful if\n   * the value is passed around.\n   * @param key statistic name\n   * @return the reference\n   * @throws NullPointerException if there is no entry of that name\n   */\n  MeanStatistic getMeanStatistic(String key);\n\n  /**\n   * Add a duration to the min/mean/max statistics, using the\n   * given prefix and adding a suffix for each specific value.\n   *\n   * The update is not-atomic, even though each individual statistic\n   * is updated thread-safely. If two threads update the values\n   * simultaneously, at the end of each operation the state will\n   * be correct. It is only during the sequence that the statistics\n   * may be observably inconsistent.\n   * @param prefix statistic prefix\n   * @param durationMillis duration in milliseconds.\n   */\n  void addTimedOperation(String prefix, long durationMillis);\n\n  /**\n   * Add a duration to the min/mean/max statistics, using the\n   * given prefix and adding a suffix for each specific value.;\n   * increment tha counter whose name == prefix.\n   *\n   * If any of the statistics are not registered, that part of\n   * the sequence will be omitted -the rest will proceed.\n   *\n   * The update is not-atomic, even though each individual statistic\n   * is updated thread-safely. If two threads update the values\n   * simultaneously, at the end of each operation the state will\n   * be correct. It is only during the sequence that the statistics\n   * may be observably inconsistent.\n   * @param prefix statistic prefix\n   * @param duration duration\n   */\n  void addTimedOperation(String prefix, Duration duration);\n\n  /**\n   * Add a statistics sample as a min, max and mean and count.\n   * @param key key to add.\n   * @param count count.\n   */\n  default void addSample(String key, long count) {\n    incrementCounter(key, count);\n    addMeanStatisticSample(key, count);\n    addMaximumSample(key, count);\n    addMinimumSample(key, count);\n  }\n}\n"
    }
  ]
}