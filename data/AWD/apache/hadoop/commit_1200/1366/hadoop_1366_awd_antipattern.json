{
  "clientClass": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport",
  "superType": "org.apache.hadoop.fs.FileSystem",
  "subType": "org.apache.hadoop.hdfs.DistributedFileSystem",
  "files": [
    "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java",
    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java",
    "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java"
  ],
  "details": {
    "clientClass2superType": {
      "fromFile": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java",
      "toFile": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java",
      "snippets": [
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithRenameToNewDir",
            "location": "952–972",
            "code": "@Test\n  public void testDiffReportWithRenameToNewDir() throws Exception {\n    final Path root = new Path(\"/\");\n    final Path foo = new Path(root, \"foo\");\n    final Path fileInFoo = new Path(foo, \"file\");\n    DFSTestUtil.createFile(hdfs, fileInFoo, BLOCKSIZE, REPLICATION, SEED);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    final Path bar = new Path(root, \"bar\");\n    hdfs.mkdirs(bar);\n    final Path fileInBar = new Path(bar, \"file\");\n    hdfs.rename(fileInFoo, fileInBar);\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n\n    verifyDiffReport(root, \"s0\", \"s1\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"foo\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"bar\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"foo/file\"),\n            DFSUtil.string2Bytes(\"bar/file\")));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.fs.FileSystem.mkdirs",
            "location": "2487–2495",
            "code": "/**\n   * Call {@link #mkdirs(Path, FsPermission)} with default permission.\n   * @param f path\n   * @return true if the directory was created\n   * @throws IOException IO failure\n   */\n  public boolean mkdirs(Path f) throws IOException {\n    return mkdirs(f, FsPermission.getDirDefault());\n  }"
          },
          "invocation": {
            "location": "961–961",
            "code": "hdfs.mkdirs(bar);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReport2",
            "location": "779–815",
            "code": "/**\n   * Make changes under a sub-directory, then delete the sub-directory. Make\n   * sure the diff report computation correctly retrieve the diff from the\n   * deleted sub-directory.\n   */\n  @Test (timeout=60000)\n  public void testDiffReport2() throws Exception {\n    Path subsub1 = new Path(sub1, \"subsub1\");\n    Path subsubsub1 = new Path(subsub1, \"subsubsub1\");\n    hdfs.mkdirs(subsubsub1);\n    modifyAndCreateSnapshot(subsubsub1, new Path[]{sub1});\n    \n    // delete subsub1\n    hdfs.delete(subsub1, true);\n    // check diff report between s0 and s2\n    verifyDiffReport(sub1, \"s0\", \"s2\", \n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1\")), \n        new DiffReportEntry(DiffType.CREATE, \n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file15\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file12\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/link13\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/link13\")));\n    // check diff report between s0 and the current status\n    verifyDiffReport(sub1, \"s0\", \"\", \n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"subsub1\")));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.fs.FileSystem.mkdirs",
            "location": "2487–2495",
            "code": "/**\n   * Call {@link #mkdirs(Path, FsPermission)} with default permission.\n   * @param f path\n   * @return true if the directory was created\n   * @throws IOException IO failure\n   */\n  public boolean mkdirs(Path f) throws IOException {\n    return mkdirs(f, FsPermission.getDirDefault());\n  }"
          },
          "invocation": {
            "location": "788–788",
            "code": "hdfs.mkdirs(subsubsub1);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testSnapRootDescendantDiffReport",
            "location": "318–378",
            "code": "@Test(timeout = 60000)\n  public void testSnapRootDescendantDiffReport() throws Exception {\n    Assume.assumeTrue(conf.getBoolean(\n        DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_DIFF_ALLOW_SNAP_ROOT_DESCENDANT,\n        DFSConfigKeys.\n            DFS_NAMENODE_SNAPSHOT_DIFF_ALLOW_SNAP_ROOT_DESCENDANT_DEFAULT));\n    Path subSub = new Path(sub1, \"subsub1\");\n    Path subSubSub = new Path(subSub, \"subsubsub1\");\n    Path nonSnapDir = new Path(dir, \"non_snap\");\n    hdfs.mkdirs(subSubSub);\n    hdfs.mkdirs(nonSnapDir);\n\n    modifyAndCreateSnapshot(sub1, new Path[]{sub1});\n    modifyAndCreateSnapshot(subSub, new Path[]{sub1});\n    modifyAndCreateSnapshot(subSubSub, new Path[]{sub1});\n\n    try {\n      hdfs.getSnapshotDiffReport(subSub, \"s1\", \"s2\");\n      hdfs.getSnapshotDiffReport(subSubSub, \"s1\", \"s2\");\n    } catch (IOException e) {\n      fail(\"Unexpected exception when getting snapshot diff report \" +\n          subSub + \": \" + e);\n    }\n\n    try {\n      hdfs.getSnapshotDiffReport(nonSnapDir, \"s1\", \"s2\");\n      fail(\"Snapshot diff report on a non snapshot directory '\"\n          + nonSnapDir.getName() + \"'should fail!\");\n    } catch (SnapshotException e) {\n      GenericTestUtils.assertExceptionContains(\n          \"The path \" + nonSnapDir +\n              \" is neither snapshottable nor under a snapshot root!\", e);\n    }\n\n    final String invalidName = \"invalid\";\n    try {\n      hdfs.getSnapshotDiffReport(subSub, invalidName, invalidName);\n      fail(\"Expect exception when providing invalid snapshot name \" +\n          \"for diff report\");\n    } catch (IOException e) {\n      GenericTestUtils.assertExceptionContains(\n          \"Cannot find the snapshot of directory \" + sub1 + \" with name \"\n              + invalidName, e);\n    }\n\n    // diff between the same snapshot\n    SnapshotDiffReport report = hdfs.getSnapshotDiffReport(subSub, \"s0\", \"s0\");\n    assertEquals(0, report.getDiffList().size());\n\n    report = hdfs.getSnapshotDiffReport(subSub, \"\", \"\");\n    assertEquals(0, report.getDiffList().size());\n\n    report = hdfs.getSnapshotDiffReport(subSubSub, \"s0\", \"s2\");\n    assertEquals(0, report.getDiffList().size());\n\n    report = hdfs.getSnapshotDiffReport(\n        hdfs.makeQualified(subSubSub), \"s0\", \"s2\");\n    assertEquals(0, report.getDiffList().size());\n\n    verifyDescendantDiffReports(sub1, subSub, subSubSub);\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.fs.FileSystem.mkdirs",
            "location": "2487–2495",
            "code": "/**\n   * Call {@link #mkdirs(Path, FsPermission)} with default permission.\n   * @param f path\n   * @return true if the directory was created\n   * @throws IOException IO failure\n   */\n  public boolean mkdirs(Path f) throws IOException {\n    return mkdirs(f, FsPermission.getDirDefault());\n  }"
          },
          "invocation": {
            "location": "327–327",
            "code": "hdfs.mkdirs(subSubSub);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testSnapRootDescendantDiffReport",
            "location": "318–378",
            "code": "@Test(timeout = 60000)\n  public void testSnapRootDescendantDiffReport() throws Exception {\n    Assume.assumeTrue(conf.getBoolean(\n        DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_DIFF_ALLOW_SNAP_ROOT_DESCENDANT,\n        DFSConfigKeys.\n            DFS_NAMENODE_SNAPSHOT_DIFF_ALLOW_SNAP_ROOT_DESCENDANT_DEFAULT));\n    Path subSub = new Path(sub1, \"subsub1\");\n    Path subSubSub = new Path(subSub, \"subsubsub1\");\n    Path nonSnapDir = new Path(dir, \"non_snap\");\n    hdfs.mkdirs(subSubSub);\n    hdfs.mkdirs(nonSnapDir);\n\n    modifyAndCreateSnapshot(sub1, new Path[]{sub1});\n    modifyAndCreateSnapshot(subSub, new Path[]{sub1});\n    modifyAndCreateSnapshot(subSubSub, new Path[]{sub1});\n\n    try {\n      hdfs.getSnapshotDiffReport(subSub, \"s1\", \"s2\");\n      hdfs.getSnapshotDiffReport(subSubSub, \"s1\", \"s2\");\n    } catch (IOException e) {\n      fail(\"Unexpected exception when getting snapshot diff report \" +\n          subSub + \": \" + e);\n    }\n\n    try {\n      hdfs.getSnapshotDiffReport(nonSnapDir, \"s1\", \"s2\");\n      fail(\"Snapshot diff report on a non snapshot directory '\"\n          + nonSnapDir.getName() + \"'should fail!\");\n    } catch (SnapshotException e) {\n      GenericTestUtils.assertExceptionContains(\n          \"The path \" + nonSnapDir +\n              \" is neither snapshottable nor under a snapshot root!\", e);\n    }\n\n    final String invalidName = \"invalid\";\n    try {\n      hdfs.getSnapshotDiffReport(subSub, invalidName, invalidName);\n      fail(\"Expect exception when providing invalid snapshot name \" +\n          \"for diff report\");\n    } catch (IOException e) {\n      GenericTestUtils.assertExceptionContains(\n          \"Cannot find the snapshot of directory \" + sub1 + \" with name \"\n              + invalidName, e);\n    }\n\n    // diff between the same snapshot\n    SnapshotDiffReport report = hdfs.getSnapshotDiffReport(subSub, \"s0\", \"s0\");\n    assertEquals(0, report.getDiffList().size());\n\n    report = hdfs.getSnapshotDiffReport(subSub, \"\", \"\");\n    assertEquals(0, report.getDiffList().size());\n\n    report = hdfs.getSnapshotDiffReport(subSubSub, \"s0\", \"s2\");\n    assertEquals(0, report.getDiffList().size());\n\n    report = hdfs.getSnapshotDiffReport(\n        hdfs.makeQualified(subSubSub), \"s0\", \"s2\");\n    assertEquals(0, report.getDiffList().size());\n\n    verifyDescendantDiffReports(sub1, subSub, subSubSub);\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.fs.FileSystem.makeQualified",
            "location": "670–683",
            "code": "/**\n   * Qualify a path to one which uses this FileSystem and, if relative,\n   * made absolute.\n   * @param path to qualify.\n   * @return this path if it contains a scheme and authority and is absolute, or\n   * a new path that includes a path and authority and is fully qualified\n   * @see Path#makeQualified(URI, Path)\n   * @throws IllegalArgumentException if the path has a schema/URI different\n   * from this FileSystem.\n   */\n  public Path makeQualified(Path path) {\n    checkPath(path);\n    return path.makeQualified(this.getUri(), this.getWorkingDirectory());\n  }"
          },
          "invocation": {
            "location": "374–374",
            "code": "hdfs.makeQualified(subSubSub), \"s0\", \"s2\");"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testSnapshotDiffReportRemoteIterator",
            "location": "1499–1592",
            "code": "@Test\n  public void testSnapshotDiffReportRemoteIterator() throws Exception {\n    final Path root = new Path(\"/\");\n    hdfs.mkdirs(root);\n    for (int i = 1; i <= 3; i++) {\n      final Path path = new Path(root, \"dir\" + i);\n      hdfs.mkdirs(path);\n    }\n    for (int i = 1; i <= 3; i++) {\n      final Path path = new Path(root, \"dir\" + i);\n      for (int j = 1; j < 4; j++) {\n        final Path file = new Path(path, \"file\" + j);\n        DFSTestUtil.createFile(hdfs, file, BLOCKSIZE, REPLICATION, SEED);\n      }\n    }\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    Path targetDir = new Path(root, \"dir4\");\n    //create directory dir4\n    hdfs.mkdirs(targetDir);\n    //moves files from dir1 to dir4\n    Path path = new Path(root, \"dir1\");\n    for (int j = 1; j < 4; j++) {\n      final Path srcPath = new Path(path, \"file\" + j);\n      final Path targetPath = new Path(targetDir, \"file\" + j);\n      hdfs.rename(srcPath, targetPath);\n    }\n    targetDir = new Path(root, \"dir3\");\n    //overwrite existing files in dir3 from files in dir1\n    path = new Path(root, \"dir2\");\n    for (int j = 1; j < 4; j++) {\n      final Path srcPath = new Path(path, \"file\" + j);\n      final Path targetPath = new Path(targetDir, \"file\" + j);\n      hdfs.rename(srcPath, targetPath, Rename.OVERWRITE);\n    }\n    final Path pathToRename = new Path(root, \"dir2\");\n    //move dir2 inside dir3\n    hdfs.rename(pathToRename, targetDir);\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n    RemoteIterator<SnapshotDiffReportListing> iterator =\n        hdfs.snapshotDiffReportListingRemoteIterator(root, \"s0\", \"s1\");\n    SnapshotDiffReportGenerator snapshotDiffReport;\n    List<SnapshotDiffReportListing.DiffReportListingEntry> modifiedList =\n        new TreeList();\n    List<SnapshotDiffReportListing.DiffReportListingEntry> createdList =\n        new ChunkedArrayList<>();\n    List<SnapshotDiffReportListing.DiffReportListingEntry> deletedList =\n        new ChunkedArrayList<>();\n    SnapshotDiffReportListing report = null;\n    List<SnapshotDiffReportListing> reportList = new ArrayList<>();\n    while (iterator.hasNext()) {\n      report = iterator.next();\n      reportList.add(report);\n      modifiedList.addAll(report.getModifyList());\n      createdList.addAll(report.getCreateList());\n      deletedList.addAll(report.getDeleteList());\n    }\n    try {\n      iterator.next();\n    } catch (Exception e) {\n      Assert.assertTrue(\n          e.getMessage().contains(\"No more entry in SnapshotDiffReport for /\"));\n    }\n    Assert.assertNotEquals(0, reportList.size());\n    // generate the snapshotDiffReport and Verify\n    snapshotDiffReport = new SnapshotDiffReportGenerator(\"/\", \"s0\", \"s1\",\n        report.getIsFromEarlier(), modifiedList, createdList, deletedList);\n    verifyDiffReportForGivenReport(root, \"s0\", \"s1\",\n        snapshotDiffReport.generateReport(),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"dir4\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2\"),\n            DFSUtil.string2Bytes(\"dir3/dir2\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file1\"),\n            DFSUtil.string2Bytes(\"dir4/file1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file2\"),\n            DFSUtil.string2Bytes(\"dir4/file2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file3\"),\n            DFSUtil.string2Bytes(\"dir4/file3\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file1\"),\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file2\"),\n            DFSUtil.string2Bytes(\"dir3/file2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file3\"),\n            DFSUtil.string2Bytes(\"dir3/file3\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir3\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file3\")));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.fs.FileSystem.mkdirs",
            "location": "2487–2495",
            "code": "/**\n   * Call {@link #mkdirs(Path, FsPermission)} with default permission.\n   * @param f path\n   * @return true if the directory was created\n   * @throws IOException IO failure\n   */\n  public boolean mkdirs(Path f) throws IOException {\n    return mkdirs(f, FsPermission.getDirDefault());\n  }"
          },
          "invocation": {
            "location": "1502–1502",
            "code": "hdfs.mkdirs(root);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReport",
            "location": "200–316",
            "code": "/**\n   * Test the computation and representation of diff between snapshots.\n   */\n  @Test(timeout = 60000)\n  public void testDiffReport() throws Exception {\n    cluster.getNamesystem().getSnapshotManager().setAllowNestedSnapshots(true);\n\n    Path subsub1 = new Path(sub1, \"subsub1\");\n    Path subsubsub1 = new Path(subsub1, \"subsubsub1\");\n    hdfs.mkdirs(subsubsub1);\n    modifyAndCreateSnapshot(sub1, new Path[]{sub1, subsubsub1});\n    modifyAndCreateSnapshot(subsubsub1, new Path[]{sub1, subsubsub1});\n\n    final String invalidName = \"invalid\";\n    try {\n      hdfs.getSnapshotDiffReport(sub1, invalidName, invalidName);\n      fail(\"Expect exception when providing invalid snapshot name \" +\n          \"for diff report\");\n    } catch (IOException e) {\n      GenericTestUtils.assertExceptionContains(\n          \"Cannot find the snapshot of directory \" + sub1 + \" with name \"\n              + invalidName, e);\n    }\n\n    // diff between the same snapshot\n    SnapshotDiffReport report = hdfs.getSnapshotDiffReport(sub1, \"s0\", \"s0\");\n    LOG.info(report.toString());\n    assertEquals(0, report.getDiffList().size());\n\n    report = hdfs.getSnapshotDiffReport(sub1, \"\", \"\");\n    LOG.info(report.toString());\n    assertEquals(0, report.getDiffList().size());\n\n    try {\n      report = hdfs.getSnapshotDiffReport(subsubsub1, null, \"s2\");\n      fail(\"Expect exception when providing null fromSnapshot \");\n    } catch (IllegalArgumentException e) {\n      GenericTestUtils.assertExceptionContains(\"null fromSnapshot\", e);\n    }\n    report = hdfs.getSnapshotDiffReport(subsubsub1, \"s0\", \"s2\");\n    LOG.info(report.toString());\n    assertEquals(0, report.getDiffList().size());\n\n    // test path with scheme also works\n    report = hdfs.getSnapshotDiffReport(hdfs.makeQualified(subsubsub1),\n        \"s0\", \"s2\");\n    LOG.info(report.toString());\n    assertEquals(0, report.getDiffList().size());\n\n    verifyDiffReport(sub1, \"s0\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"file15\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"file12\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"file13\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"link13\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"link13\")));\n\n    verifyDiffReport(sub1, \"s0\", \"s5\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"file15\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"file12\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"file10\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"file13\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"link13\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"link13\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file10\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/link13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file15\")));\n\n    verifyDiffReport(sub1, \"s2\", \"s5\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"file10\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file10\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/link13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file15\")));\n\n    verifyDiffReport(sub1, \"s3\", \"\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file15\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file12\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file10\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/link13\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/link13\")));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.fs.FileSystem.mkdirs",
            "location": "2487–2495",
            "code": "/**\n   * Call {@link #mkdirs(Path, FsPermission)} with default permission.\n   * @param f path\n   * @return true if the directory was created\n   * @throws IOException IO failure\n   */\n  public boolean mkdirs(Path f) throws IOException {\n    return mkdirs(f, FsPermission.getDirDefault());\n  }"
          },
          "invocation": {
            "location": "209–209",
            "code": "hdfs.mkdirs(subsubsub1);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReport",
            "location": "200–316",
            "code": "/**\n   * Test the computation and representation of diff between snapshots.\n   */\n  @Test(timeout = 60000)\n  public void testDiffReport() throws Exception {\n    cluster.getNamesystem().getSnapshotManager().setAllowNestedSnapshots(true);\n\n    Path subsub1 = new Path(sub1, \"subsub1\");\n    Path subsubsub1 = new Path(subsub1, \"subsubsub1\");\n    hdfs.mkdirs(subsubsub1);\n    modifyAndCreateSnapshot(sub1, new Path[]{sub1, subsubsub1});\n    modifyAndCreateSnapshot(subsubsub1, new Path[]{sub1, subsubsub1});\n\n    final String invalidName = \"invalid\";\n    try {\n      hdfs.getSnapshotDiffReport(sub1, invalidName, invalidName);\n      fail(\"Expect exception when providing invalid snapshot name \" +\n          \"for diff report\");\n    } catch (IOException e) {\n      GenericTestUtils.assertExceptionContains(\n          \"Cannot find the snapshot of directory \" + sub1 + \" with name \"\n              + invalidName, e);\n    }\n\n    // diff between the same snapshot\n    SnapshotDiffReport report = hdfs.getSnapshotDiffReport(sub1, \"s0\", \"s0\");\n    LOG.info(report.toString());\n    assertEquals(0, report.getDiffList().size());\n\n    report = hdfs.getSnapshotDiffReport(sub1, \"\", \"\");\n    LOG.info(report.toString());\n    assertEquals(0, report.getDiffList().size());\n\n    try {\n      report = hdfs.getSnapshotDiffReport(subsubsub1, null, \"s2\");\n      fail(\"Expect exception when providing null fromSnapshot \");\n    } catch (IllegalArgumentException e) {\n      GenericTestUtils.assertExceptionContains(\"null fromSnapshot\", e);\n    }\n    report = hdfs.getSnapshotDiffReport(subsubsub1, \"s0\", \"s2\");\n    LOG.info(report.toString());\n    assertEquals(0, report.getDiffList().size());\n\n    // test path with scheme also works\n    report = hdfs.getSnapshotDiffReport(hdfs.makeQualified(subsubsub1),\n        \"s0\", \"s2\");\n    LOG.info(report.toString());\n    assertEquals(0, report.getDiffList().size());\n\n    verifyDiffReport(sub1, \"s0\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"file15\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"file12\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"file13\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"link13\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"link13\")));\n\n    verifyDiffReport(sub1, \"s0\", \"s5\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"file15\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"file12\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"file10\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"file13\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"link13\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"link13\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file10\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/link13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file15\")));\n\n    verifyDiffReport(sub1, \"s2\", \"s5\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"file10\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file10\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/link13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file15\")));\n\n    verifyDiffReport(sub1, \"s3\", \"\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file15\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file12\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file10\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/link13\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/link13\")));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.fs.FileSystem.makeQualified",
            "location": "670–683",
            "code": "/**\n   * Qualify a path to one which uses this FileSystem and, if relative,\n   * made absolute.\n   * @param path to qualify.\n   * @return this path if it contains a scheme and authority and is absolute, or\n   * a new path that includes a path and authority and is fully qualified\n   * @see Path#makeQualified(URI, Path)\n   * @throws IllegalArgumentException if the path has a schema/URI different\n   * from this FileSystem.\n   */\n  public Path makeQualified(Path path) {\n    checkPath(path);\n    return path.makeQualified(this.getUri(), this.getWorkingDirectory());\n  }"
          },
          "invocation": {
            "location": "244–244",
            "code": "report = hdfs.getSnapshotDiffReport(hdfs.makeQualified(subsubsub1),"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithRename",
            "location": "835–874",
            "code": "/**\n   * Rename a directory to its prior descendant, and verify the diff report.\n   */\n  @Test\n  public void testDiffReportWithRename() throws Exception {\n    final Path root = new Path(\"/\");\n    final Path sdir1 = new Path(root, \"dir1\");\n    final Path sdir2 = new Path(root, \"dir2\");\n    final Path foo = new Path(sdir1, \"foo\");\n    final Path bar = new Path(foo, \"bar\");\n    hdfs.mkdirs(bar);\n    hdfs.mkdirs(sdir2);\n\n    // create snapshot on root\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n\n    // /dir1/foo/bar -> /dir2/bar\n    final Path bar2 = new Path(sdir2, \"bar\");\n    hdfs.rename(bar, bar2);\n\n    // /dir1/foo -> /dir2/bar/foo\n    final Path foo2 = new Path(bar2, \"foo\");\n    hdfs.rename(foo, foo2);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s2\");\n    // let's delete /dir2 to make things more complicated\n    hdfs.delete(sdir2, true);\n\n    verifyDiffReport(root, \"s1\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/foo\"),\n            DFSUtil.string2Bytes(\"dir2/bar/foo\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir2\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"dir1/foo/bar\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1/foo\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil\n            .string2Bytes(\"dir1/foo/bar\"), DFSUtil.string2Bytes(\"dir2/bar\")));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.fs.FileSystem.mkdirs",
            "location": "2487–2495",
            "code": "/**\n   * Call {@link #mkdirs(Path, FsPermission)} with default permission.\n   * @param f path\n   * @return true if the directory was created\n   * @throws IOException IO failure\n   */\n  public boolean mkdirs(Path f) throws IOException {\n    return mkdirs(f, FsPermission.getDirDefault());\n  }"
          },
          "invocation": {
            "location": "845–845",
            "code": "hdfs.mkdirs(bar);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithRpcLimit3",
            "location": "1419–1469",
            "code": "/**\n   * Tests to verify the diff report with maximum SnapsdiffReportEntries limit\n   * over an rpc being set to 3.\n   * @throws Exception\n   */\n  @Test\n  public void testDiffReportWithRpcLimit3() throws Exception {\n    final Path root = new Path(\"/\");\n    hdfs.mkdirs(root);\n    Path path = new Path(root, \"dir1\");\n    hdfs.mkdirs(path);\n    for (int j = 1; j <= 4; j++) {\n      final Path file = new Path(path, \"file\" + j);\n      DFSTestUtil.createFile(hdfs, file, BLOCKSIZE, REPLICATION, SEED);\n    }\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    path = new Path(root, \"dir1\");\n    for (int j = 1; j <= 4; j++) {\n      final Path file = new Path(path, \"file\" + j);\n      hdfs.delete(file, false);\n    }\n    for (int j = 5; j <= 10; j++) {\n      final Path file = new Path(path, \"file\" + j);\n      DFSTestUtil.createFile(hdfs, file, BLOCKSIZE, REPLICATION, SEED);\n    }\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n    verifyDiffReport(root, \"s0\", \"s1\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir1/file5\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir1/file6\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir1/file7\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir1/file8\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir1/file9\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir1/file10\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir1/file1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir1/file2\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir1/file3\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir1/file4\")));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.fs.FileSystem.mkdirs",
            "location": "2487–2495",
            "code": "/**\n   * Call {@link #mkdirs(Path, FsPermission)} with default permission.\n   * @param f path\n   * @return true if the directory was created\n   * @throws IOException IO failure\n   */\n  public boolean mkdirs(Path f) throws IOException {\n    return mkdirs(f, FsPermission.getDirDefault());\n  }"
          },
          "invocation": {
            "location": "1427–1427",
            "code": "hdfs.mkdirs(root);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithQuota",
            "location": "817–833",
            "code": "@Test\n  public void testDiffReportWithQuota() throws Exception {\n    final Path testdir = new Path(sub1, \"testdir1\");\n    hdfs.mkdirs(testdir);\n    hdfs.allowSnapshot(testdir);\n    // Set quota BEFORE creating the snapshot\n    hdfs.setQuota(testdir, 10, 10);\n    hdfs.createSnapshot(testdir, \"s0\");\n    final SnapshotDiffReport report =\n        hdfs.getSnapshotDiffReport(testdir, \"s0\", \"\");\n    // The diff should be null. Snapshot dir inode should keep the quota.\n    Assert.assertEquals(0, report.getDiffList().size());\n    // Cleanup\n    hdfs.deleteSnapshot(testdir, \"s0\");\n    hdfs.disallowSnapshot(testdir);\n    hdfs.delete(testdir, true);\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.fs.FileSystem.mkdirs",
            "location": "2487–2495",
            "code": "/**\n   * Call {@link #mkdirs(Path, FsPermission)} with default permission.\n   * @param f path\n   * @return true if the directory was created\n   * @throws IOException IO failure\n   */\n  public boolean mkdirs(Path f) throws IOException {\n    return mkdirs(f, FsPermission.getDirDefault());\n  }"
          },
          "invocation": {
            "location": "820–820",
            "code": "hdfs.mkdirs(testdir);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testSnapRootDescendantDiffReportWithRename",
            "location": "580–728",
            "code": "@Test\n  public void testSnapRootDescendantDiffReportWithRename() throws Exception {\n    Assume.assumeTrue(conf.getBoolean(\n        DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_DIFF_ALLOW_SNAP_ROOT_DESCENDANT,\n        DFSConfigKeys.\n            DFS_NAMENODE_SNAPSHOT_DIFF_ALLOW_SNAP_ROOT_DESCENDANT_DEFAULT));\n    Path subSub = new Path(sub1, \"subsub1\");\n    Path subSubSub = new Path(subSub, \"subsubsub1\");\n    Path nonSnapDir = new Path(dir, \"non_snap\");\n    hdfs.mkdirs(subSubSub);\n    hdfs.mkdirs(nonSnapDir);\n\n    hdfs.allowSnapshot(sub1);\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n    Path file20 = new Path(subSubSub, \"file20\");\n    DFSTestUtil.createFile(hdfs, file20, BLOCKSIZE, REPLICATION_1, SEED);\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // Case 1: Move a file away from a descendant dir, but within the snap root.\n    // mv <snaproot>/<subsub>/<subsubsub>/file20 <snaproot>/<subsub>/file20\n    hdfs.rename(file20, new Path(subSub, file20.getName()));\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // The snapshot diff for the snap root detects the change as file rename\n    // as the file move happened within the snap root.\n    verifyDiffReport(sub1, \"s1\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1\")),\n        new DiffReportEntry(DiffType.RENAME,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file20\"),\n            DFSUtil.string2Bytes(\"subsub1/file20\")));\n\n    // The snapshot diff for the descendant dir <subsub> still detects the\n    // change as file rename as the file move happened under the snap root\n    // descendant dir.\n    verifyDiffReport(subSub, \"s1\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsubsub1\")),\n        new DiffReportEntry(DiffType.RENAME,\n            DFSUtil.string2Bytes(\"subsubsub1/file20\"),\n            DFSUtil.string2Bytes(\"file20\")));\n\n    // The snapshot diff for the descendant dir <subsubsub> detects the\n    // change as file delete as the file got moved from its scope.\n    verifyDiffReport(subSubSub, \"s1\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"file20\")));\n\n    // Case 2: Move the file from the snap root descendant dir to any\n    // non snap root dir. mv <snaproot>/<subsub>/file20 <nonsnaproot>/file20.\n    hdfs.rename(new Path(subSub, file20.getName()),\n        new Path(dir, file20.getName()));\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // The snapshot diff for the snap root detects the change as file delete\n    // as the file got moved away from the snap root dir to some non snap\n    // root dir.\n    verifyDiffReport(sub1, \"s2\", \"s3\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/file20\")));\n\n    // The snapshot diff for the snap root descendant <subsub> detects the\n    // change as file delete as the file was previously under its scope and\n    // got moved away from its scope.\n    verifyDiffReport(subSub, \"s2\", \"s3\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"file20\")));\n\n    // The file was already not under the descendant dir <subsubsub> scope.\n    // So, the snapshot diff report for the descendant dir doesn't\n    // show the file rename at all.\n    verifyDiffReport(subSubSub, \"s2\", \"s3\",\n        new DiffReportEntry[]{});\n\n    // Case 3: Move the file from the non-snap root dir to snap root dir\n    // mv <nonsnaproot>/file20 <snaproot>/file20\n    hdfs.rename(new Path(dir, file20.getName()),\n        new Path(sub1, file20.getName()));\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // Snap root directory should show the file moved in as a new file.\n    verifyDiffReport(sub1, \"s3\", \"s4\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file20\")));\n\n    // Snap descendant directories don't have visibility to the moved in file.\n    verifyDiffReport(subSub, \"s3\", \"s4\",\n        new DiffReportEntry[]{});\n    verifyDiffReport(subSubSub, \"s3\", \"s4\",\n        new DiffReportEntry[]{});\n\n    hdfs.rename(new Path(sub1, file20.getName()),\n        new Path(subSub, file20.getName()));\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // Snap root directory now shows the rename as both source and\n    // destination paths are under the snap root.\n    verifyDiffReport(sub1, \"s4\", \"s5\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.RENAME,\n            DFSUtil.string2Bytes(\"file20\"),\n            DFSUtil.string2Bytes(\"subsub1/file20\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")));\n\n    // For the descendant directory under the snap root, the file\n    // moved in shows up as a new file created.\n    verifyDiffReport(subSub, \"s4\", \"s5\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file20\")));\n\n    verifyDiffReport(subSubSub, \"s4\", \"s5\",\n        new DiffReportEntry[]{});\n\n    // Case 4: Snapshot diff for the newly created descendant directory.\n    Path subSubSub2 = new Path(subSub, \"subsubsub2\");\n    hdfs.mkdirs(subSubSub2);\n    Path file30 = new Path(subSubSub2, \"file30\");\n    DFSTestUtil.createFile(hdfs, file30, BLOCKSIZE, REPLICATION_1, SEED);\n    hdfs.createFile(file30);\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    verifyDiffReport(sub1, \"s5\", \"s6\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub2\")));\n\n    verifyDiffReport(subSubSub2, \"s5\", \"s6\",\n        new DiffReportEntry[]{});\n\n    verifyDiffReport(subSubSub2, \"s1\", \"s2\",\n        new DiffReportEntry[]{});\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.fs.FileSystem.mkdirs",
            "location": "2487–2495",
            "code": "/**\n   * Call {@link #mkdirs(Path, FsPermission)} with default permission.\n   * @param f path\n   * @return true if the directory was created\n   * @throws IOException IO failure\n   */\n  public boolean mkdirs(Path f) throws IOException {\n    return mkdirs(f, FsPermission.getDirDefault());\n  }"
          },
          "invocation": {
            "location": "589–589",
            "code": "hdfs.mkdirs(subSubSub);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithRpcLimit",
            "location": "1304–1350",
            "code": "/**\n   * Tests to verfy the diff report with maximum SnapsdiffReportEntries limit\n   * over an rpc being set to 3.\n   * @throws Exception\n   */\n  @Test\n  public void testDiffReportWithRpcLimit() throws Exception {\n    final Path root = new Path(\"/\");\n    hdfs.mkdirs(root);\n    for (int i = 1; i < 4; i++) {\n      final Path path = new Path(root, \"dir\" + i);\n      hdfs.mkdirs(path);\n    }\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    for (int i = 1; i < 4; i++) {\n      final Path path = new Path(root, \"dir\" + i);\n      for (int j = 1; j < 4; j++) {\n        final Path file = new Path(path, \"file\" + j);\n        DFSTestUtil.createFile(hdfs, file, BLOCKSIZE, REPLICATION, SEED);\n      }\n    }\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n    verifyDiffReport(root, \"s0\", \"s1\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir1/file1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir1/file2\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir1/file3\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir2\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir2/file1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir2/file2\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir2/file3\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir3\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir3/file2\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir3/file3\")));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.fs.FileSystem.mkdirs",
            "location": "2487–2495",
            "code": "/**\n   * Call {@link #mkdirs(Path, FsPermission)} with default permission.\n   * @param f path\n   * @return true if the directory was created\n   * @throws IOException IO failure\n   */\n  public boolean mkdirs(Path f) throws IOException {\n    return mkdirs(f, FsPermission.getDirDefault());\n  }"
          },
          "invocation": {
            "location": "1312–1312",
            "code": "hdfs.mkdirs(root);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithRpcLimit2",
            "location": "1352–1417",
            "code": "@Test\n  public void testDiffReportWithRpcLimit2() throws Exception {\n    final Path root = new Path(\"/\");\n    hdfs.mkdirs(root);\n    for (int i = 1; i <=3; i++) {\n      final Path path = new Path(root, \"dir\" + i);\n      hdfs.mkdirs(path);\n    }\n    for (int i = 1; i <= 3; i++) {\n      final Path path = new Path(root, \"dir\" + i);\n      for (int j = 1; j < 4; j++) {\n        final Path file = new Path(path, \"file\" + j);\n        DFSTestUtil.createFile(hdfs, file, BLOCKSIZE, REPLICATION, SEED);\n      }\n    }\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    Path targetDir = new Path(root, \"dir4\");\n    //create directory dir4\n    hdfs.mkdirs(targetDir);\n    //moves files from dir1 to dir4\n    Path path = new Path(root, \"dir1\");\n    for (int j = 1; j < 4; j++) {\n      final Path srcPath = new Path(path, \"file\" + j);\n      final Path targetPath = new Path(targetDir, \"file\" + j);\n      hdfs.rename(srcPath, targetPath);\n    }\n    targetDir = new Path(root, \"dir3\");\n    //overwrite existing files in dir3 from files in dir1\n    path = new Path(root, \"dir2\");\n    for (int j = 1; j < 4; j++) {\n      final Path srcPath = new Path(path, \"file\" + j);\n      final Path targetPath = new Path(targetDir, \"file\" + j);\n      hdfs.rename(srcPath, targetPath, Rename.OVERWRITE);\n    }\n    final Path pathToRename = new Path(root, \"dir2\");\n    //move dir2 inside dir3\n    hdfs.rename(pathToRename, targetDir);\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n    verifyDiffReport(root, \"s0\", \"s1\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir4\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2\"),\n            DFSUtil.string2Bytes(\"dir3/dir2\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file1\"),\n            DFSUtil.string2Bytes(\"dir4/file1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file2\"),\n            DFSUtil.string2Bytes(\"dir4/file2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file3\"),\n            DFSUtil.string2Bytes(\"dir4/file3\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file1\"),\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file2\"),\n            DFSUtil.string2Bytes(\"dir3/file2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file3\"),\n            DFSUtil.string2Bytes(\"dir3/file3\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir3\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file3\")));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.fs.FileSystem.mkdirs",
            "location": "2487–2495",
            "code": "/**\n   * Call {@link #mkdirs(Path, FsPermission)} with default permission.\n   * @param f path\n   * @return true if the directory was created\n   * @throws IOException IO failure\n   */\n  public boolean mkdirs(Path f) throws IOException {\n    return mkdirs(f, FsPermission.getDirDefault());\n  }"
          },
          "invocation": {
            "location": "1355–1355",
            "code": "hdfs.mkdirs(root);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithOpenFiles",
            "location": "1054–1163",
            "code": "/**\n   * Test Snapshot diff report for snapshots with open files captures in them.\n   * Also verify if the diff report remains the same across NameNode restarts.\n   */\n  @Test (timeout = 120000)\n  public void testDiffReportWithOpenFiles() throws Exception {\n    // Construct the directory tree\n    final Path level0A = new Path(\"/level_0_A\");\n    final Path flumeSnapRootDir = level0A;\n    final String flumeFileName = \"flume.log\";\n    final String flumeSnap1Name = \"flume_snap_1\";\n    final String flumeSnap2Name = \"flume_snap_2\";\n\n    // Create files and open a stream\n    final Path flumeFile = new Path(level0A, flumeFileName);\n    createFile(flumeFile);\n    FSDataOutputStream flumeOutputStream = hdfs.append(flumeFile);\n\n    // Create Snapshot S1\n    final Path flumeS1Dir = SnapshotTestHelper.createSnapshot(\n        hdfs, flumeSnapRootDir, flumeSnap1Name);\n    final Path flumeS1Path = new Path(flumeS1Dir, flumeFileName);\n    final long flumeFileLengthAfterS1 = hdfs.getFileStatus(flumeFile).getLen();\n\n    // Verify if Snap S1 file length is same as the the live one\n    Assert.assertEquals(flumeFileLengthAfterS1,\n        hdfs.getFileStatus(flumeS1Path).getLen());\n\n    verifyDiffReport(level0A, flumeSnap1Name, \"\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")));\n\n    long flumeFileWrittenDataLength = flumeFileLengthAfterS1;\n    int newWriteLength = (int) (BLOCKSIZE * 1.5);\n    byte[] buf = new byte[newWriteLength];\n    Random random = new Random();\n    random.nextBytes(buf);\n\n    // Write more data to flume file\n    flumeFileWrittenDataLength += writeToStream(flumeOutputStream, buf);\n\n    // Create Snapshot S2\n    final Path flumeS2Dir = SnapshotTestHelper.createSnapshot(\n        hdfs, flumeSnapRootDir, flumeSnap2Name);\n    final Path flumeS2Path = new Path(flumeS2Dir, flumeFileName);\n\n    // Verify live files length is same as all data written till now\n    final long flumeFileLengthAfterS2 = hdfs.getFileStatus(flumeFile).getLen();\n    Assert.assertEquals(flumeFileWrittenDataLength, flumeFileLengthAfterS2);\n\n    // Verify if Snap S2 file length is same as the live one\n    Assert.assertEquals(flumeFileLengthAfterS2,\n        hdfs.getFileStatus(flumeS2Path).getLen());\n\n    verifyDiffReport(level0A, flumeSnap1Name, \"\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(flumeFileName)));\n\n    verifyDiffReport(level0A, flumeSnap2Name, \"\");\n\n    verifyDiffReport(level0A, flumeSnap1Name, flumeSnap2Name,\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(flumeFileName)));\n\n    // Write more data to flume file\n    flumeFileWrittenDataLength += writeToStream(flumeOutputStream, buf);\n\n    // Verify old flume snapshots have point-in-time / frozen file lengths\n    // even after the live file have moved forward.\n    Assert.assertEquals(flumeFileLengthAfterS1,\n        hdfs.getFileStatus(flumeS1Path).getLen());\n    Assert.assertEquals(flumeFileLengthAfterS2,\n        hdfs.getFileStatus(flumeS2Path).getLen());\n\n    flumeOutputStream.close();\n\n    // Verify if Snap S2 file length is same as the live one\n    Assert.assertEquals(flumeFileWrittenDataLength,\n        hdfs.getFileStatus(flumeFile).getLen());\n\n    // Verify old flume snapshots have point-in-time / frozen file lengths\n    // even after the live file have moved forward.\n    Assert.assertEquals(flumeFileLengthAfterS1,\n        hdfs.getFileStatus(flumeS1Path).getLen());\n    Assert.assertEquals(flumeFileLengthAfterS2,\n        hdfs.getFileStatus(flumeS2Path).getLen());\n\n    verifyDiffReport(level0A, flumeSnap1Name, \"\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(flumeFileName)));\n\n    verifyDiffReport(level0A, flumeSnap2Name, \"\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(flumeFileName)));\n\n    verifyDiffReport(level0A, flumeSnap1Name, flumeSnap2Name,\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(flumeFileName)));\n\n    restartNameNode();\n\n    verifyDiffReport(level0A, flumeSnap1Name, flumeSnap2Name,\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(flumeFileName)));\n\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.fs.FileSystem.append",
            "location": "1503–1517",
            "code": "/**\n   * Append to an existing file (optional operation).\n   * Same as\n   * {@code append(f, getConf().getInt(IO_FILE_BUFFER_SIZE_KEY,\n   *     IO_FILE_BUFFER_SIZE_DEFAULT), null)}\n   * @param f the existing file to be appended.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default).\n   * @return output stream.\n   */\n  public FSDataOutputStream append(Path f) throws IOException {\n    return append(f, getConf().getInt(IO_FILE_BUFFER_SIZE_KEY,\n        IO_FILE_BUFFER_SIZE_DEFAULT), null);\n  }"
          },
          "invocation": {
            "location": "1070–1070",
            "code": "FSDataOutputStream flumeOutputStream = hdfs.append(flumeFile);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testSnapshotDiffReportRemoteIterator2",
            "location": "1594–1605",
            "code": "@Test\n  public void testSnapshotDiffReportRemoteIterator2() throws Exception {\n    final Path root = new Path(\"/\");\n    hdfs.mkdirs(root);\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    try {\n      hdfs.snapshotDiffReportListingRemoteIterator(root, \"s0\", \"\");\n    } catch (Exception e) {\n      Assert.assertTrue(e.getMessage().contains(\"Remote Iterator is\"\n          + \"supported for snapshotDiffReport between two snapshots\"));\n    }\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.fs.FileSystem.mkdirs",
            "location": "2487–2495",
            "code": "/**\n   * Call {@link #mkdirs(Path, FsPermission)} with default permission.\n   * @param f path\n   * @return true if the directory was created\n   * @throws IOException IO failure\n   */\n  public boolean mkdirs(Path f) throws IOException {\n    return mkdirs(f, FsPermission.getDirDefault());\n  }"
          },
          "invocation": {
            "location": "1597–1597",
            "code": "hdfs.mkdirs(root);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testSnapshotDiffInfo",
            "location": "730–777",
            "code": "@Test\n  public void testSnapshotDiffInfo() throws Exception {\n    Path snapshotRootDirPath = dir;\n    Path snapshotDirDescendantPath = new Path(snapshotRootDirPath, \"desc\");\n    Path snapshotDirNonDescendantPath = new Path(\"/dummy/non/snap/desc\");\n    hdfs.mkdirs(snapshotDirDescendantPath);\n    hdfs.mkdirs(snapshotDirNonDescendantPath);\n\n    hdfs.allowSnapshot(snapshotRootDirPath);\n    hdfs.createSnapshot(snapshotRootDirPath, \"s0\");\n    hdfs.createSnapshot(snapshotRootDirPath, \"s1\");\n\n    INodeDirectory snapshotRootDir = cluster.getNameNode()\n        .getNamesystem().getFSDirectory().getINode(\n            snapshotRootDirPath.toUri().getPath())\n        .asDirectory();\n    INodeDirectory snapshotRootDescendantDir = cluster.getNameNode()\n        .getNamesystem().getFSDirectory().getINode(\n            snapshotDirDescendantPath.toUri().getPath())\n        .asDirectory();\n    INodeDirectory snapshotRootNonDescendantDir = cluster.getNameNode()\n        .getNamesystem().getFSDirectory().getINode(\n            snapshotDirNonDescendantPath.toUri().getPath())\n        .asDirectory();\n    try {\n      SnapshotDiffInfo sdi = new SnapshotDiffInfo(\n          snapshotRootDir,\n          snapshotRootDescendantDir,\n          new Snapshot(0, \"s0\", snapshotRootDescendantDir),\n          new Snapshot(0, \"s1\", snapshotRootDescendantDir));\n      LOG.info(\"SnapshotDiffInfo: \" + sdi.getFrom() + \" - \" + sdi.getTo());\n    } catch (IllegalArgumentException iae){\n      fail(\"Unexpected exception when constructing SnapshotDiffInfo: \" + iae);\n    }\n\n    try {\n      SnapshotDiffInfo sdi = new SnapshotDiffInfo(\n          snapshotRootDir,\n          snapshotRootNonDescendantDir,\n          new Snapshot(0, \"s0\", snapshotRootNonDescendantDir),\n          new Snapshot(0, \"s1\", snapshotRootNonDescendantDir));\n      LOG.info(\"SnapshotDiffInfo: \" + sdi.getFrom() + \" - \" + sdi.getTo());\n      fail(\"SnapshotDiffInfo construction should fail for non snapshot root \" +\n          \"or non snapshot root descendant directories!\");\n    } catch (IllegalArgumentException iae) {\n      // expected exception\n    }\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.fs.FileSystem.mkdirs",
            "location": "2487–2495",
            "code": "/**\n   * Call {@link #mkdirs(Path, FsPermission)} with default permission.\n   * @param f path\n   * @return true if the directory was created\n   * @throws IOException IO failure\n   */\n  public boolean mkdirs(Path f) throws IOException {\n    return mkdirs(f, FsPermission.getDirDefault());\n  }"
          },
          "invocation": {
            "location": "735–735",
            "code": "hdfs.mkdirs(snapshotDirDescendantPath);"
          }
        }
      ]
    },
    "clientClass2subType": {
      "fromFile": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java",
      "toFile": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java",
      "snippets": [
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithRenameToNewDir",
            "location": "952–972",
            "code": "@Test\n  public void testDiffReportWithRenameToNewDir() throws Exception {\n    final Path root = new Path(\"/\");\n    final Path foo = new Path(root, \"foo\");\n    final Path fileInFoo = new Path(foo, \"file\");\n    DFSTestUtil.createFile(hdfs, fileInFoo, BLOCKSIZE, REPLICATION, SEED);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    final Path bar = new Path(root, \"bar\");\n    hdfs.mkdirs(bar);\n    final Path fileInBar = new Path(bar, \"file\");\n    hdfs.rename(fileInFoo, fileInBar);\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n\n    verifyDiffReport(root, \"s0\", \"s1\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"foo\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"bar\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"foo/file\"),\n            DFSUtil.string2Bytes(\"bar/file\")));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.rename",
            "location": "913–942",
            "code": "@SuppressWarnings(\"deprecation\")\n  @Override\n  public boolean rename(Path src, Path dst) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.RENAME);\n\n    final Path absSrc = fixRelativePart(src);\n    final Path absDst = fixRelativePart(dst);\n\n    // Try the rename without resolving first\n    try {\n      return dfs.rename(getPathName(absSrc), getPathName(absDst));\n    } catch (UnresolvedLinkException e) {\n      // Fully resolve the source\n      final Path source = getFileLinkStatus(absSrc).getPath();\n      // Keep trying to resolve the destination\n      return new FileSystemLinkResolver<Boolean>() {\n        @Override\n        public Boolean doCall(final Path p) throws IOException {\n          return dfs.rename(getPathName(source), getPathName(p));\n        }\n        @Override\n        public Boolean next(final FileSystem fs, final Path p)\n            throws IOException {\n          // Should just throw an error in FileSystem#checkPath\n          return doCall(p);\n        }\n      }.resolve(this, absDst);\n    }\n  }"
          },
          "invocation": {
            "location": "963–963",
            "code": "hdfs.rename(fileInFoo, fileInBar);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReport2",
            "location": "779–815",
            "code": "/**\n   * Make changes under a sub-directory, then delete the sub-directory. Make\n   * sure the diff report computation correctly retrieve the diff from the\n   * deleted sub-directory.\n   */\n  @Test (timeout=60000)\n  public void testDiffReport2() throws Exception {\n    Path subsub1 = new Path(sub1, \"subsub1\");\n    Path subsubsub1 = new Path(subsub1, \"subsubsub1\");\n    hdfs.mkdirs(subsubsub1);\n    modifyAndCreateSnapshot(subsubsub1, new Path[]{sub1});\n    \n    // delete subsub1\n    hdfs.delete(subsub1, true);\n    // check diff report between s0 and s2\n    verifyDiffReport(sub1, \"s0\", \"s2\", \n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1\")), \n        new DiffReportEntry(DiffType.CREATE, \n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file15\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file12\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/link13\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/link13\")));\n    // check diff report between s0 and the current status\n    verifyDiffReport(sub1, \"s0\", \"\", \n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"subsub1\")));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.delete",
            "location": "996–1012",
            "code": "@Override\n  public boolean delete(Path f, final boolean recursive) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.DELETE);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p) throws IOException {\n        return dfs.delete(getPathName(p), recursive);\n      }\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.delete(p, recursive);\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "792–792",
            "code": "hdfs.delete(subsub1, true);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testSnapRootDescendantDiffReport",
            "location": "318–378",
            "code": "@Test(timeout = 60000)\n  public void testSnapRootDescendantDiffReport() throws Exception {\n    Assume.assumeTrue(conf.getBoolean(\n        DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_DIFF_ALLOW_SNAP_ROOT_DESCENDANT,\n        DFSConfigKeys.\n            DFS_NAMENODE_SNAPSHOT_DIFF_ALLOW_SNAP_ROOT_DESCENDANT_DEFAULT));\n    Path subSub = new Path(sub1, \"subsub1\");\n    Path subSubSub = new Path(subSub, \"subsubsub1\");\n    Path nonSnapDir = new Path(dir, \"non_snap\");\n    hdfs.mkdirs(subSubSub);\n    hdfs.mkdirs(nonSnapDir);\n\n    modifyAndCreateSnapshot(sub1, new Path[]{sub1});\n    modifyAndCreateSnapshot(subSub, new Path[]{sub1});\n    modifyAndCreateSnapshot(subSubSub, new Path[]{sub1});\n\n    try {\n      hdfs.getSnapshotDiffReport(subSub, \"s1\", \"s2\");\n      hdfs.getSnapshotDiffReport(subSubSub, \"s1\", \"s2\");\n    } catch (IOException e) {\n      fail(\"Unexpected exception when getting snapshot diff report \" +\n          subSub + \": \" + e);\n    }\n\n    try {\n      hdfs.getSnapshotDiffReport(nonSnapDir, \"s1\", \"s2\");\n      fail(\"Snapshot diff report on a non snapshot directory '\"\n          + nonSnapDir.getName() + \"'should fail!\");\n    } catch (SnapshotException e) {\n      GenericTestUtils.assertExceptionContains(\n          \"The path \" + nonSnapDir +\n              \" is neither snapshottable nor under a snapshot root!\", e);\n    }\n\n    final String invalidName = \"invalid\";\n    try {\n      hdfs.getSnapshotDiffReport(subSub, invalidName, invalidName);\n      fail(\"Expect exception when providing invalid snapshot name \" +\n          \"for diff report\");\n    } catch (IOException e) {\n      GenericTestUtils.assertExceptionContains(\n          \"Cannot find the snapshot of directory \" + sub1 + \" with name \"\n              + invalidName, e);\n    }\n\n    // diff between the same snapshot\n    SnapshotDiffReport report = hdfs.getSnapshotDiffReport(subSub, \"s0\", \"s0\");\n    assertEquals(0, report.getDiffList().size());\n\n    report = hdfs.getSnapshotDiffReport(subSub, \"\", \"\");\n    assertEquals(0, report.getDiffList().size());\n\n    report = hdfs.getSnapshotDiffReport(subSubSub, \"s0\", \"s2\");\n    assertEquals(0, report.getDiffList().size());\n\n    report = hdfs.getSnapshotDiffReport(\n        hdfs.makeQualified(subSubSub), \"s0\", \"s2\");\n    assertEquals(0, report.getDiffList().size());\n\n    verifyDescendantDiffReports(sub1, subSub, subSubSub);\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.getSnapshotDiffReport",
            "location": "2445–2478",
            "code": "/**\n   * Get the difference between two snapshots, or between a snapshot and the\n   * current tree of a directory.\n   *\n   * @see DFSClient#getSnapshotDiffReportListing\n   */\n  public SnapshotDiffReport getSnapshotDiffReport(final Path snapshotDir,\n      final String fromSnapshot, final String toSnapshot) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_SNAPSHOT_DIFF);\n    Path absF = fixRelativePart(snapshotDir);\n    return new FileSystemLinkResolver<SnapshotDiffReport>() {\n      @Override\n      public SnapshotDiffReport doCall(final Path p)\n          throws IOException {\n        return getSnapshotDiffReportInternal(getPathName(p), fromSnapshot,\n            toSnapshot);\n      }\n\n      @Override\n      public SnapshotDiffReport next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.getSnapshotDiffReport(p, fromSnapshot, toSnapshot);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + snapshotDir + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "335–335",
            "code": "hdfs.getSnapshotDiffReport(subSub, \"s1\", \"s2\");"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testSnapshotDiffReportRemoteIterator",
            "location": "1499–1592",
            "code": "@Test\n  public void testSnapshotDiffReportRemoteIterator() throws Exception {\n    final Path root = new Path(\"/\");\n    hdfs.mkdirs(root);\n    for (int i = 1; i <= 3; i++) {\n      final Path path = new Path(root, \"dir\" + i);\n      hdfs.mkdirs(path);\n    }\n    for (int i = 1; i <= 3; i++) {\n      final Path path = new Path(root, \"dir\" + i);\n      for (int j = 1; j < 4; j++) {\n        final Path file = new Path(path, \"file\" + j);\n        DFSTestUtil.createFile(hdfs, file, BLOCKSIZE, REPLICATION, SEED);\n      }\n    }\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    Path targetDir = new Path(root, \"dir4\");\n    //create directory dir4\n    hdfs.mkdirs(targetDir);\n    //moves files from dir1 to dir4\n    Path path = new Path(root, \"dir1\");\n    for (int j = 1; j < 4; j++) {\n      final Path srcPath = new Path(path, \"file\" + j);\n      final Path targetPath = new Path(targetDir, \"file\" + j);\n      hdfs.rename(srcPath, targetPath);\n    }\n    targetDir = new Path(root, \"dir3\");\n    //overwrite existing files in dir3 from files in dir1\n    path = new Path(root, \"dir2\");\n    for (int j = 1; j < 4; j++) {\n      final Path srcPath = new Path(path, \"file\" + j);\n      final Path targetPath = new Path(targetDir, \"file\" + j);\n      hdfs.rename(srcPath, targetPath, Rename.OVERWRITE);\n    }\n    final Path pathToRename = new Path(root, \"dir2\");\n    //move dir2 inside dir3\n    hdfs.rename(pathToRename, targetDir);\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n    RemoteIterator<SnapshotDiffReportListing> iterator =\n        hdfs.snapshotDiffReportListingRemoteIterator(root, \"s0\", \"s1\");\n    SnapshotDiffReportGenerator snapshotDiffReport;\n    List<SnapshotDiffReportListing.DiffReportListingEntry> modifiedList =\n        new TreeList();\n    List<SnapshotDiffReportListing.DiffReportListingEntry> createdList =\n        new ChunkedArrayList<>();\n    List<SnapshotDiffReportListing.DiffReportListingEntry> deletedList =\n        new ChunkedArrayList<>();\n    SnapshotDiffReportListing report = null;\n    List<SnapshotDiffReportListing> reportList = new ArrayList<>();\n    while (iterator.hasNext()) {\n      report = iterator.next();\n      reportList.add(report);\n      modifiedList.addAll(report.getModifyList());\n      createdList.addAll(report.getCreateList());\n      deletedList.addAll(report.getDeleteList());\n    }\n    try {\n      iterator.next();\n    } catch (Exception e) {\n      Assert.assertTrue(\n          e.getMessage().contains(\"No more entry in SnapshotDiffReport for /\"));\n    }\n    Assert.assertNotEquals(0, reportList.size());\n    // generate the snapshotDiffReport and Verify\n    snapshotDiffReport = new SnapshotDiffReportGenerator(\"/\", \"s0\", \"s1\",\n        report.getIsFromEarlier(), modifiedList, createdList, deletedList);\n    verifyDiffReportForGivenReport(root, \"s0\", \"s1\",\n        snapshotDiffReport.generateReport(),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"dir4\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2\"),\n            DFSUtil.string2Bytes(\"dir3/dir2\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file1\"),\n            DFSUtil.string2Bytes(\"dir4/file1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file2\"),\n            DFSUtil.string2Bytes(\"dir4/file2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file3\"),\n            DFSUtil.string2Bytes(\"dir4/file3\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file1\"),\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file2\"),\n            DFSUtil.string2Bytes(\"dir3/file2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file3\"),\n            DFSUtil.string2Bytes(\"dir3/file3\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir3\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file3\")));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.rename",
            "location": "913–942",
            "code": "@SuppressWarnings(\"deprecation\")\n  @Override\n  public boolean rename(Path src, Path dst) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.RENAME);\n\n    final Path absSrc = fixRelativePart(src);\n    final Path absDst = fixRelativePart(dst);\n\n    // Try the rename without resolving first\n    try {\n      return dfs.rename(getPathName(absSrc), getPathName(absDst));\n    } catch (UnresolvedLinkException e) {\n      // Fully resolve the source\n      final Path source = getFileLinkStatus(absSrc).getPath();\n      // Keep trying to resolve the destination\n      return new FileSystemLinkResolver<Boolean>() {\n        @Override\n        public Boolean doCall(final Path p) throws IOException {\n          return dfs.rename(getPathName(source), getPathName(p));\n        }\n        @Override\n        public Boolean next(final FileSystem fs, final Path p)\n            throws IOException {\n          // Should just throw an error in FileSystem#checkPath\n          return doCall(p);\n        }\n      }.resolve(this, absDst);\n    }\n  }"
          },
          "invocation": {
            "location": "1523–1523",
            "code": "hdfs.rename(srcPath, targetPath);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testSnapshotDiffReportRemoteIterator",
            "location": "1499–1592",
            "code": "@Test\n  public void testSnapshotDiffReportRemoteIterator() throws Exception {\n    final Path root = new Path(\"/\");\n    hdfs.mkdirs(root);\n    for (int i = 1; i <= 3; i++) {\n      final Path path = new Path(root, \"dir\" + i);\n      hdfs.mkdirs(path);\n    }\n    for (int i = 1; i <= 3; i++) {\n      final Path path = new Path(root, \"dir\" + i);\n      for (int j = 1; j < 4; j++) {\n        final Path file = new Path(path, \"file\" + j);\n        DFSTestUtil.createFile(hdfs, file, BLOCKSIZE, REPLICATION, SEED);\n      }\n    }\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    Path targetDir = new Path(root, \"dir4\");\n    //create directory dir4\n    hdfs.mkdirs(targetDir);\n    //moves files from dir1 to dir4\n    Path path = new Path(root, \"dir1\");\n    for (int j = 1; j < 4; j++) {\n      final Path srcPath = new Path(path, \"file\" + j);\n      final Path targetPath = new Path(targetDir, \"file\" + j);\n      hdfs.rename(srcPath, targetPath);\n    }\n    targetDir = new Path(root, \"dir3\");\n    //overwrite existing files in dir3 from files in dir1\n    path = new Path(root, \"dir2\");\n    for (int j = 1; j < 4; j++) {\n      final Path srcPath = new Path(path, \"file\" + j);\n      final Path targetPath = new Path(targetDir, \"file\" + j);\n      hdfs.rename(srcPath, targetPath, Rename.OVERWRITE);\n    }\n    final Path pathToRename = new Path(root, \"dir2\");\n    //move dir2 inside dir3\n    hdfs.rename(pathToRename, targetDir);\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n    RemoteIterator<SnapshotDiffReportListing> iterator =\n        hdfs.snapshotDiffReportListingRemoteIterator(root, \"s0\", \"s1\");\n    SnapshotDiffReportGenerator snapshotDiffReport;\n    List<SnapshotDiffReportListing.DiffReportListingEntry> modifiedList =\n        new TreeList();\n    List<SnapshotDiffReportListing.DiffReportListingEntry> createdList =\n        new ChunkedArrayList<>();\n    List<SnapshotDiffReportListing.DiffReportListingEntry> deletedList =\n        new ChunkedArrayList<>();\n    SnapshotDiffReportListing report = null;\n    List<SnapshotDiffReportListing> reportList = new ArrayList<>();\n    while (iterator.hasNext()) {\n      report = iterator.next();\n      reportList.add(report);\n      modifiedList.addAll(report.getModifyList());\n      createdList.addAll(report.getCreateList());\n      deletedList.addAll(report.getDeleteList());\n    }\n    try {\n      iterator.next();\n    } catch (Exception e) {\n      Assert.assertTrue(\n          e.getMessage().contains(\"No more entry in SnapshotDiffReport for /\"));\n    }\n    Assert.assertNotEquals(0, reportList.size());\n    // generate the snapshotDiffReport and Verify\n    snapshotDiffReport = new SnapshotDiffReportGenerator(\"/\", \"s0\", \"s1\",\n        report.getIsFromEarlier(), modifiedList, createdList, deletedList);\n    verifyDiffReportForGivenReport(root, \"s0\", \"s1\",\n        snapshotDiffReport.generateReport(),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"dir4\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2\"),\n            DFSUtil.string2Bytes(\"dir3/dir2\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file1\"),\n            DFSUtil.string2Bytes(\"dir4/file1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file2\"),\n            DFSUtil.string2Bytes(\"dir4/file2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file3\"),\n            DFSUtil.string2Bytes(\"dir4/file3\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file1\"),\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file2\"),\n            DFSUtil.string2Bytes(\"dir3/file2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file3\"),\n            DFSUtil.string2Bytes(\"dir3/file3\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir3\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file3\")));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.snapshotDiffReportListingRemoteIterator",
            "location": "2346–2390",
            "code": "/**\n   * Returns a remote iterator so that followup calls are made on demand\n   * while consuming the SnapshotDiffReportListing entries.\n   * This reduces memory consumption overhead in case the snapshotDiffReport\n   * is huge.\n   *\n   * @param snapshotDir\n   *          full path of the directory where snapshots are taken\n   * @param fromSnapshot\n   *          snapshot name of the from point. Null indicates the current\n   *          tree\n   * @param toSnapshot\n   *          snapshot name of the to point. Null indicates the current\n   *          tree.\n   * @return Remote iterator\n   */\n  public RemoteIterator\n      <SnapshotDiffReportListing> snapshotDiffReportListingRemoteIterator(\n      final Path snapshotDir, final String fromSnapshot,\n      final String toSnapshot) throws IOException {\n    Path absF = fixRelativePart(snapshotDir);\n    return new FileSystemLinkResolver\n        <RemoteIterator<SnapshotDiffReportListing>>() {\n      @Override\n      public RemoteIterator<SnapshotDiffReportListing> doCall(final Path p)\n          throws IOException {\n        if (!DFSUtilClient.isValidSnapshotName(fromSnapshot) ||\n            !DFSUtilClient.isValidSnapshotName(toSnapshot)) {\n          throw new UnsupportedOperationException(\"Remote Iterator is\"\n              + \"supported for snapshotDiffReport between two snapshots\");\n        }\n        return new SnapshotDiffReportListingIterator(getPathName(p),\n            fromSnapshot, toSnapshot);\n      }\n\n      @Override\n      public RemoteIterator<SnapshotDiffReportListing> next(final FileSystem fs,\n          final Path p) throws IOException {\n        return ((DistributedFileSystem) fs)\n            .snapshotDiffReportListingRemoteIterator(p, fromSnapshot,\n                toSnapshot);\n      }\n    }.resolve(this, absF);\n\n  }"
          },
          "invocation": {
            "location": "1538–1538",
            "code": "hdfs.snapshotDiffReportListingRemoteIterator(root, \"s0\", \"s1\");"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testSnapshotDiffReportRemoteIterator",
            "location": "1499–1592",
            "code": "@Test\n  public void testSnapshotDiffReportRemoteIterator() throws Exception {\n    final Path root = new Path(\"/\");\n    hdfs.mkdirs(root);\n    for (int i = 1; i <= 3; i++) {\n      final Path path = new Path(root, \"dir\" + i);\n      hdfs.mkdirs(path);\n    }\n    for (int i = 1; i <= 3; i++) {\n      final Path path = new Path(root, \"dir\" + i);\n      for (int j = 1; j < 4; j++) {\n        final Path file = new Path(path, \"file\" + j);\n        DFSTestUtil.createFile(hdfs, file, BLOCKSIZE, REPLICATION, SEED);\n      }\n    }\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    Path targetDir = new Path(root, \"dir4\");\n    //create directory dir4\n    hdfs.mkdirs(targetDir);\n    //moves files from dir1 to dir4\n    Path path = new Path(root, \"dir1\");\n    for (int j = 1; j < 4; j++) {\n      final Path srcPath = new Path(path, \"file\" + j);\n      final Path targetPath = new Path(targetDir, \"file\" + j);\n      hdfs.rename(srcPath, targetPath);\n    }\n    targetDir = new Path(root, \"dir3\");\n    //overwrite existing files in dir3 from files in dir1\n    path = new Path(root, \"dir2\");\n    for (int j = 1; j < 4; j++) {\n      final Path srcPath = new Path(path, \"file\" + j);\n      final Path targetPath = new Path(targetDir, \"file\" + j);\n      hdfs.rename(srcPath, targetPath, Rename.OVERWRITE);\n    }\n    final Path pathToRename = new Path(root, \"dir2\");\n    //move dir2 inside dir3\n    hdfs.rename(pathToRename, targetDir);\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n    RemoteIterator<SnapshotDiffReportListing> iterator =\n        hdfs.snapshotDiffReportListingRemoteIterator(root, \"s0\", \"s1\");\n    SnapshotDiffReportGenerator snapshotDiffReport;\n    List<SnapshotDiffReportListing.DiffReportListingEntry> modifiedList =\n        new TreeList();\n    List<SnapshotDiffReportListing.DiffReportListingEntry> createdList =\n        new ChunkedArrayList<>();\n    List<SnapshotDiffReportListing.DiffReportListingEntry> deletedList =\n        new ChunkedArrayList<>();\n    SnapshotDiffReportListing report = null;\n    List<SnapshotDiffReportListing> reportList = new ArrayList<>();\n    while (iterator.hasNext()) {\n      report = iterator.next();\n      reportList.add(report);\n      modifiedList.addAll(report.getModifyList());\n      createdList.addAll(report.getCreateList());\n      deletedList.addAll(report.getDeleteList());\n    }\n    try {\n      iterator.next();\n    } catch (Exception e) {\n      Assert.assertTrue(\n          e.getMessage().contains(\"No more entry in SnapshotDiffReport for /\"));\n    }\n    Assert.assertNotEquals(0, reportList.size());\n    // generate the snapshotDiffReport and Verify\n    snapshotDiffReport = new SnapshotDiffReportGenerator(\"/\", \"s0\", \"s1\",\n        report.getIsFromEarlier(), modifiedList, createdList, deletedList);\n    verifyDiffReportForGivenReport(root, \"s0\", \"s1\",\n        snapshotDiffReport.generateReport(),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"dir4\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2\"),\n            DFSUtil.string2Bytes(\"dir3/dir2\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file1\"),\n            DFSUtil.string2Bytes(\"dir4/file1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file2\"),\n            DFSUtil.string2Bytes(\"dir4/file2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file3\"),\n            DFSUtil.string2Bytes(\"dir4/file3\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file1\"),\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file2\"),\n            DFSUtil.string2Bytes(\"dir3/file2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file3\"),\n            DFSUtil.string2Bytes(\"dir3/file3\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir3\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file3\")));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.rename",
            "location": "944–976",
            "code": "/**\n   * This rename operation is guaranteed to be atomic.\n   */\n  @SuppressWarnings(\"deprecation\")\n  @Override\n  public void rename(Path src, Path dst, final Options.Rename... options)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.RENAME);\n    final Path absSrc = fixRelativePart(src);\n    final Path absDst = fixRelativePart(dst);\n    // Try the rename without resolving first\n    try {\n      dfs.rename(getPathName(absSrc), getPathName(absDst), options);\n    } catch (UnresolvedLinkException e) {\n      // Fully resolve the source\n      final Path source = getFileLinkStatus(absSrc).getPath();\n      // Keep trying to resolve the destination\n      new FileSystemLinkResolver<Void>() {\n        @Override\n        public Void doCall(final Path p) throws IOException {\n          dfs.rename(getPathName(source), getPathName(p), options);\n          return null;\n        }\n        @Override\n        public Void next(final FileSystem fs, final Path p)\n            throws IOException {\n          // Should just throw an error in FileSystem#checkPath\n          return doCall(p);\n        }\n      }.resolve(this, absDst);\n    }\n  }"
          },
          "invocation": {
            "location": "1531–1531",
            "code": "hdfs.rename(srcPath, targetPath, Rename.OVERWRITE);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReport",
            "location": "200–316",
            "code": "/**\n   * Test the computation and representation of diff between snapshots.\n   */\n  @Test(timeout = 60000)\n  public void testDiffReport() throws Exception {\n    cluster.getNamesystem().getSnapshotManager().setAllowNestedSnapshots(true);\n\n    Path subsub1 = new Path(sub1, \"subsub1\");\n    Path subsubsub1 = new Path(subsub1, \"subsubsub1\");\n    hdfs.mkdirs(subsubsub1);\n    modifyAndCreateSnapshot(sub1, new Path[]{sub1, subsubsub1});\n    modifyAndCreateSnapshot(subsubsub1, new Path[]{sub1, subsubsub1});\n\n    final String invalidName = \"invalid\";\n    try {\n      hdfs.getSnapshotDiffReport(sub1, invalidName, invalidName);\n      fail(\"Expect exception when providing invalid snapshot name \" +\n          \"for diff report\");\n    } catch (IOException e) {\n      GenericTestUtils.assertExceptionContains(\n          \"Cannot find the snapshot of directory \" + sub1 + \" with name \"\n              + invalidName, e);\n    }\n\n    // diff between the same snapshot\n    SnapshotDiffReport report = hdfs.getSnapshotDiffReport(sub1, \"s0\", \"s0\");\n    LOG.info(report.toString());\n    assertEquals(0, report.getDiffList().size());\n\n    report = hdfs.getSnapshotDiffReport(sub1, \"\", \"\");\n    LOG.info(report.toString());\n    assertEquals(0, report.getDiffList().size());\n\n    try {\n      report = hdfs.getSnapshotDiffReport(subsubsub1, null, \"s2\");\n      fail(\"Expect exception when providing null fromSnapshot \");\n    } catch (IllegalArgumentException e) {\n      GenericTestUtils.assertExceptionContains(\"null fromSnapshot\", e);\n    }\n    report = hdfs.getSnapshotDiffReport(subsubsub1, \"s0\", \"s2\");\n    LOG.info(report.toString());\n    assertEquals(0, report.getDiffList().size());\n\n    // test path with scheme also works\n    report = hdfs.getSnapshotDiffReport(hdfs.makeQualified(subsubsub1),\n        \"s0\", \"s2\");\n    LOG.info(report.toString());\n    assertEquals(0, report.getDiffList().size());\n\n    verifyDiffReport(sub1, \"s0\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"file15\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"file12\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"file13\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"link13\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"link13\")));\n\n    verifyDiffReport(sub1, \"s0\", \"s5\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"file15\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"file12\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"file10\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"file11\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"file13\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"link13\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(\"link13\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file10\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/link13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file15\")));\n\n    verifyDiffReport(sub1, \"s2\", \"s5\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"file10\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file10\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/link13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file15\")));\n\n    verifyDiffReport(sub1, \"s3\", \"\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file15\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file12\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file10\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file11\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file13\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/link13\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/link13\")));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.getSnapshotDiffReport",
            "location": "2445–2478",
            "code": "/**\n   * Get the difference between two snapshots, or between a snapshot and the\n   * current tree of a directory.\n   *\n   * @see DFSClient#getSnapshotDiffReportListing\n   */\n  public SnapshotDiffReport getSnapshotDiffReport(final Path snapshotDir,\n      final String fromSnapshot, final String toSnapshot) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_SNAPSHOT_DIFF);\n    Path absF = fixRelativePart(snapshotDir);\n    return new FileSystemLinkResolver<SnapshotDiffReport>() {\n      @Override\n      public SnapshotDiffReport doCall(final Path p)\n          throws IOException {\n        return getSnapshotDiffReportInternal(getPathName(p), fromSnapshot,\n            toSnapshot);\n      }\n\n      @Override\n      public SnapshotDiffReport next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.getSnapshotDiffReport(p, fromSnapshot, toSnapshot);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + snapshotDir + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "215–215",
            "code": "hdfs.getSnapshotDiffReport(sub1, invalidName, invalidName);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDontCaptureAccessTimeOnlyChangeReport",
            "location": "1203–1302",
            "code": "/**\n   * Check to see access time is not captured in snapshot when applicable.\n   * When DFS_NAMENODE_SNAPSHOT_SKIP_CAPTURE_ACCESSTIME_ONLY_CHANGE\n   * is set to true, and if a file's access time changed between two\n   * snapshots but has no other modification, then the access time is not\n   * captured in snapshot.\n   */\n  @Test\n  public void testDontCaptureAccessTimeOnlyChangeReport() throws Exception {\n    final Path froot = new Path(\"/\");\n    final Path root = new Path(froot, \"/testSdiffCalc\");\n\n    // items created pre enabling snapshot\n    final Path filePreSS = new Path(root, \"fParent/filePreSS\");\n    final Path dirPreSS = new Path(root, \"dirPreSS\");\n    final Path dirPreSSChild = new Path(dirPreSS, \"dirPreSSChild\");\n\n    // items created after enabling snapshot\n    final Path filePostSS = new Path(root, \"fParent/filePostSS\");\n    final Path dirPostSS = new Path(root, \"dirPostSS\");\n    final Path dirPostSSChild = new Path(dirPostSS, \"dirPostSSChild\");\n\n    DFSTestUtil.createFile(hdfs, filePreSS, BLOCKSIZE, REPLICATION, SEED);\n    DFSTestUtil.createFile(hdfs, dirPreSSChild, BLOCKSIZE, REPLICATION, SEED);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    printAtime(filePreSS, root, \"s0\");\n    printAtime(dirPreSS, root, \"s0\");\n\n    // items created after creating the first snapshot\n    DFSTestUtil.createFile(hdfs, filePostSS, BLOCKSIZE, REPLICATION, SEED);\n    DFSTestUtil.createFile(hdfs, dirPostSSChild, BLOCKSIZE, REPLICATION, SEED);\n\n    Thread.sleep(3000);\n    long now = Time.now();\n    hdfs.setTimes(filePreSS, -1, now);\n    hdfs.setTimes(filePostSS, -1, now);\n    hdfs.setTimes(dirPreSS, -1, now);\n    hdfs.setTimes(dirPostSS, -1, now);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n    printAtime(filePreSS, root, \"s1\");\n    printAtime(dirPreSS, root, \"s1\");\n    printAtime(filePostSS, root, \"s1\");\n    printAtime(dirPostSS, root, \"s1\");\n\n    Thread.sleep(3000);\n    now = Time.now();\n    hdfs.setTimes(filePreSS, -1, now);\n    hdfs.setTimes(filePostSS, -1, now);\n    hdfs.setTimes(dirPreSS, -1, now);\n    hdfs.setTimes(dirPostSS, -1, now);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s2\");\n    printAtime(filePreSS, root, \"s2\");\n    printAtime(dirPreSS, root, \"s2\");\n    printAtime(filePostSS, root, \"s2\");\n    printAtime(dirPostSS, root, \"s2\");\n\n    Thread.sleep(3000);\n    now = Time.now();\n    // modify filePostSS, and change access time\n    hdfs.setReplication(filePostSS, (short) (REPLICATION - 1));\n    hdfs.setTimes(filePostSS, -1, now);\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s3\");\n\n    LOG.info(\"\\nsnapshotDiff s0 -> s1:\");\n    LOG.info(hdfs.getSnapshotDiffReport(root, \"s0\", \"s1\").toString());\n    LOG.info(\"\\nsnapshotDiff s1 -> s2:\");\n    LOG.info(hdfs.getSnapshotDiffReport(root, \"s1\", \"s2\").toString());\n\n    assertAtimeEquals(filePreSS, root, \"s0\", \"s1\");\n    assertAtimeEquals(dirPreSS, root, \"s0\", \"s1\");\n\n    assertAtimeEquals(filePreSS, root, \"s1\", \"s2\");\n    assertAtimeEquals(dirPreSS, root, \"s1\", \"s2\");\n\n    assertAtimeEquals(filePostSS, root, \"s1\", \"s2\");\n    assertAtimeEquals(dirPostSS, root, \"s1\", \"s2\");\n\n    // access time should be captured in snapshot due to\n    // other modification\n    assertAtimeNotEquals(filePostSS, root, \"s2\", \"s3\");\n\n    // restart NN, and see the access time relationship\n    // still stands (no change caused by edit logs\n    // loading)\n    cluster.restartNameNodes();\n    cluster.waitActive();\n    assertAtimeEquals(filePreSS, root, \"s0\", \"s1\");\n    assertAtimeEquals(dirPreSS, root, \"s0\", \"s1\");\n\n    assertAtimeEquals(filePreSS, root, \"s1\", \"s2\");\n    assertAtimeEquals(dirPreSS, root, \"s1\", \"s2\");\n\n    assertAtimeEquals(filePostSS, root, \"s1\", \"s2\");\n    assertAtimeEquals(dirPostSS, root, \"s1\", \"s2\");\n\n    assertAtimeNotEquals(filePostSS, root, \"s2\", \"s3\");\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.setTimes",
            "location": "2040–2060",
            "code": "@Override\n  public void setTimes(Path p, final long mtime, final long atime)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.SET_TIMES);\n    Path absF = fixRelativePart(p);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.setTimes(getPathName(p), mtime, atime);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        fs.setTimes(p, mtime, atime);\n        return null;\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "1238–1238",
            "code": "hdfs.setTimes(filePreSS, -1, now);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDontCaptureAccessTimeOnlyChangeReport",
            "location": "1203–1302",
            "code": "/**\n   * Check to see access time is not captured in snapshot when applicable.\n   * When DFS_NAMENODE_SNAPSHOT_SKIP_CAPTURE_ACCESSTIME_ONLY_CHANGE\n   * is set to true, and if a file's access time changed between two\n   * snapshots but has no other modification, then the access time is not\n   * captured in snapshot.\n   */\n  @Test\n  public void testDontCaptureAccessTimeOnlyChangeReport() throws Exception {\n    final Path froot = new Path(\"/\");\n    final Path root = new Path(froot, \"/testSdiffCalc\");\n\n    // items created pre enabling snapshot\n    final Path filePreSS = new Path(root, \"fParent/filePreSS\");\n    final Path dirPreSS = new Path(root, \"dirPreSS\");\n    final Path dirPreSSChild = new Path(dirPreSS, \"dirPreSSChild\");\n\n    // items created after enabling snapshot\n    final Path filePostSS = new Path(root, \"fParent/filePostSS\");\n    final Path dirPostSS = new Path(root, \"dirPostSS\");\n    final Path dirPostSSChild = new Path(dirPostSS, \"dirPostSSChild\");\n\n    DFSTestUtil.createFile(hdfs, filePreSS, BLOCKSIZE, REPLICATION, SEED);\n    DFSTestUtil.createFile(hdfs, dirPreSSChild, BLOCKSIZE, REPLICATION, SEED);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    printAtime(filePreSS, root, \"s0\");\n    printAtime(dirPreSS, root, \"s0\");\n\n    // items created after creating the first snapshot\n    DFSTestUtil.createFile(hdfs, filePostSS, BLOCKSIZE, REPLICATION, SEED);\n    DFSTestUtil.createFile(hdfs, dirPostSSChild, BLOCKSIZE, REPLICATION, SEED);\n\n    Thread.sleep(3000);\n    long now = Time.now();\n    hdfs.setTimes(filePreSS, -1, now);\n    hdfs.setTimes(filePostSS, -1, now);\n    hdfs.setTimes(dirPreSS, -1, now);\n    hdfs.setTimes(dirPostSS, -1, now);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n    printAtime(filePreSS, root, \"s1\");\n    printAtime(dirPreSS, root, \"s1\");\n    printAtime(filePostSS, root, \"s1\");\n    printAtime(dirPostSS, root, \"s1\");\n\n    Thread.sleep(3000);\n    now = Time.now();\n    hdfs.setTimes(filePreSS, -1, now);\n    hdfs.setTimes(filePostSS, -1, now);\n    hdfs.setTimes(dirPreSS, -1, now);\n    hdfs.setTimes(dirPostSS, -1, now);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s2\");\n    printAtime(filePreSS, root, \"s2\");\n    printAtime(dirPreSS, root, \"s2\");\n    printAtime(filePostSS, root, \"s2\");\n    printAtime(dirPostSS, root, \"s2\");\n\n    Thread.sleep(3000);\n    now = Time.now();\n    // modify filePostSS, and change access time\n    hdfs.setReplication(filePostSS, (short) (REPLICATION - 1));\n    hdfs.setTimes(filePostSS, -1, now);\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s3\");\n\n    LOG.info(\"\\nsnapshotDiff s0 -> s1:\");\n    LOG.info(hdfs.getSnapshotDiffReport(root, \"s0\", \"s1\").toString());\n    LOG.info(\"\\nsnapshotDiff s1 -> s2:\");\n    LOG.info(hdfs.getSnapshotDiffReport(root, \"s1\", \"s2\").toString());\n\n    assertAtimeEquals(filePreSS, root, \"s0\", \"s1\");\n    assertAtimeEquals(dirPreSS, root, \"s0\", \"s1\");\n\n    assertAtimeEquals(filePreSS, root, \"s1\", \"s2\");\n    assertAtimeEquals(dirPreSS, root, \"s1\", \"s2\");\n\n    assertAtimeEquals(filePostSS, root, \"s1\", \"s2\");\n    assertAtimeEquals(dirPostSS, root, \"s1\", \"s2\");\n\n    // access time should be captured in snapshot due to\n    // other modification\n    assertAtimeNotEquals(filePostSS, root, \"s2\", \"s3\");\n\n    // restart NN, and see the access time relationship\n    // still stands (no change caused by edit logs\n    // loading)\n    cluster.restartNameNodes();\n    cluster.waitActive();\n    assertAtimeEquals(filePreSS, root, \"s0\", \"s1\");\n    assertAtimeEquals(dirPreSS, root, \"s0\", \"s1\");\n\n    assertAtimeEquals(filePreSS, root, \"s1\", \"s2\");\n    assertAtimeEquals(dirPreSS, root, \"s1\", \"s2\");\n\n    assertAtimeEquals(filePostSS, root, \"s1\", \"s2\");\n    assertAtimeEquals(dirPostSS, root, \"s1\", \"s2\");\n\n    assertAtimeNotEquals(filePostSS, root, \"s2\", \"s3\");\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.setReplication",
            "location": "740–757",
            "code": "@Override\n  public boolean setReplication(Path src, final short replication)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.SET_REPLICATION);\n    Path absF = fixRelativePart(src);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p) throws IOException {\n        return dfs.setReplication(getPathName(p), replication);\n      }\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.setReplication(p, replication);\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "1265–1265",
            "code": "hdfs.setReplication(filePostSS, (short) (REPLICATION - 1));"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDontCaptureAccessTimeOnlyChangeReport",
            "location": "1203–1302",
            "code": "/**\n   * Check to see access time is not captured in snapshot when applicable.\n   * When DFS_NAMENODE_SNAPSHOT_SKIP_CAPTURE_ACCESSTIME_ONLY_CHANGE\n   * is set to true, and if a file's access time changed between two\n   * snapshots but has no other modification, then the access time is not\n   * captured in snapshot.\n   */\n  @Test\n  public void testDontCaptureAccessTimeOnlyChangeReport() throws Exception {\n    final Path froot = new Path(\"/\");\n    final Path root = new Path(froot, \"/testSdiffCalc\");\n\n    // items created pre enabling snapshot\n    final Path filePreSS = new Path(root, \"fParent/filePreSS\");\n    final Path dirPreSS = new Path(root, \"dirPreSS\");\n    final Path dirPreSSChild = new Path(dirPreSS, \"dirPreSSChild\");\n\n    // items created after enabling snapshot\n    final Path filePostSS = new Path(root, \"fParent/filePostSS\");\n    final Path dirPostSS = new Path(root, \"dirPostSS\");\n    final Path dirPostSSChild = new Path(dirPostSS, \"dirPostSSChild\");\n\n    DFSTestUtil.createFile(hdfs, filePreSS, BLOCKSIZE, REPLICATION, SEED);\n    DFSTestUtil.createFile(hdfs, dirPreSSChild, BLOCKSIZE, REPLICATION, SEED);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    printAtime(filePreSS, root, \"s0\");\n    printAtime(dirPreSS, root, \"s0\");\n\n    // items created after creating the first snapshot\n    DFSTestUtil.createFile(hdfs, filePostSS, BLOCKSIZE, REPLICATION, SEED);\n    DFSTestUtil.createFile(hdfs, dirPostSSChild, BLOCKSIZE, REPLICATION, SEED);\n\n    Thread.sleep(3000);\n    long now = Time.now();\n    hdfs.setTimes(filePreSS, -1, now);\n    hdfs.setTimes(filePostSS, -1, now);\n    hdfs.setTimes(dirPreSS, -1, now);\n    hdfs.setTimes(dirPostSS, -1, now);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n    printAtime(filePreSS, root, \"s1\");\n    printAtime(dirPreSS, root, \"s1\");\n    printAtime(filePostSS, root, \"s1\");\n    printAtime(dirPostSS, root, \"s1\");\n\n    Thread.sleep(3000);\n    now = Time.now();\n    hdfs.setTimes(filePreSS, -1, now);\n    hdfs.setTimes(filePostSS, -1, now);\n    hdfs.setTimes(dirPreSS, -1, now);\n    hdfs.setTimes(dirPostSS, -1, now);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s2\");\n    printAtime(filePreSS, root, \"s2\");\n    printAtime(dirPreSS, root, \"s2\");\n    printAtime(filePostSS, root, \"s2\");\n    printAtime(dirPostSS, root, \"s2\");\n\n    Thread.sleep(3000);\n    now = Time.now();\n    // modify filePostSS, and change access time\n    hdfs.setReplication(filePostSS, (short) (REPLICATION - 1));\n    hdfs.setTimes(filePostSS, -1, now);\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s3\");\n\n    LOG.info(\"\\nsnapshotDiff s0 -> s1:\");\n    LOG.info(hdfs.getSnapshotDiffReport(root, \"s0\", \"s1\").toString());\n    LOG.info(\"\\nsnapshotDiff s1 -> s2:\");\n    LOG.info(hdfs.getSnapshotDiffReport(root, \"s1\", \"s2\").toString());\n\n    assertAtimeEquals(filePreSS, root, \"s0\", \"s1\");\n    assertAtimeEquals(dirPreSS, root, \"s0\", \"s1\");\n\n    assertAtimeEquals(filePreSS, root, \"s1\", \"s2\");\n    assertAtimeEquals(dirPreSS, root, \"s1\", \"s2\");\n\n    assertAtimeEquals(filePostSS, root, \"s1\", \"s2\");\n    assertAtimeEquals(dirPostSS, root, \"s1\", \"s2\");\n\n    // access time should be captured in snapshot due to\n    // other modification\n    assertAtimeNotEquals(filePostSS, root, \"s2\", \"s3\");\n\n    // restart NN, and see the access time relationship\n    // still stands (no change caused by edit logs\n    // loading)\n    cluster.restartNameNodes();\n    cluster.waitActive();\n    assertAtimeEquals(filePreSS, root, \"s0\", \"s1\");\n    assertAtimeEquals(dirPreSS, root, \"s0\", \"s1\");\n\n    assertAtimeEquals(filePreSS, root, \"s1\", \"s2\");\n    assertAtimeEquals(dirPreSS, root, \"s1\", \"s2\");\n\n    assertAtimeEquals(filePostSS, root, \"s1\", \"s2\");\n    assertAtimeEquals(dirPostSS, root, \"s1\", \"s2\");\n\n    assertAtimeNotEquals(filePostSS, root, \"s2\", \"s3\");\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.getSnapshotDiffReport",
            "location": "2445–2478",
            "code": "/**\n   * Get the difference between two snapshots, or between a snapshot and the\n   * current tree of a directory.\n   *\n   * @see DFSClient#getSnapshotDiffReportListing\n   */\n  public SnapshotDiffReport getSnapshotDiffReport(final Path snapshotDir,\n      final String fromSnapshot, final String toSnapshot) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_SNAPSHOT_DIFF);\n    Path absF = fixRelativePart(snapshotDir);\n    return new FileSystemLinkResolver<SnapshotDiffReport>() {\n      @Override\n      public SnapshotDiffReport doCall(final Path p)\n          throws IOException {\n        return getSnapshotDiffReportInternal(getPathName(p), fromSnapshot,\n            toSnapshot);\n      }\n\n      @Override\n      public SnapshotDiffReport next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.getSnapshotDiffReport(p, fromSnapshot, toSnapshot);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + snapshotDir + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "1270–1270",
            "code": "LOG.info(hdfs.getSnapshotDiffReport(root, \"s0\", \"s1\").toString());"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithRenameAndAppend",
            "location": "974–995",
            "code": "/**\n   * Rename a file and then append some data to it\n   */\n  @Test\n  public void testDiffReportWithRenameAndAppend() throws Exception {\n    final Path root = new Path(\"/\");\n    final Path foo = new Path(root, \"foo\");\n    DFSTestUtil.createFile(hdfs, foo, BLOCKSIZE, REPLICATION, SEED);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    final Path bar = new Path(root, \"bar\");\n    hdfs.rename(foo, bar);\n    DFSTestUtil.appendFile(hdfs, bar, 10); // append 10 bytes\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n\n    // we always put modification on the file before rename\n    verifyDiffReport(root, \"s0\", \"s1\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"foo\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"foo\"),\n            DFSUtil.string2Bytes(\"bar\")));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.rename",
            "location": "913–942",
            "code": "@SuppressWarnings(\"deprecation\")\n  @Override\n  public boolean rename(Path src, Path dst) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.RENAME);\n\n    final Path absSrc = fixRelativePart(src);\n    final Path absDst = fixRelativePart(dst);\n\n    // Try the rename without resolving first\n    try {\n      return dfs.rename(getPathName(absSrc), getPathName(absDst));\n    } catch (UnresolvedLinkException e) {\n      // Fully resolve the source\n      final Path source = getFileLinkStatus(absSrc).getPath();\n      // Keep trying to resolve the destination\n      return new FileSystemLinkResolver<Boolean>() {\n        @Override\n        public Boolean doCall(final Path p) throws IOException {\n          return dfs.rename(getPathName(source), getPathName(p));\n        }\n        @Override\n        public Boolean next(final FileSystem fs, final Path p)\n            throws IOException {\n          // Should just throw an error in FileSystem#checkPath\n          return doCall(p);\n        }\n      }.resolve(this, absDst);\n    }\n  }"
          },
          "invocation": {
            "location": "985–985",
            "code": "hdfs.rename(foo, bar);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithRename",
            "location": "835–874",
            "code": "/**\n   * Rename a directory to its prior descendant, and verify the diff report.\n   */\n  @Test\n  public void testDiffReportWithRename() throws Exception {\n    final Path root = new Path(\"/\");\n    final Path sdir1 = new Path(root, \"dir1\");\n    final Path sdir2 = new Path(root, \"dir2\");\n    final Path foo = new Path(sdir1, \"foo\");\n    final Path bar = new Path(foo, \"bar\");\n    hdfs.mkdirs(bar);\n    hdfs.mkdirs(sdir2);\n\n    // create snapshot on root\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n\n    // /dir1/foo/bar -> /dir2/bar\n    final Path bar2 = new Path(sdir2, \"bar\");\n    hdfs.rename(bar, bar2);\n\n    // /dir1/foo -> /dir2/bar/foo\n    final Path foo2 = new Path(bar2, \"foo\");\n    hdfs.rename(foo, foo2);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s2\");\n    // let's delete /dir2 to make things more complicated\n    hdfs.delete(sdir2, true);\n\n    verifyDiffReport(root, \"s1\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/foo\"),\n            DFSUtil.string2Bytes(\"dir2/bar/foo\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir2\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"dir1/foo/bar\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1/foo\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil\n            .string2Bytes(\"dir1/foo/bar\"), DFSUtil.string2Bytes(\"dir2/bar\")));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.delete",
            "location": "996–1012",
            "code": "@Override\n  public boolean delete(Path f, final boolean recursive) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.DELETE);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p) throws IOException {\n        return dfs.delete(getPathName(p), recursive);\n      }\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.delete(p, recursive);\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "861–861",
            "code": "hdfs.delete(sdir2, true);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithRename",
            "location": "835–874",
            "code": "/**\n   * Rename a directory to its prior descendant, and verify the diff report.\n   */\n  @Test\n  public void testDiffReportWithRename() throws Exception {\n    final Path root = new Path(\"/\");\n    final Path sdir1 = new Path(root, \"dir1\");\n    final Path sdir2 = new Path(root, \"dir2\");\n    final Path foo = new Path(sdir1, \"foo\");\n    final Path bar = new Path(foo, \"bar\");\n    hdfs.mkdirs(bar);\n    hdfs.mkdirs(sdir2);\n\n    // create snapshot on root\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n\n    // /dir1/foo/bar -> /dir2/bar\n    final Path bar2 = new Path(sdir2, \"bar\");\n    hdfs.rename(bar, bar2);\n\n    // /dir1/foo -> /dir2/bar/foo\n    final Path foo2 = new Path(bar2, \"foo\");\n    hdfs.rename(foo, foo2);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s2\");\n    // let's delete /dir2 to make things more complicated\n    hdfs.delete(sdir2, true);\n\n    verifyDiffReport(root, \"s1\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/foo\"),\n            DFSUtil.string2Bytes(\"dir2/bar/foo\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir2\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"dir1/foo/bar\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1/foo\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil\n            .string2Bytes(\"dir1/foo/bar\"), DFSUtil.string2Bytes(\"dir2/bar\")));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.rename",
            "location": "913–942",
            "code": "@SuppressWarnings(\"deprecation\")\n  @Override\n  public boolean rename(Path src, Path dst) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.RENAME);\n\n    final Path absSrc = fixRelativePart(src);\n    final Path absDst = fixRelativePart(dst);\n\n    // Try the rename without resolving first\n    try {\n      return dfs.rename(getPathName(absSrc), getPathName(absDst));\n    } catch (UnresolvedLinkException e) {\n      // Fully resolve the source\n      final Path source = getFileLinkStatus(absSrc).getPath();\n      // Keep trying to resolve the destination\n      return new FileSystemLinkResolver<Boolean>() {\n        @Override\n        public Boolean doCall(final Path p) throws IOException {\n          return dfs.rename(getPathName(source), getPathName(p));\n        }\n        @Override\n        public Boolean next(final FileSystem fs, final Path p)\n            throws IOException {\n          // Should just throw an error in FileSystem#checkPath\n          return doCall(p);\n        }\n      }.resolve(this, absDst);\n    }\n  }"
          },
          "invocation": {
            "location": "853–853",
            "code": "hdfs.rename(bar, bar2);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.modifyAndCreateSnapshot",
            "location": "128–190",
            "code": "/**\n   * Create/modify/delete files under a given directory, also create snapshots\n   * of directories.\n   */\n  protected void modifyAndCreateSnapshot(Path modifyDir, Path[] snapshotDirs)\n      throws Exception {\n    Path file10 = new Path(modifyDir, \"file10\");\n    Path file11 = new Path(modifyDir, \"file11\");\n    Path file12 = new Path(modifyDir, \"file12\");\n    Path file13 = new Path(modifyDir, \"file13\");\n    Path link13 = new Path(modifyDir, \"link13\");\n    Path file14 = new Path(modifyDir, \"file14\");\n    Path file15 = new Path(modifyDir, \"file15\");\n    DFSTestUtil.createFile(hdfs, file10, BLOCKSIZE, REPLICATION_1, SEED);\n    DFSTestUtil.createFile(hdfs, file11, BLOCKSIZE, REPLICATION_1, SEED);\n    DFSTestUtil.createFile(hdfs, file12, BLOCKSIZE, REPLICATION_1, SEED);\n    DFSTestUtil.createFile(hdfs, file13, BLOCKSIZE, REPLICATION_1, SEED);\n    // create link13\n    hdfs.createSymlink(file13, link13, false);\n    // create snapshot\n    for (Path snapshotDir : snapshotDirs) {\n      hdfs.allowSnapshot(snapshotDir);\n      hdfs.createSnapshot(snapshotDir, genSnapshotName(snapshotDir));\n    }\n\n    // delete file11\n    hdfs.delete(file11, true);\n    // modify file12\n    hdfs.setReplication(file12, REPLICATION);\n    // modify file13\n    hdfs.setReplication(file13, REPLICATION);\n    // delete link13\n    hdfs.delete(link13, false);\n    // create file14\n    DFSTestUtil.createFile(hdfs, file14, BLOCKSIZE, REPLICATION, SEED);\n    // create file15\n    DFSTestUtil.createFile(hdfs, file15, BLOCKSIZE, REPLICATION, SEED);\n\n    // create snapshot\n    for (Path snapshotDir : snapshotDirs) {\n      hdfs.createSnapshot(snapshotDir, genSnapshotName(snapshotDir));\n    }\n\n    // create file11 again\n    DFSTestUtil.createFile(hdfs, file11, BLOCKSIZE, REPLICATION, SEED);\n    // delete file12\n    hdfs.delete(file12, true);\n    // modify file13\n    hdfs.setReplication(file13, (short) (REPLICATION - 2));\n    // create link13 again\n    hdfs.createSymlink(file13, link13, false);\n    // delete file14\n    hdfs.delete(file14, true);\n    // modify file15\n    hdfs.setReplication(file15, (short) (REPLICATION - 1));\n\n    // create snapshot\n    for (Path snapshotDir : snapshotDirs) {\n      hdfs.createSnapshot(snapshotDir, genSnapshotName(snapshotDir));\n    }\n    // modify file10\n    hdfs.setReplication(file10, (short) (REPLICATION + 1));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.delete",
            "location": "996–1012",
            "code": "@Override\n  public boolean delete(Path f, final boolean recursive) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.DELETE);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p) throws IOException {\n        return dfs.delete(getPathName(p), recursive);\n      }\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.delete(p, recursive);\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "154–154",
            "code": "hdfs.delete(file11, true);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.modifyAndCreateSnapshot",
            "location": "128–190",
            "code": "/**\n   * Create/modify/delete files under a given directory, also create snapshots\n   * of directories.\n   */\n  protected void modifyAndCreateSnapshot(Path modifyDir, Path[] snapshotDirs)\n      throws Exception {\n    Path file10 = new Path(modifyDir, \"file10\");\n    Path file11 = new Path(modifyDir, \"file11\");\n    Path file12 = new Path(modifyDir, \"file12\");\n    Path file13 = new Path(modifyDir, \"file13\");\n    Path link13 = new Path(modifyDir, \"link13\");\n    Path file14 = new Path(modifyDir, \"file14\");\n    Path file15 = new Path(modifyDir, \"file15\");\n    DFSTestUtil.createFile(hdfs, file10, BLOCKSIZE, REPLICATION_1, SEED);\n    DFSTestUtil.createFile(hdfs, file11, BLOCKSIZE, REPLICATION_1, SEED);\n    DFSTestUtil.createFile(hdfs, file12, BLOCKSIZE, REPLICATION_1, SEED);\n    DFSTestUtil.createFile(hdfs, file13, BLOCKSIZE, REPLICATION_1, SEED);\n    // create link13\n    hdfs.createSymlink(file13, link13, false);\n    // create snapshot\n    for (Path snapshotDir : snapshotDirs) {\n      hdfs.allowSnapshot(snapshotDir);\n      hdfs.createSnapshot(snapshotDir, genSnapshotName(snapshotDir));\n    }\n\n    // delete file11\n    hdfs.delete(file11, true);\n    // modify file12\n    hdfs.setReplication(file12, REPLICATION);\n    // modify file13\n    hdfs.setReplication(file13, REPLICATION);\n    // delete link13\n    hdfs.delete(link13, false);\n    // create file14\n    DFSTestUtil.createFile(hdfs, file14, BLOCKSIZE, REPLICATION, SEED);\n    // create file15\n    DFSTestUtil.createFile(hdfs, file15, BLOCKSIZE, REPLICATION, SEED);\n\n    // create snapshot\n    for (Path snapshotDir : snapshotDirs) {\n      hdfs.createSnapshot(snapshotDir, genSnapshotName(snapshotDir));\n    }\n\n    // create file11 again\n    DFSTestUtil.createFile(hdfs, file11, BLOCKSIZE, REPLICATION, SEED);\n    // delete file12\n    hdfs.delete(file12, true);\n    // modify file13\n    hdfs.setReplication(file13, (short) (REPLICATION - 2));\n    // create link13 again\n    hdfs.createSymlink(file13, link13, false);\n    // delete file14\n    hdfs.delete(file14, true);\n    // modify file15\n    hdfs.setReplication(file15, (short) (REPLICATION - 1));\n\n    // create snapshot\n    for (Path snapshotDir : snapshotDirs) {\n      hdfs.createSnapshot(snapshotDir, genSnapshotName(snapshotDir));\n    }\n    // modify file10\n    hdfs.setReplication(file10, (short) (REPLICATION + 1));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.setReplication",
            "location": "740–757",
            "code": "@Override\n  public boolean setReplication(Path src, final short replication)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.SET_REPLICATION);\n    Path absF = fixRelativePart(src);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p) throws IOException {\n        return dfs.setReplication(getPathName(p), replication);\n      }\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.setReplication(p, replication);\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "156–156",
            "code": "hdfs.setReplication(file12, REPLICATION);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.modifyAndCreateSnapshot",
            "location": "128–190",
            "code": "/**\n   * Create/modify/delete files under a given directory, also create snapshots\n   * of directories.\n   */\n  protected void modifyAndCreateSnapshot(Path modifyDir, Path[] snapshotDirs)\n      throws Exception {\n    Path file10 = new Path(modifyDir, \"file10\");\n    Path file11 = new Path(modifyDir, \"file11\");\n    Path file12 = new Path(modifyDir, \"file12\");\n    Path file13 = new Path(modifyDir, \"file13\");\n    Path link13 = new Path(modifyDir, \"link13\");\n    Path file14 = new Path(modifyDir, \"file14\");\n    Path file15 = new Path(modifyDir, \"file15\");\n    DFSTestUtil.createFile(hdfs, file10, BLOCKSIZE, REPLICATION_1, SEED);\n    DFSTestUtil.createFile(hdfs, file11, BLOCKSIZE, REPLICATION_1, SEED);\n    DFSTestUtil.createFile(hdfs, file12, BLOCKSIZE, REPLICATION_1, SEED);\n    DFSTestUtil.createFile(hdfs, file13, BLOCKSIZE, REPLICATION_1, SEED);\n    // create link13\n    hdfs.createSymlink(file13, link13, false);\n    // create snapshot\n    for (Path snapshotDir : snapshotDirs) {\n      hdfs.allowSnapshot(snapshotDir);\n      hdfs.createSnapshot(snapshotDir, genSnapshotName(snapshotDir));\n    }\n\n    // delete file11\n    hdfs.delete(file11, true);\n    // modify file12\n    hdfs.setReplication(file12, REPLICATION);\n    // modify file13\n    hdfs.setReplication(file13, REPLICATION);\n    // delete link13\n    hdfs.delete(link13, false);\n    // create file14\n    DFSTestUtil.createFile(hdfs, file14, BLOCKSIZE, REPLICATION, SEED);\n    // create file15\n    DFSTestUtil.createFile(hdfs, file15, BLOCKSIZE, REPLICATION, SEED);\n\n    // create snapshot\n    for (Path snapshotDir : snapshotDirs) {\n      hdfs.createSnapshot(snapshotDir, genSnapshotName(snapshotDir));\n    }\n\n    // create file11 again\n    DFSTestUtil.createFile(hdfs, file11, BLOCKSIZE, REPLICATION, SEED);\n    // delete file12\n    hdfs.delete(file12, true);\n    // modify file13\n    hdfs.setReplication(file13, (short) (REPLICATION - 2));\n    // create link13 again\n    hdfs.createSymlink(file13, link13, false);\n    // delete file14\n    hdfs.delete(file14, true);\n    // modify file15\n    hdfs.setReplication(file15, (short) (REPLICATION - 1));\n\n    // create snapshot\n    for (Path snapshotDir : snapshotDirs) {\n      hdfs.createSnapshot(snapshotDir, genSnapshotName(snapshotDir));\n    }\n    // modify file10\n    hdfs.setReplication(file10, (short) (REPLICATION + 1));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.createSnapshot",
            "location": "2233–2258",
            "code": "@Override\n  public Path createSnapshot(final Path path, final String snapshotName)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.CREATE_SNAPSHOT);\n    Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<Path>() {\n      @Override\n      public Path doCall(final Path p) throws IOException {\n        return new Path(dfs.createSnapshot(getPathName(p), snapshotName));\n      }\n\n      @Override\n      public Path next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          return myDfs.createSnapshot(p);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + path + \" -> \" + p);\n        }\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "150–150",
            "code": "hdfs.createSnapshot(snapshotDir, genSnapshotName(snapshotDir));"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.modifyAndCreateSnapshot",
            "location": "128–190",
            "code": "/**\n   * Create/modify/delete files under a given directory, also create snapshots\n   * of directories.\n   */\n  protected void modifyAndCreateSnapshot(Path modifyDir, Path[] snapshotDirs)\n      throws Exception {\n    Path file10 = new Path(modifyDir, \"file10\");\n    Path file11 = new Path(modifyDir, \"file11\");\n    Path file12 = new Path(modifyDir, \"file12\");\n    Path file13 = new Path(modifyDir, \"file13\");\n    Path link13 = new Path(modifyDir, \"link13\");\n    Path file14 = new Path(modifyDir, \"file14\");\n    Path file15 = new Path(modifyDir, \"file15\");\n    DFSTestUtil.createFile(hdfs, file10, BLOCKSIZE, REPLICATION_1, SEED);\n    DFSTestUtil.createFile(hdfs, file11, BLOCKSIZE, REPLICATION_1, SEED);\n    DFSTestUtil.createFile(hdfs, file12, BLOCKSIZE, REPLICATION_1, SEED);\n    DFSTestUtil.createFile(hdfs, file13, BLOCKSIZE, REPLICATION_1, SEED);\n    // create link13\n    hdfs.createSymlink(file13, link13, false);\n    // create snapshot\n    for (Path snapshotDir : snapshotDirs) {\n      hdfs.allowSnapshot(snapshotDir);\n      hdfs.createSnapshot(snapshotDir, genSnapshotName(snapshotDir));\n    }\n\n    // delete file11\n    hdfs.delete(file11, true);\n    // modify file12\n    hdfs.setReplication(file12, REPLICATION);\n    // modify file13\n    hdfs.setReplication(file13, REPLICATION);\n    // delete link13\n    hdfs.delete(link13, false);\n    // create file14\n    DFSTestUtil.createFile(hdfs, file14, BLOCKSIZE, REPLICATION, SEED);\n    // create file15\n    DFSTestUtil.createFile(hdfs, file15, BLOCKSIZE, REPLICATION, SEED);\n\n    // create snapshot\n    for (Path snapshotDir : snapshotDirs) {\n      hdfs.createSnapshot(snapshotDir, genSnapshotName(snapshotDir));\n    }\n\n    // create file11 again\n    DFSTestUtil.createFile(hdfs, file11, BLOCKSIZE, REPLICATION, SEED);\n    // delete file12\n    hdfs.delete(file12, true);\n    // modify file13\n    hdfs.setReplication(file13, (short) (REPLICATION - 2));\n    // create link13 again\n    hdfs.createSymlink(file13, link13, false);\n    // delete file14\n    hdfs.delete(file14, true);\n    // modify file15\n    hdfs.setReplication(file15, (short) (REPLICATION - 1));\n\n    // create snapshot\n    for (Path snapshotDir : snapshotDirs) {\n      hdfs.createSnapshot(snapshotDir, genSnapshotName(snapshotDir));\n    }\n    // modify file10\n    hdfs.setReplication(file10, (short) (REPLICATION + 1));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.createSymlink",
            "location": "1855–1877",
            "code": "@SuppressWarnings(\"deprecation\")\n  @Override\n  public void createSymlink(final Path target, final Path link,\n      final boolean createParent) throws IOException {\n    if (!FileSystem.areSymlinksEnabled()) {\n      throw new UnsupportedOperationException(\"Symlinks not supported\");\n    }\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.CREATE_SYM_LINK);\n    final Path absF = fixRelativePart(link);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.createSymlink(target.toString(), getPathName(p), createParent);\n        return null;\n      }\n      @Override\n      public Void next(final FileSystem fs, final Path p) throws IOException {\n        fs.createSymlink(target, p, createParent);\n        return null;\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "146–146",
            "code": "hdfs.createSymlink(file13, link13, false);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.modifyAndCreateSnapshot",
            "location": "128–190",
            "code": "/**\n   * Create/modify/delete files under a given directory, also create snapshots\n   * of directories.\n   */\n  protected void modifyAndCreateSnapshot(Path modifyDir, Path[] snapshotDirs)\n      throws Exception {\n    Path file10 = new Path(modifyDir, \"file10\");\n    Path file11 = new Path(modifyDir, \"file11\");\n    Path file12 = new Path(modifyDir, \"file12\");\n    Path file13 = new Path(modifyDir, \"file13\");\n    Path link13 = new Path(modifyDir, \"link13\");\n    Path file14 = new Path(modifyDir, \"file14\");\n    Path file15 = new Path(modifyDir, \"file15\");\n    DFSTestUtil.createFile(hdfs, file10, BLOCKSIZE, REPLICATION_1, SEED);\n    DFSTestUtil.createFile(hdfs, file11, BLOCKSIZE, REPLICATION_1, SEED);\n    DFSTestUtil.createFile(hdfs, file12, BLOCKSIZE, REPLICATION_1, SEED);\n    DFSTestUtil.createFile(hdfs, file13, BLOCKSIZE, REPLICATION_1, SEED);\n    // create link13\n    hdfs.createSymlink(file13, link13, false);\n    // create snapshot\n    for (Path snapshotDir : snapshotDirs) {\n      hdfs.allowSnapshot(snapshotDir);\n      hdfs.createSnapshot(snapshotDir, genSnapshotName(snapshotDir));\n    }\n\n    // delete file11\n    hdfs.delete(file11, true);\n    // modify file12\n    hdfs.setReplication(file12, REPLICATION);\n    // modify file13\n    hdfs.setReplication(file13, REPLICATION);\n    // delete link13\n    hdfs.delete(link13, false);\n    // create file14\n    DFSTestUtil.createFile(hdfs, file14, BLOCKSIZE, REPLICATION, SEED);\n    // create file15\n    DFSTestUtil.createFile(hdfs, file15, BLOCKSIZE, REPLICATION, SEED);\n\n    // create snapshot\n    for (Path snapshotDir : snapshotDirs) {\n      hdfs.createSnapshot(snapshotDir, genSnapshotName(snapshotDir));\n    }\n\n    // create file11 again\n    DFSTestUtil.createFile(hdfs, file11, BLOCKSIZE, REPLICATION, SEED);\n    // delete file12\n    hdfs.delete(file12, true);\n    // modify file13\n    hdfs.setReplication(file13, (short) (REPLICATION - 2));\n    // create link13 again\n    hdfs.createSymlink(file13, link13, false);\n    // delete file14\n    hdfs.delete(file14, true);\n    // modify file15\n    hdfs.setReplication(file15, (short) (REPLICATION - 1));\n\n    // create snapshot\n    for (Path snapshotDir : snapshotDirs) {\n      hdfs.createSnapshot(snapshotDir, genSnapshotName(snapshotDir));\n    }\n    // modify file10\n    hdfs.setReplication(file10, (short) (REPLICATION + 1));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.allowSnapshot",
            "location": "2133–2159",
            "code": "/** @see org.apache.hadoop.hdfs.client.HdfsAdmin#allowSnapshot(Path) */\n  public void allowSnapshot(final Path path) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.ALLOW_SNAPSHOT);\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.allowSnapshot(getPathName(p));\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.allowSnapshot(p);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + path + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "149–149",
            "code": "hdfs.allowSnapshot(snapshotDir);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithRpcLimit3",
            "location": "1419–1469",
            "code": "/**\n   * Tests to verify the diff report with maximum SnapsdiffReportEntries limit\n   * over an rpc being set to 3.\n   * @throws Exception\n   */\n  @Test\n  public void testDiffReportWithRpcLimit3() throws Exception {\n    final Path root = new Path(\"/\");\n    hdfs.mkdirs(root);\n    Path path = new Path(root, \"dir1\");\n    hdfs.mkdirs(path);\n    for (int j = 1; j <= 4; j++) {\n      final Path file = new Path(path, \"file\" + j);\n      DFSTestUtil.createFile(hdfs, file, BLOCKSIZE, REPLICATION, SEED);\n    }\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    path = new Path(root, \"dir1\");\n    for (int j = 1; j <= 4; j++) {\n      final Path file = new Path(path, \"file\" + j);\n      hdfs.delete(file, false);\n    }\n    for (int j = 5; j <= 10; j++) {\n      final Path file = new Path(path, \"file\" + j);\n      DFSTestUtil.createFile(hdfs, file, BLOCKSIZE, REPLICATION, SEED);\n    }\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n    verifyDiffReport(root, \"s0\", \"s1\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir1/file5\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir1/file6\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir1/file7\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir1/file8\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir1/file9\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir1/file10\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir1/file1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir1/file2\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir1/file3\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir1/file4\")));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.delete",
            "location": "996–1012",
            "code": "@Override\n  public boolean delete(Path f, final boolean recursive) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.DELETE);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p) throws IOException {\n        return dfs.delete(getPathName(p), recursive);\n      }\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.delete(p, recursive);\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "1438–1438",
            "code": "hdfs.delete(file, false);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithQuota",
            "location": "817–833",
            "code": "@Test\n  public void testDiffReportWithQuota() throws Exception {\n    final Path testdir = new Path(sub1, \"testdir1\");\n    hdfs.mkdirs(testdir);\n    hdfs.allowSnapshot(testdir);\n    // Set quota BEFORE creating the snapshot\n    hdfs.setQuota(testdir, 10, 10);\n    hdfs.createSnapshot(testdir, \"s0\");\n    final SnapshotDiffReport report =\n        hdfs.getSnapshotDiffReport(testdir, \"s0\", \"\");\n    // The diff should be null. Snapshot dir inode should keep the quota.\n    Assert.assertEquals(0, report.getDiffList().size());\n    // Cleanup\n    hdfs.deleteSnapshot(testdir, \"s0\");\n    hdfs.disallowSnapshot(testdir);\n    hdfs.delete(testdir, true);\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.deleteSnapshot",
            "location": "2317–2344",
            "code": "@Override\n  public void deleteSnapshot(final Path snapshotDir, final String snapshotName)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.DELETE_SNAPSHOT);\n    Path absF = fixRelativePart(snapshotDir);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.deleteSnapshot(getPathName(p), snapshotName);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.deleteSnapshot(p, snapshotName);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + snapshotDir + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "830–830",
            "code": "hdfs.deleteSnapshot(testdir, \"s0\");"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithQuota",
            "location": "817–833",
            "code": "@Test\n  public void testDiffReportWithQuota() throws Exception {\n    final Path testdir = new Path(sub1, \"testdir1\");\n    hdfs.mkdirs(testdir);\n    hdfs.allowSnapshot(testdir);\n    // Set quota BEFORE creating the snapshot\n    hdfs.setQuota(testdir, 10, 10);\n    hdfs.createSnapshot(testdir, \"s0\");\n    final SnapshotDiffReport report =\n        hdfs.getSnapshotDiffReport(testdir, \"s0\", \"\");\n    // The diff should be null. Snapshot dir inode should keep the quota.\n    Assert.assertEquals(0, report.getDiffList().size());\n    // Cleanup\n    hdfs.deleteSnapshot(testdir, \"s0\");\n    hdfs.disallowSnapshot(testdir);\n    hdfs.delete(testdir, true);\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.delete",
            "location": "996–1012",
            "code": "@Override\n  public boolean delete(Path f, final boolean recursive) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.DELETE);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p) throws IOException {\n        return dfs.delete(getPathName(p), recursive);\n      }\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.delete(p, recursive);\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "832–832",
            "code": "hdfs.delete(testdir, true);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithQuota",
            "location": "817–833",
            "code": "@Test\n  public void testDiffReportWithQuota() throws Exception {\n    final Path testdir = new Path(sub1, \"testdir1\");\n    hdfs.mkdirs(testdir);\n    hdfs.allowSnapshot(testdir);\n    // Set quota BEFORE creating the snapshot\n    hdfs.setQuota(testdir, 10, 10);\n    hdfs.createSnapshot(testdir, \"s0\");\n    final SnapshotDiffReport report =\n        hdfs.getSnapshotDiffReport(testdir, \"s0\", \"\");\n    // The diff should be null. Snapshot dir inode should keep the quota.\n    Assert.assertEquals(0, report.getDiffList().size());\n    // Cleanup\n    hdfs.deleteSnapshot(testdir, \"s0\");\n    hdfs.disallowSnapshot(testdir);\n    hdfs.delete(testdir, true);\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.disallowSnapshot",
            "location": "2161–2189",
            "code": "/** @see org.apache.hadoop.hdfs.client.HdfsAdmin#disallowSnapshot(Path) */\n  public void disallowSnapshot(final Path path) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.DISALLOW_SNAPSHOT);\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        checkTrashRootAndRemoveIfEmpty(p);\n        dfs.disallowSnapshot(getPathName(p));\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.checkTrashRootAndRemoveIfEmpty(p);\n          myDfs.disallowSnapshot(p);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + path + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "831–831",
            "code": "hdfs.disallowSnapshot(testdir);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithQuota",
            "location": "817–833",
            "code": "@Test\n  public void testDiffReportWithQuota() throws Exception {\n    final Path testdir = new Path(sub1, \"testdir1\");\n    hdfs.mkdirs(testdir);\n    hdfs.allowSnapshot(testdir);\n    // Set quota BEFORE creating the snapshot\n    hdfs.setQuota(testdir, 10, 10);\n    hdfs.createSnapshot(testdir, \"s0\");\n    final SnapshotDiffReport report =\n        hdfs.getSnapshotDiffReport(testdir, \"s0\", \"\");\n    // The diff should be null. Snapshot dir inode should keep the quota.\n    Assert.assertEquals(0, report.getDiffList().size());\n    // Cleanup\n    hdfs.deleteSnapshot(testdir, \"s0\");\n    hdfs.disallowSnapshot(testdir);\n    hdfs.delete(testdir, true);\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.createSnapshot",
            "location": "2233–2258",
            "code": "@Override\n  public Path createSnapshot(final Path path, final String snapshotName)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.CREATE_SNAPSHOT);\n    Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<Path>() {\n      @Override\n      public Path doCall(final Path p) throws IOException {\n        return new Path(dfs.createSnapshot(getPathName(p), snapshotName));\n      }\n\n      @Override\n      public Path next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          return myDfs.createSnapshot(p);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + path + \" -> \" + p);\n        }\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "824–824",
            "code": "hdfs.createSnapshot(testdir, \"s0\");"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithQuota",
            "location": "817–833",
            "code": "@Test\n  public void testDiffReportWithQuota() throws Exception {\n    final Path testdir = new Path(sub1, \"testdir1\");\n    hdfs.mkdirs(testdir);\n    hdfs.allowSnapshot(testdir);\n    // Set quota BEFORE creating the snapshot\n    hdfs.setQuota(testdir, 10, 10);\n    hdfs.createSnapshot(testdir, \"s0\");\n    final SnapshotDiffReport report =\n        hdfs.getSnapshotDiffReport(testdir, \"s0\", \"\");\n    // The diff should be null. Snapshot dir inode should keep the quota.\n    Assert.assertEquals(0, report.getDiffList().size());\n    // Cleanup\n    hdfs.deleteSnapshot(testdir, \"s0\");\n    hdfs.disallowSnapshot(testdir);\n    hdfs.delete(testdir, true);\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.getSnapshotDiffReport",
            "location": "2445–2478",
            "code": "/**\n   * Get the difference between two snapshots, or between a snapshot and the\n   * current tree of a directory.\n   *\n   * @see DFSClient#getSnapshotDiffReportListing\n   */\n  public SnapshotDiffReport getSnapshotDiffReport(final Path snapshotDir,\n      final String fromSnapshot, final String toSnapshot) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_SNAPSHOT_DIFF);\n    Path absF = fixRelativePart(snapshotDir);\n    return new FileSystemLinkResolver<SnapshotDiffReport>() {\n      @Override\n      public SnapshotDiffReport doCall(final Path p)\n          throws IOException {\n        return getSnapshotDiffReportInternal(getPathName(p), fromSnapshot,\n            toSnapshot);\n      }\n\n      @Override\n      public SnapshotDiffReport next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.getSnapshotDiffReport(p, fromSnapshot, toSnapshot);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + snapshotDir + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "826–826",
            "code": "hdfs.getSnapshotDiffReport(testdir, \"s0\", \"\");"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithQuota",
            "location": "817–833",
            "code": "@Test\n  public void testDiffReportWithQuota() throws Exception {\n    final Path testdir = new Path(sub1, \"testdir1\");\n    hdfs.mkdirs(testdir);\n    hdfs.allowSnapshot(testdir);\n    // Set quota BEFORE creating the snapshot\n    hdfs.setQuota(testdir, 10, 10);\n    hdfs.createSnapshot(testdir, \"s0\");\n    final SnapshotDiffReport report =\n        hdfs.getSnapshotDiffReport(testdir, \"s0\", \"\");\n    // The diff should be null. Snapshot dir inode should keep the quota.\n    Assert.assertEquals(0, report.getDiffList().size());\n    // Cleanup\n    hdfs.deleteSnapshot(testdir, \"s0\");\n    hdfs.disallowSnapshot(testdir);\n    hdfs.delete(testdir, true);\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.allowSnapshot",
            "location": "2133–2159",
            "code": "/** @see org.apache.hadoop.hdfs.client.HdfsAdmin#allowSnapshot(Path) */\n  public void allowSnapshot(final Path path) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.ALLOW_SNAPSHOT);\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.allowSnapshot(getPathName(p));\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.allowSnapshot(p);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + path + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "821–821",
            "code": "hdfs.allowSnapshot(testdir);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithQuota",
            "location": "817–833",
            "code": "@Test\n  public void testDiffReportWithQuota() throws Exception {\n    final Path testdir = new Path(sub1, \"testdir1\");\n    hdfs.mkdirs(testdir);\n    hdfs.allowSnapshot(testdir);\n    // Set quota BEFORE creating the snapshot\n    hdfs.setQuota(testdir, 10, 10);\n    hdfs.createSnapshot(testdir, \"s0\");\n    final SnapshotDiffReport report =\n        hdfs.getSnapshotDiffReport(testdir, \"s0\", \"\");\n    // The diff should be null. Snapshot dir inode should keep the quota.\n    Assert.assertEquals(0, report.getDiffList().size());\n    // Cleanup\n    hdfs.deleteSnapshot(testdir, \"s0\");\n    hdfs.disallowSnapshot(testdir);\n    hdfs.delete(testdir, true);\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.setQuota",
            "location": "1051–1075",
            "code": "/** Set a directory's quotas\n   * @see org.apache.hadoop.hdfs.protocol.ClientProtocol#setQuota(String,\n   * long, long, StorageType)\n   */\n  @Override\n  public void setQuota(Path src, final long namespaceQuota,\n      final long storagespaceQuota) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.SET_QUOTA_USAGE);\n    Path absF = fixRelativePart(src);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.setQuota(getPathName(p), namespaceQuota, storagespaceQuota);\n        return null;\n      }\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        // setQuota is not defined in FileSystem, so we only can resolve\n        // within this DFS\n        return doCall(p);\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "823–823",
            "code": "hdfs.setQuota(testdir, 10, 10);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithRenameOutside",
            "location": "876–909",
            "code": "/**\n   * Rename a file/dir outside of the snapshottable dir should be reported as\n   * deleted. Rename a file/dir from outside should be reported as created.\n   */\n  @Test\n  public void testDiffReportWithRenameOutside() throws Exception {\n    final Path root = new Path(\"/\");\n    final Path dir1 = new Path(root, \"dir1\");\n    final Path dir2 = new Path(root, \"dir2\");\n    final Path foo = new Path(dir1, \"foo\");\n    final Path fileInFoo = new Path(foo, \"file\");\n    final Path bar = new Path(dir2, \"bar\");\n    final Path fileInBar = new Path(bar, \"file\");\n    DFSTestUtil.createFile(hdfs, fileInFoo, BLOCKSIZE, REPLICATION, SEED);\n    DFSTestUtil.createFile(hdfs, fileInBar, BLOCKSIZE, REPLICATION, SEED);\n\n    // create snapshot on /dir1\n    SnapshotTestHelper.createSnapshot(hdfs, dir1, \"s0\");\n\n    // move bar into dir1\n    final Path newBar = new Path(dir1, \"newBar\");\n    hdfs.rename(bar, newBar);\n    // move foo out of dir1 into dir2\n    final Path newFoo = new Path(dir2, \"new\");\n    hdfs.rename(foo, newFoo);\n\n    SnapshotTestHelper.createSnapshot(hdfs, dir1, \"s1\");\n    verifyDiffReport(dir1, \"s0\", \"s1\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes(newBar\n            .getName())),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(foo.getName())));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.rename",
            "location": "913–942",
            "code": "@SuppressWarnings(\"deprecation\")\n  @Override\n  public boolean rename(Path src, Path dst) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.RENAME);\n\n    final Path absSrc = fixRelativePart(src);\n    final Path absDst = fixRelativePart(dst);\n\n    // Try the rename without resolving first\n    try {\n      return dfs.rename(getPathName(absSrc), getPathName(absDst));\n    } catch (UnresolvedLinkException e) {\n      // Fully resolve the source\n      final Path source = getFileLinkStatus(absSrc).getPath();\n      // Keep trying to resolve the destination\n      return new FileSystemLinkResolver<Boolean>() {\n        @Override\n        public Boolean doCall(final Path p) throws IOException {\n          return dfs.rename(getPathName(source), getPathName(p));\n        }\n        @Override\n        public Boolean next(final FileSystem fs, final Path p)\n            throws IOException {\n          // Should just throw an error in FileSystem#checkPath\n          return doCall(p);\n        }\n      }.resolve(this, absDst);\n    }\n  }"
          },
          "invocation": {
            "location": "897–897",
            "code": "hdfs.rename(bar, newBar);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testSnapRootDescendantDiffReportWithRename",
            "location": "580–728",
            "code": "@Test\n  public void testSnapRootDescendantDiffReportWithRename() throws Exception {\n    Assume.assumeTrue(conf.getBoolean(\n        DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_DIFF_ALLOW_SNAP_ROOT_DESCENDANT,\n        DFSConfigKeys.\n            DFS_NAMENODE_SNAPSHOT_DIFF_ALLOW_SNAP_ROOT_DESCENDANT_DEFAULT));\n    Path subSub = new Path(sub1, \"subsub1\");\n    Path subSubSub = new Path(subSub, \"subsubsub1\");\n    Path nonSnapDir = new Path(dir, \"non_snap\");\n    hdfs.mkdirs(subSubSub);\n    hdfs.mkdirs(nonSnapDir);\n\n    hdfs.allowSnapshot(sub1);\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n    Path file20 = new Path(subSubSub, \"file20\");\n    DFSTestUtil.createFile(hdfs, file20, BLOCKSIZE, REPLICATION_1, SEED);\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // Case 1: Move a file away from a descendant dir, but within the snap root.\n    // mv <snaproot>/<subsub>/<subsubsub>/file20 <snaproot>/<subsub>/file20\n    hdfs.rename(file20, new Path(subSub, file20.getName()));\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // The snapshot diff for the snap root detects the change as file rename\n    // as the file move happened within the snap root.\n    verifyDiffReport(sub1, \"s1\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1\")),\n        new DiffReportEntry(DiffType.RENAME,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file20\"),\n            DFSUtil.string2Bytes(\"subsub1/file20\")));\n\n    // The snapshot diff for the descendant dir <subsub> still detects the\n    // change as file rename as the file move happened under the snap root\n    // descendant dir.\n    verifyDiffReport(subSub, \"s1\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsubsub1\")),\n        new DiffReportEntry(DiffType.RENAME,\n            DFSUtil.string2Bytes(\"subsubsub1/file20\"),\n            DFSUtil.string2Bytes(\"file20\")));\n\n    // The snapshot diff for the descendant dir <subsubsub> detects the\n    // change as file delete as the file got moved from its scope.\n    verifyDiffReport(subSubSub, \"s1\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"file20\")));\n\n    // Case 2: Move the file from the snap root descendant dir to any\n    // non snap root dir. mv <snaproot>/<subsub>/file20 <nonsnaproot>/file20.\n    hdfs.rename(new Path(subSub, file20.getName()),\n        new Path(dir, file20.getName()));\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // The snapshot diff for the snap root detects the change as file delete\n    // as the file got moved away from the snap root dir to some non snap\n    // root dir.\n    verifyDiffReport(sub1, \"s2\", \"s3\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/file20\")));\n\n    // The snapshot diff for the snap root descendant <subsub> detects the\n    // change as file delete as the file was previously under its scope and\n    // got moved away from its scope.\n    verifyDiffReport(subSub, \"s2\", \"s3\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"file20\")));\n\n    // The file was already not under the descendant dir <subsubsub> scope.\n    // So, the snapshot diff report for the descendant dir doesn't\n    // show the file rename at all.\n    verifyDiffReport(subSubSub, \"s2\", \"s3\",\n        new DiffReportEntry[]{});\n\n    // Case 3: Move the file from the non-snap root dir to snap root dir\n    // mv <nonsnaproot>/file20 <snaproot>/file20\n    hdfs.rename(new Path(dir, file20.getName()),\n        new Path(sub1, file20.getName()));\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // Snap root directory should show the file moved in as a new file.\n    verifyDiffReport(sub1, \"s3\", \"s4\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file20\")));\n\n    // Snap descendant directories don't have visibility to the moved in file.\n    verifyDiffReport(subSub, \"s3\", \"s4\",\n        new DiffReportEntry[]{});\n    verifyDiffReport(subSubSub, \"s3\", \"s4\",\n        new DiffReportEntry[]{});\n\n    hdfs.rename(new Path(sub1, file20.getName()),\n        new Path(subSub, file20.getName()));\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // Snap root directory now shows the rename as both source and\n    // destination paths are under the snap root.\n    verifyDiffReport(sub1, \"s4\", \"s5\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.RENAME,\n            DFSUtil.string2Bytes(\"file20\"),\n            DFSUtil.string2Bytes(\"subsub1/file20\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")));\n\n    // For the descendant directory under the snap root, the file\n    // moved in shows up as a new file created.\n    verifyDiffReport(subSub, \"s4\", \"s5\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file20\")));\n\n    verifyDiffReport(subSubSub, \"s4\", \"s5\",\n        new DiffReportEntry[]{});\n\n    // Case 4: Snapshot diff for the newly created descendant directory.\n    Path subSubSub2 = new Path(subSub, \"subsubsub2\");\n    hdfs.mkdirs(subSubSub2);\n    Path file30 = new Path(subSubSub2, \"file30\");\n    DFSTestUtil.createFile(hdfs, file30, BLOCKSIZE, REPLICATION_1, SEED);\n    hdfs.createFile(file30);\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    verifyDiffReport(sub1, \"s5\", \"s6\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub2\")));\n\n    verifyDiffReport(subSubSub2, \"s5\", \"s6\",\n        new DiffReportEntry[]{});\n\n    verifyDiffReport(subSubSub2, \"s1\", \"s2\",\n        new DiffReportEntry[]{});\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.createFile",
            "location": "3885–3895",
            "code": "/**\n   * Create a HdfsDataOutputStreamBuilder to create a file on DFS.\n   * Similar to {@link #create(Path)}, file is overwritten by default.\n   *\n   * @param path the path of the file to create.\n   * @return A HdfsDataOutputStreamBuilder for creating a file.\n   */\n  @Override\n  public HdfsDataOutputStreamBuilder createFile(Path path) {\n    return new HdfsDataOutputStreamBuilder(this, path).create().overwrite(true);\n  }"
          },
          "invocation": {
            "location": "714–714",
            "code": "hdfs.createFile(file30);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testSnapRootDescendantDiffReportWithRename",
            "location": "580–728",
            "code": "@Test\n  public void testSnapRootDescendantDiffReportWithRename() throws Exception {\n    Assume.assumeTrue(conf.getBoolean(\n        DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_DIFF_ALLOW_SNAP_ROOT_DESCENDANT,\n        DFSConfigKeys.\n            DFS_NAMENODE_SNAPSHOT_DIFF_ALLOW_SNAP_ROOT_DESCENDANT_DEFAULT));\n    Path subSub = new Path(sub1, \"subsub1\");\n    Path subSubSub = new Path(subSub, \"subsubsub1\");\n    Path nonSnapDir = new Path(dir, \"non_snap\");\n    hdfs.mkdirs(subSubSub);\n    hdfs.mkdirs(nonSnapDir);\n\n    hdfs.allowSnapshot(sub1);\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n    Path file20 = new Path(subSubSub, \"file20\");\n    DFSTestUtil.createFile(hdfs, file20, BLOCKSIZE, REPLICATION_1, SEED);\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // Case 1: Move a file away from a descendant dir, but within the snap root.\n    // mv <snaproot>/<subsub>/<subsubsub>/file20 <snaproot>/<subsub>/file20\n    hdfs.rename(file20, new Path(subSub, file20.getName()));\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // The snapshot diff for the snap root detects the change as file rename\n    // as the file move happened within the snap root.\n    verifyDiffReport(sub1, \"s1\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1\")),\n        new DiffReportEntry(DiffType.RENAME,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file20\"),\n            DFSUtil.string2Bytes(\"subsub1/file20\")));\n\n    // The snapshot diff for the descendant dir <subsub> still detects the\n    // change as file rename as the file move happened under the snap root\n    // descendant dir.\n    verifyDiffReport(subSub, \"s1\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsubsub1\")),\n        new DiffReportEntry(DiffType.RENAME,\n            DFSUtil.string2Bytes(\"subsubsub1/file20\"),\n            DFSUtil.string2Bytes(\"file20\")));\n\n    // The snapshot diff for the descendant dir <subsubsub> detects the\n    // change as file delete as the file got moved from its scope.\n    verifyDiffReport(subSubSub, \"s1\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"file20\")));\n\n    // Case 2: Move the file from the snap root descendant dir to any\n    // non snap root dir. mv <snaproot>/<subsub>/file20 <nonsnaproot>/file20.\n    hdfs.rename(new Path(subSub, file20.getName()),\n        new Path(dir, file20.getName()));\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // The snapshot diff for the snap root detects the change as file delete\n    // as the file got moved away from the snap root dir to some non snap\n    // root dir.\n    verifyDiffReport(sub1, \"s2\", \"s3\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/file20\")));\n\n    // The snapshot diff for the snap root descendant <subsub> detects the\n    // change as file delete as the file was previously under its scope and\n    // got moved away from its scope.\n    verifyDiffReport(subSub, \"s2\", \"s3\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"file20\")));\n\n    // The file was already not under the descendant dir <subsubsub> scope.\n    // So, the snapshot diff report for the descendant dir doesn't\n    // show the file rename at all.\n    verifyDiffReport(subSubSub, \"s2\", \"s3\",\n        new DiffReportEntry[]{});\n\n    // Case 3: Move the file from the non-snap root dir to snap root dir\n    // mv <nonsnaproot>/file20 <snaproot>/file20\n    hdfs.rename(new Path(dir, file20.getName()),\n        new Path(sub1, file20.getName()));\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // Snap root directory should show the file moved in as a new file.\n    verifyDiffReport(sub1, \"s3\", \"s4\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file20\")));\n\n    // Snap descendant directories don't have visibility to the moved in file.\n    verifyDiffReport(subSub, \"s3\", \"s4\",\n        new DiffReportEntry[]{});\n    verifyDiffReport(subSubSub, \"s3\", \"s4\",\n        new DiffReportEntry[]{});\n\n    hdfs.rename(new Path(sub1, file20.getName()),\n        new Path(subSub, file20.getName()));\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // Snap root directory now shows the rename as both source and\n    // destination paths are under the snap root.\n    verifyDiffReport(sub1, \"s4\", \"s5\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.RENAME,\n            DFSUtil.string2Bytes(\"file20\"),\n            DFSUtil.string2Bytes(\"subsub1/file20\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")));\n\n    // For the descendant directory under the snap root, the file\n    // moved in shows up as a new file created.\n    verifyDiffReport(subSub, \"s4\", \"s5\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file20\")));\n\n    verifyDiffReport(subSubSub, \"s4\", \"s5\",\n        new DiffReportEntry[]{});\n\n    // Case 4: Snapshot diff for the newly created descendant directory.\n    Path subSubSub2 = new Path(subSub, \"subsubsub2\");\n    hdfs.mkdirs(subSubSub2);\n    Path file30 = new Path(subSubSub2, \"file30\");\n    DFSTestUtil.createFile(hdfs, file30, BLOCKSIZE, REPLICATION_1, SEED);\n    hdfs.createFile(file30);\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    verifyDiffReport(sub1, \"s5\", \"s6\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub2\")));\n\n    verifyDiffReport(subSubSub2, \"s5\", \"s6\",\n        new DiffReportEntry[]{});\n\n    verifyDiffReport(subSubSub2, \"s1\", \"s2\",\n        new DiffReportEntry[]{});\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.createSnapshot",
            "location": "2233–2258",
            "code": "@Override\n  public Path createSnapshot(final Path path, final String snapshotName)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.CREATE_SNAPSHOT);\n    Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<Path>() {\n      @Override\n      public Path doCall(final Path p) throws IOException {\n        return new Path(dfs.createSnapshot(getPathName(p), snapshotName));\n      }\n\n      @Override\n      public Path next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          return myDfs.createSnapshot(p);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + path + \" -> \" + p);\n        }\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "593–593",
            "code": "hdfs.createSnapshot(sub1, genSnapshotName(sub1));"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testSnapRootDescendantDiffReportWithRename",
            "location": "580–728",
            "code": "@Test\n  public void testSnapRootDescendantDiffReportWithRename() throws Exception {\n    Assume.assumeTrue(conf.getBoolean(\n        DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_DIFF_ALLOW_SNAP_ROOT_DESCENDANT,\n        DFSConfigKeys.\n            DFS_NAMENODE_SNAPSHOT_DIFF_ALLOW_SNAP_ROOT_DESCENDANT_DEFAULT));\n    Path subSub = new Path(sub1, \"subsub1\");\n    Path subSubSub = new Path(subSub, \"subsubsub1\");\n    Path nonSnapDir = new Path(dir, \"non_snap\");\n    hdfs.mkdirs(subSubSub);\n    hdfs.mkdirs(nonSnapDir);\n\n    hdfs.allowSnapshot(sub1);\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n    Path file20 = new Path(subSubSub, \"file20\");\n    DFSTestUtil.createFile(hdfs, file20, BLOCKSIZE, REPLICATION_1, SEED);\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // Case 1: Move a file away from a descendant dir, but within the snap root.\n    // mv <snaproot>/<subsub>/<subsubsub>/file20 <snaproot>/<subsub>/file20\n    hdfs.rename(file20, new Path(subSub, file20.getName()));\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // The snapshot diff for the snap root detects the change as file rename\n    // as the file move happened within the snap root.\n    verifyDiffReport(sub1, \"s1\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1\")),\n        new DiffReportEntry(DiffType.RENAME,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file20\"),\n            DFSUtil.string2Bytes(\"subsub1/file20\")));\n\n    // The snapshot diff for the descendant dir <subsub> still detects the\n    // change as file rename as the file move happened under the snap root\n    // descendant dir.\n    verifyDiffReport(subSub, \"s1\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsubsub1\")),\n        new DiffReportEntry(DiffType.RENAME,\n            DFSUtil.string2Bytes(\"subsubsub1/file20\"),\n            DFSUtil.string2Bytes(\"file20\")));\n\n    // The snapshot diff for the descendant dir <subsubsub> detects the\n    // change as file delete as the file got moved from its scope.\n    verifyDiffReport(subSubSub, \"s1\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"file20\")));\n\n    // Case 2: Move the file from the snap root descendant dir to any\n    // non snap root dir. mv <snaproot>/<subsub>/file20 <nonsnaproot>/file20.\n    hdfs.rename(new Path(subSub, file20.getName()),\n        new Path(dir, file20.getName()));\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // The snapshot diff for the snap root detects the change as file delete\n    // as the file got moved away from the snap root dir to some non snap\n    // root dir.\n    verifyDiffReport(sub1, \"s2\", \"s3\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/file20\")));\n\n    // The snapshot diff for the snap root descendant <subsub> detects the\n    // change as file delete as the file was previously under its scope and\n    // got moved away from its scope.\n    verifyDiffReport(subSub, \"s2\", \"s3\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"file20\")));\n\n    // The file was already not under the descendant dir <subsubsub> scope.\n    // So, the snapshot diff report for the descendant dir doesn't\n    // show the file rename at all.\n    verifyDiffReport(subSubSub, \"s2\", \"s3\",\n        new DiffReportEntry[]{});\n\n    // Case 3: Move the file from the non-snap root dir to snap root dir\n    // mv <nonsnaproot>/file20 <snaproot>/file20\n    hdfs.rename(new Path(dir, file20.getName()),\n        new Path(sub1, file20.getName()));\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // Snap root directory should show the file moved in as a new file.\n    verifyDiffReport(sub1, \"s3\", \"s4\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file20\")));\n\n    // Snap descendant directories don't have visibility to the moved in file.\n    verifyDiffReport(subSub, \"s3\", \"s4\",\n        new DiffReportEntry[]{});\n    verifyDiffReport(subSubSub, \"s3\", \"s4\",\n        new DiffReportEntry[]{});\n\n    hdfs.rename(new Path(sub1, file20.getName()),\n        new Path(subSub, file20.getName()));\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // Snap root directory now shows the rename as both source and\n    // destination paths are under the snap root.\n    verifyDiffReport(sub1, \"s4\", \"s5\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.RENAME,\n            DFSUtil.string2Bytes(\"file20\"),\n            DFSUtil.string2Bytes(\"subsub1/file20\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")));\n\n    // For the descendant directory under the snap root, the file\n    // moved in shows up as a new file created.\n    verifyDiffReport(subSub, \"s4\", \"s5\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file20\")));\n\n    verifyDiffReport(subSubSub, \"s4\", \"s5\",\n        new DiffReportEntry[]{});\n\n    // Case 4: Snapshot diff for the newly created descendant directory.\n    Path subSubSub2 = new Path(subSub, \"subsubsub2\");\n    hdfs.mkdirs(subSubSub2);\n    Path file30 = new Path(subSubSub2, \"file30\");\n    DFSTestUtil.createFile(hdfs, file30, BLOCKSIZE, REPLICATION_1, SEED);\n    hdfs.createFile(file30);\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    verifyDiffReport(sub1, \"s5\", \"s6\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub2\")));\n\n    verifyDiffReport(subSubSub2, \"s5\", \"s6\",\n        new DiffReportEntry[]{});\n\n    verifyDiffReport(subSubSub2, \"s1\", \"s2\",\n        new DiffReportEntry[]{});\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.rename",
            "location": "913–942",
            "code": "@SuppressWarnings(\"deprecation\")\n  @Override\n  public boolean rename(Path src, Path dst) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.RENAME);\n\n    final Path absSrc = fixRelativePart(src);\n    final Path absDst = fixRelativePart(dst);\n\n    // Try the rename without resolving first\n    try {\n      return dfs.rename(getPathName(absSrc), getPathName(absDst));\n    } catch (UnresolvedLinkException e) {\n      // Fully resolve the source\n      final Path source = getFileLinkStatus(absSrc).getPath();\n      // Keep trying to resolve the destination\n      return new FileSystemLinkResolver<Boolean>() {\n        @Override\n        public Boolean doCall(final Path p) throws IOException {\n          return dfs.rename(getPathName(source), getPathName(p));\n        }\n        @Override\n        public Boolean next(final FileSystem fs, final Path p)\n            throws IOException {\n          // Should just throw an error in FileSystem#checkPath\n          return doCall(p);\n        }\n      }.resolve(this, absDst);\n    }\n  }"
          },
          "invocation": {
            "location": "600–600",
            "code": "hdfs.rename(file20, new Path(subSub, file20.getName()));"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testSnapRootDescendantDiffReportWithRename",
            "location": "580–728",
            "code": "@Test\n  public void testSnapRootDescendantDiffReportWithRename() throws Exception {\n    Assume.assumeTrue(conf.getBoolean(\n        DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_DIFF_ALLOW_SNAP_ROOT_DESCENDANT,\n        DFSConfigKeys.\n            DFS_NAMENODE_SNAPSHOT_DIFF_ALLOW_SNAP_ROOT_DESCENDANT_DEFAULT));\n    Path subSub = new Path(sub1, \"subsub1\");\n    Path subSubSub = new Path(subSub, \"subsubsub1\");\n    Path nonSnapDir = new Path(dir, \"non_snap\");\n    hdfs.mkdirs(subSubSub);\n    hdfs.mkdirs(nonSnapDir);\n\n    hdfs.allowSnapshot(sub1);\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n    Path file20 = new Path(subSubSub, \"file20\");\n    DFSTestUtil.createFile(hdfs, file20, BLOCKSIZE, REPLICATION_1, SEED);\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // Case 1: Move a file away from a descendant dir, but within the snap root.\n    // mv <snaproot>/<subsub>/<subsubsub>/file20 <snaproot>/<subsub>/file20\n    hdfs.rename(file20, new Path(subSub, file20.getName()));\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // The snapshot diff for the snap root detects the change as file rename\n    // as the file move happened within the snap root.\n    verifyDiffReport(sub1, \"s1\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1\")),\n        new DiffReportEntry(DiffType.RENAME,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub1/file20\"),\n            DFSUtil.string2Bytes(\"subsub1/file20\")));\n\n    // The snapshot diff for the descendant dir <subsub> still detects the\n    // change as file rename as the file move happened under the snap root\n    // descendant dir.\n    verifyDiffReport(subSub, \"s1\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsubsub1\")),\n        new DiffReportEntry(DiffType.RENAME,\n            DFSUtil.string2Bytes(\"subsubsub1/file20\"),\n            DFSUtil.string2Bytes(\"file20\")));\n\n    // The snapshot diff for the descendant dir <subsubsub> detects the\n    // change as file delete as the file got moved from its scope.\n    verifyDiffReport(subSubSub, \"s1\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"file20\")));\n\n    // Case 2: Move the file from the snap root descendant dir to any\n    // non snap root dir. mv <snaproot>/<subsub>/file20 <nonsnaproot>/file20.\n    hdfs.rename(new Path(subSub, file20.getName()),\n        new Path(dir, file20.getName()));\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // The snapshot diff for the snap root detects the change as file delete\n    // as the file got moved away from the snap root dir to some non snap\n    // root dir.\n    verifyDiffReport(sub1, \"s2\", \"s3\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"subsub1/file20\")));\n\n    // The snapshot diff for the snap root descendant <subsub> detects the\n    // change as file delete as the file was previously under its scope and\n    // got moved away from its scope.\n    verifyDiffReport(subSub, \"s2\", \"s3\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"file20\")));\n\n    // The file was already not under the descendant dir <subsubsub> scope.\n    // So, the snapshot diff report for the descendant dir doesn't\n    // show the file rename at all.\n    verifyDiffReport(subSubSub, \"s2\", \"s3\",\n        new DiffReportEntry[]{});\n\n    // Case 3: Move the file from the non-snap root dir to snap root dir\n    // mv <nonsnaproot>/file20 <snaproot>/file20\n    hdfs.rename(new Path(dir, file20.getName()),\n        new Path(sub1, file20.getName()));\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // Snap root directory should show the file moved in as a new file.\n    verifyDiffReport(sub1, \"s3\", \"s4\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file20\")));\n\n    // Snap descendant directories don't have visibility to the moved in file.\n    verifyDiffReport(subSub, \"s3\", \"s4\",\n        new DiffReportEntry[]{});\n    verifyDiffReport(subSubSub, \"s3\", \"s4\",\n        new DiffReportEntry[]{});\n\n    hdfs.rename(new Path(sub1, file20.getName()),\n        new Path(subSub, file20.getName()));\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    // Snap root directory now shows the rename as both source and\n    // destination paths are under the snap root.\n    verifyDiffReport(sub1, \"s4\", \"s5\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.RENAME,\n            DFSUtil.string2Bytes(\"file20\"),\n            DFSUtil.string2Bytes(\"subsub1/file20\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")));\n\n    // For the descendant directory under the snap root, the file\n    // moved in shows up as a new file created.\n    verifyDiffReport(subSub, \"s4\", \"s5\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"file20\")));\n\n    verifyDiffReport(subSubSub, \"s4\", \"s5\",\n        new DiffReportEntry[]{});\n\n    // Case 4: Snapshot diff for the newly created descendant directory.\n    Path subSubSub2 = new Path(subSub, \"subsubsub2\");\n    hdfs.mkdirs(subSubSub2);\n    Path file30 = new Path(subSubSub2, \"file30\");\n    DFSTestUtil.createFile(hdfs, file30, BLOCKSIZE, REPLICATION_1, SEED);\n    hdfs.createFile(file30);\n    hdfs.createSnapshot(sub1, genSnapshotName(sub1));\n\n    verifyDiffReport(sub1, \"s5\", \"s6\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(\"subsub1\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"subsub1/subsubsub2\")));\n\n    verifyDiffReport(subSubSub2, \"s5\", \"s6\",\n        new DiffReportEntry[]{});\n\n    verifyDiffReport(subSubSub2, \"s1\", \"s2\",\n        new DiffReportEntry[]{});\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.allowSnapshot",
            "location": "2133–2159",
            "code": "/** @see org.apache.hadoop.hdfs.client.HdfsAdmin#allowSnapshot(Path) */\n  public void allowSnapshot(final Path path) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.ALLOW_SNAPSHOT);\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.allowSnapshot(getPathName(p));\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.allowSnapshot(p);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + path + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "592–592",
            "code": "hdfs.allowSnapshot(sub1);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.getAccessTime",
            "location": "1165–1167",
            "code": "private long getAccessTime(Path path) throws IOException {\n    return hdfs.getFileStatus(path).getAccessTime();\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus",
            "location": "1810–1841",
            "code": "/**\n   * Returns the stat information about the file.\n   *\n   * If the given path is a symlink, the path will be resolved to a target path\n   * and it will get the resolved path's FileStatus object. It will not be\n   * represented as a symlink and isDirectory API returns true if the resolved\n   * path is a directory, false otherwise.\n   *\n   * @throws FileNotFoundException if the file does not exist.\n   */\n  @Override\n  public FileStatus getFileStatus(Path f) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_FILE_STATUS);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<FileStatus>() {\n      @Override\n      public FileStatus doCall(final Path p) throws IOException {\n        HdfsFileStatus fi = dfs.getFileInfo(getPathName(p));\n        if (fi != null) {\n          return fi.makeQualified(getUri(), p);\n        } else {\n          throw new FileNotFoundException(\"File does not exist: \" + p);\n        }\n      }\n      @Override\n      public FileStatus next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.getFileStatus(p);\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "1166–1166",
            "code": "return hdfs.getFileStatus(path).getAccessTime();"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithRenameAndSnapshotDeletion",
            "location": "997–1030",
            "code": "/**\n   * Nested renamed dir/file and the withNameList in the WithCount node of the\n   * parental directory is empty due to snapshot deletion. See HDFS-6996 for\n   * details.\n   */\n  @Test\n  public void testDiffReportWithRenameAndSnapshotDeletion() throws Exception {\n    final Path root = new Path(\"/\");\n    final Path foo = new Path(root, \"foo\");\n    final Path bar = new Path(foo, \"bar\");\n    DFSTestUtil.createFile(hdfs, bar, BLOCKSIZE, REPLICATION, SEED);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    // rename /foo to /foo2\n    final Path foo2 = new Path(root, \"foo2\");\n    hdfs.rename(foo, foo2);\n    // now /foo/bar becomes /foo2/bar\n    final Path bar2 = new Path(foo2, \"bar\");\n\n    // delete snapshot s0 so that the withNameList inside of the WithCount node\n    // of foo becomes empty\n    hdfs.deleteSnapshot(root, \"s0\");\n\n    // create snapshot s1 and rename bar again\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n    final Path bar3 = new Path(foo2, \"bar-new\");\n    hdfs.rename(bar2, bar3);\n\n    // we always put modification on the file before rename\n    verifyDiffReport(root, \"s1\", \"\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"foo2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"foo2/bar\"),\n            DFSUtil.string2Bytes(\"foo2/bar-new\")));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.deleteSnapshot",
            "location": "2317–2344",
            "code": "@Override\n  public void deleteSnapshot(final Path snapshotDir, final String snapshotName)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.DELETE_SNAPSHOT);\n    Path absF = fixRelativePart(snapshotDir);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.deleteSnapshot(getPathName(p), snapshotName);\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.deleteSnapshot(p, snapshotName);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + snapshotDir + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "1018–1018",
            "code": "hdfs.deleteSnapshot(root, \"s0\");"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithRenameAndSnapshotDeletion",
            "location": "997–1030",
            "code": "/**\n   * Nested renamed dir/file and the withNameList in the WithCount node of the\n   * parental directory is empty due to snapshot deletion. See HDFS-6996 for\n   * details.\n   */\n  @Test\n  public void testDiffReportWithRenameAndSnapshotDeletion() throws Exception {\n    final Path root = new Path(\"/\");\n    final Path foo = new Path(root, \"foo\");\n    final Path bar = new Path(foo, \"bar\");\n    DFSTestUtil.createFile(hdfs, bar, BLOCKSIZE, REPLICATION, SEED);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    // rename /foo to /foo2\n    final Path foo2 = new Path(root, \"foo2\");\n    hdfs.rename(foo, foo2);\n    // now /foo/bar becomes /foo2/bar\n    final Path bar2 = new Path(foo2, \"bar\");\n\n    // delete snapshot s0 so that the withNameList inside of the WithCount node\n    // of foo becomes empty\n    hdfs.deleteSnapshot(root, \"s0\");\n\n    // create snapshot s1 and rename bar again\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n    final Path bar3 = new Path(foo2, \"bar-new\");\n    hdfs.rename(bar2, bar3);\n\n    // we always put modification on the file before rename\n    verifyDiffReport(root, \"s1\", \"\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"foo2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"foo2/bar\"),\n            DFSUtil.string2Bytes(\"foo2/bar-new\")));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.rename",
            "location": "913–942",
            "code": "@SuppressWarnings(\"deprecation\")\n  @Override\n  public boolean rename(Path src, Path dst) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.RENAME);\n\n    final Path absSrc = fixRelativePart(src);\n    final Path absDst = fixRelativePart(dst);\n\n    // Try the rename without resolving first\n    try {\n      return dfs.rename(getPathName(absSrc), getPathName(absDst));\n    } catch (UnresolvedLinkException e) {\n      // Fully resolve the source\n      final Path source = getFileLinkStatus(absSrc).getPath();\n      // Keep trying to resolve the destination\n      return new FileSystemLinkResolver<Boolean>() {\n        @Override\n        public Boolean doCall(final Path p) throws IOException {\n          return dfs.rename(getPathName(source), getPathName(p));\n        }\n        @Override\n        public Boolean next(final FileSystem fs, final Path p)\n            throws IOException {\n          // Should just throw an error in FileSystem#checkPath\n          return doCall(p);\n        }\n      }.resolve(this, absDst);\n    }\n  }"
          },
          "invocation": {
            "location": "1012–1012",
            "code": "hdfs.rename(foo, foo2);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithRpcLimit2",
            "location": "1352–1417",
            "code": "@Test\n  public void testDiffReportWithRpcLimit2() throws Exception {\n    final Path root = new Path(\"/\");\n    hdfs.mkdirs(root);\n    for (int i = 1; i <=3; i++) {\n      final Path path = new Path(root, \"dir\" + i);\n      hdfs.mkdirs(path);\n    }\n    for (int i = 1; i <= 3; i++) {\n      final Path path = new Path(root, \"dir\" + i);\n      for (int j = 1; j < 4; j++) {\n        final Path file = new Path(path, \"file\" + j);\n        DFSTestUtil.createFile(hdfs, file, BLOCKSIZE, REPLICATION, SEED);\n      }\n    }\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    Path targetDir = new Path(root, \"dir4\");\n    //create directory dir4\n    hdfs.mkdirs(targetDir);\n    //moves files from dir1 to dir4\n    Path path = new Path(root, \"dir1\");\n    for (int j = 1; j < 4; j++) {\n      final Path srcPath = new Path(path, \"file\" + j);\n      final Path targetPath = new Path(targetDir, \"file\" + j);\n      hdfs.rename(srcPath, targetPath);\n    }\n    targetDir = new Path(root, \"dir3\");\n    //overwrite existing files in dir3 from files in dir1\n    path = new Path(root, \"dir2\");\n    for (int j = 1; j < 4; j++) {\n      final Path srcPath = new Path(path, \"file\" + j);\n      final Path targetPath = new Path(targetDir, \"file\" + j);\n      hdfs.rename(srcPath, targetPath, Rename.OVERWRITE);\n    }\n    final Path pathToRename = new Path(root, \"dir2\");\n    //move dir2 inside dir3\n    hdfs.rename(pathToRename, targetDir);\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n    verifyDiffReport(root, \"s0\", \"s1\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir4\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2\"),\n            DFSUtil.string2Bytes(\"dir3/dir2\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file1\"),\n            DFSUtil.string2Bytes(\"dir4/file1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file2\"),\n            DFSUtil.string2Bytes(\"dir4/file2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file3\"),\n            DFSUtil.string2Bytes(\"dir4/file3\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file1\"),\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file2\"),\n            DFSUtil.string2Bytes(\"dir3/file2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file3\"),\n            DFSUtil.string2Bytes(\"dir3/file3\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir3\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file3\")));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.rename",
            "location": "913–942",
            "code": "@SuppressWarnings(\"deprecation\")\n  @Override\n  public boolean rename(Path src, Path dst) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.RENAME);\n\n    final Path absSrc = fixRelativePart(src);\n    final Path absDst = fixRelativePart(dst);\n\n    // Try the rename without resolving first\n    try {\n      return dfs.rename(getPathName(absSrc), getPathName(absDst));\n    } catch (UnresolvedLinkException e) {\n      // Fully resolve the source\n      final Path source = getFileLinkStatus(absSrc).getPath();\n      // Keep trying to resolve the destination\n      return new FileSystemLinkResolver<Boolean>() {\n        @Override\n        public Boolean doCall(final Path p) throws IOException {\n          return dfs.rename(getPathName(source), getPathName(p));\n        }\n        @Override\n        public Boolean next(final FileSystem fs, final Path p)\n            throws IOException {\n          // Should just throw an error in FileSystem#checkPath\n          return doCall(p);\n        }\n      }.resolve(this, absDst);\n    }\n  }"
          },
          "invocation": {
            "location": "1376–1376",
            "code": "hdfs.rename(srcPath, targetPath);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithRpcLimit2",
            "location": "1352–1417",
            "code": "@Test\n  public void testDiffReportWithRpcLimit2() throws Exception {\n    final Path root = new Path(\"/\");\n    hdfs.mkdirs(root);\n    for (int i = 1; i <=3; i++) {\n      final Path path = new Path(root, \"dir\" + i);\n      hdfs.mkdirs(path);\n    }\n    for (int i = 1; i <= 3; i++) {\n      final Path path = new Path(root, \"dir\" + i);\n      for (int j = 1; j < 4; j++) {\n        final Path file = new Path(path, \"file\" + j);\n        DFSTestUtil.createFile(hdfs, file, BLOCKSIZE, REPLICATION, SEED);\n      }\n    }\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    Path targetDir = new Path(root, \"dir4\");\n    //create directory dir4\n    hdfs.mkdirs(targetDir);\n    //moves files from dir1 to dir4\n    Path path = new Path(root, \"dir1\");\n    for (int j = 1; j < 4; j++) {\n      final Path srcPath = new Path(path, \"file\" + j);\n      final Path targetPath = new Path(targetDir, \"file\" + j);\n      hdfs.rename(srcPath, targetPath);\n    }\n    targetDir = new Path(root, \"dir3\");\n    //overwrite existing files in dir3 from files in dir1\n    path = new Path(root, \"dir2\");\n    for (int j = 1; j < 4; j++) {\n      final Path srcPath = new Path(path, \"file\" + j);\n      final Path targetPath = new Path(targetDir, \"file\" + j);\n      hdfs.rename(srcPath, targetPath, Rename.OVERWRITE);\n    }\n    final Path pathToRename = new Path(root, \"dir2\");\n    //move dir2 inside dir3\n    hdfs.rename(pathToRename, targetDir);\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n    verifyDiffReport(root, \"s0\", \"s1\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.CREATE,\n            DFSUtil.string2Bytes(\"dir4\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2\"),\n            DFSUtil.string2Bytes(\"dir3/dir2\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file1\"),\n            DFSUtil.string2Bytes(\"dir4/file1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file2\"),\n            DFSUtil.string2Bytes(\"dir4/file2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir1/file3\"),\n            DFSUtil.string2Bytes(\"dir4/file3\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file1\"),\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file2\"),\n            DFSUtil.string2Bytes(\"dir3/file2\")),\n        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(\"dir2/file3\"),\n            DFSUtil.string2Bytes(\"dir3/file3\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir3\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file1\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir3/file3\")));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.rename",
            "location": "944–976",
            "code": "/**\n   * This rename operation is guaranteed to be atomic.\n   */\n  @SuppressWarnings(\"deprecation\")\n  @Override\n  public void rename(Path src, Path dst, final Options.Rename... options)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.RENAME);\n    final Path absSrc = fixRelativePart(src);\n    final Path absDst = fixRelativePart(dst);\n    // Try the rename without resolving first\n    try {\n      dfs.rename(getPathName(absSrc), getPathName(absDst), options);\n    } catch (UnresolvedLinkException e) {\n      // Fully resolve the source\n      final Path source = getFileLinkStatus(absSrc).getPath();\n      // Keep trying to resolve the destination\n      new FileSystemLinkResolver<Void>() {\n        @Override\n        public Void doCall(final Path p) throws IOException {\n          dfs.rename(getPathName(source), getPathName(p), options);\n          return null;\n        }\n        @Override\n        public Void next(final FileSystem fs, final Path p)\n            throws IOException {\n          // Should just throw an error in FileSystem#checkPath\n          return doCall(p);\n        }\n      }.resolve(this, absDst);\n    }\n  }"
          },
          "invocation": {
            "location": "1384–1384",
            "code": "hdfs.rename(srcPath, targetPath, Rename.OVERWRITE);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithOpenFiles",
            "location": "1054–1163",
            "code": "/**\n   * Test Snapshot diff report for snapshots with open files captures in them.\n   * Also verify if the diff report remains the same across NameNode restarts.\n   */\n  @Test (timeout = 120000)\n  public void testDiffReportWithOpenFiles() throws Exception {\n    // Construct the directory tree\n    final Path level0A = new Path(\"/level_0_A\");\n    final Path flumeSnapRootDir = level0A;\n    final String flumeFileName = \"flume.log\";\n    final String flumeSnap1Name = \"flume_snap_1\";\n    final String flumeSnap2Name = \"flume_snap_2\";\n\n    // Create files and open a stream\n    final Path flumeFile = new Path(level0A, flumeFileName);\n    createFile(flumeFile);\n    FSDataOutputStream flumeOutputStream = hdfs.append(flumeFile);\n\n    // Create Snapshot S1\n    final Path flumeS1Dir = SnapshotTestHelper.createSnapshot(\n        hdfs, flumeSnapRootDir, flumeSnap1Name);\n    final Path flumeS1Path = new Path(flumeS1Dir, flumeFileName);\n    final long flumeFileLengthAfterS1 = hdfs.getFileStatus(flumeFile).getLen();\n\n    // Verify if Snap S1 file length is same as the the live one\n    Assert.assertEquals(flumeFileLengthAfterS1,\n        hdfs.getFileStatus(flumeS1Path).getLen());\n\n    verifyDiffReport(level0A, flumeSnap1Name, \"\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")));\n\n    long flumeFileWrittenDataLength = flumeFileLengthAfterS1;\n    int newWriteLength = (int) (BLOCKSIZE * 1.5);\n    byte[] buf = new byte[newWriteLength];\n    Random random = new Random();\n    random.nextBytes(buf);\n\n    // Write more data to flume file\n    flumeFileWrittenDataLength += writeToStream(flumeOutputStream, buf);\n\n    // Create Snapshot S2\n    final Path flumeS2Dir = SnapshotTestHelper.createSnapshot(\n        hdfs, flumeSnapRootDir, flumeSnap2Name);\n    final Path flumeS2Path = new Path(flumeS2Dir, flumeFileName);\n\n    // Verify live files length is same as all data written till now\n    final long flumeFileLengthAfterS2 = hdfs.getFileStatus(flumeFile).getLen();\n    Assert.assertEquals(flumeFileWrittenDataLength, flumeFileLengthAfterS2);\n\n    // Verify if Snap S2 file length is same as the live one\n    Assert.assertEquals(flumeFileLengthAfterS2,\n        hdfs.getFileStatus(flumeS2Path).getLen());\n\n    verifyDiffReport(level0A, flumeSnap1Name, \"\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(flumeFileName)));\n\n    verifyDiffReport(level0A, flumeSnap2Name, \"\");\n\n    verifyDiffReport(level0A, flumeSnap1Name, flumeSnap2Name,\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(flumeFileName)));\n\n    // Write more data to flume file\n    flumeFileWrittenDataLength += writeToStream(flumeOutputStream, buf);\n\n    // Verify old flume snapshots have point-in-time / frozen file lengths\n    // even after the live file have moved forward.\n    Assert.assertEquals(flumeFileLengthAfterS1,\n        hdfs.getFileStatus(flumeS1Path).getLen());\n    Assert.assertEquals(flumeFileLengthAfterS2,\n        hdfs.getFileStatus(flumeS2Path).getLen());\n\n    flumeOutputStream.close();\n\n    // Verify if Snap S2 file length is same as the live one\n    Assert.assertEquals(flumeFileWrittenDataLength,\n        hdfs.getFileStatus(flumeFile).getLen());\n\n    // Verify old flume snapshots have point-in-time / frozen file lengths\n    // even after the live file have moved forward.\n    Assert.assertEquals(flumeFileLengthAfterS1,\n        hdfs.getFileStatus(flumeS1Path).getLen());\n    Assert.assertEquals(flumeFileLengthAfterS2,\n        hdfs.getFileStatus(flumeS2Path).getLen());\n\n    verifyDiffReport(level0A, flumeSnap1Name, \"\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(flumeFileName)));\n\n    verifyDiffReport(level0A, flumeSnap2Name, \"\",\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(flumeFileName)));\n\n    verifyDiffReport(level0A, flumeSnap1Name, flumeSnap2Name,\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(flumeFileName)));\n\n    restartNameNode();\n\n    verifyDiffReport(level0A, flumeSnap1Name, flumeSnap2Name,\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY,\n            DFSUtil.string2Bytes(flumeFileName)));\n\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus",
            "location": "1810–1841",
            "code": "/**\n   * Returns the stat information about the file.\n   *\n   * If the given path is a symlink, the path will be resolved to a target path\n   * and it will get the resolved path's FileStatus object. It will not be\n   * represented as a symlink and isDirectory API returns true if the resolved\n   * path is a directory, false otherwise.\n   *\n   * @throws FileNotFoundException if the file does not exist.\n   */\n  @Override\n  public FileStatus getFileStatus(Path f) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_FILE_STATUS);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<FileStatus>() {\n      @Override\n      public FileStatus doCall(final Path p) throws IOException {\n        HdfsFileStatus fi = dfs.getFileInfo(getPathName(p));\n        if (fi != null) {\n          return fi.makeQualified(getUri(), p);\n        } else {\n          throw new FileNotFoundException(\"File does not exist: \" + p);\n        }\n      }\n      @Override\n      public FileStatus next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.getFileStatus(p);\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "1076–1076",
            "code": "final long flumeFileLengthAfterS1 = hdfs.getFileStatus(flumeFile).getLen();"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithRenameAndDelete",
            "location": "911–950",
            "code": "/**\n   * Renaming a file/dir then delete the ancestor dir of the rename target\n   * should be reported as deleted.\n   */\n  @Test\n  public void testDiffReportWithRenameAndDelete() throws Exception {\n    final Path root = new Path(\"/\");\n    final Path dir1 = new Path(root, \"dir1\");\n    final Path dir2 = new Path(root, \"dir2\");\n    final Path foo = new Path(dir1, \"foo\");\n    final Path fileInFoo = new Path(foo, \"file\");\n    final Path bar = new Path(dir2, \"bar\");\n    final Path fileInBar = new Path(bar, \"file\");\n    DFSTestUtil.createFile(hdfs, fileInFoo, BLOCKSIZE, REPLICATION, SEED);\n    DFSTestUtil.createFile(hdfs, fileInBar, BLOCKSIZE, REPLICATION, SEED);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    hdfs.rename(fileInFoo, fileInBar, Rename.OVERWRITE);\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n    verifyDiffReport(root, \"s0\", \"s1\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1/foo\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir2/bar\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil\n            .string2Bytes(\"dir2/bar/file\")),\n        new DiffReportEntry(DiffType.RENAME,\n            DFSUtil.string2Bytes(\"dir1/foo/file\"),\n            DFSUtil.string2Bytes(\"dir2/bar/file\")));\n\n    // delete bar\n    hdfs.delete(bar, true);\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s2\");\n    verifyDiffReport(root, \"s0\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1/foo\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir2\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"dir2/bar\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir1/foo/file\")));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.delete",
            "location": "996–1012",
            "code": "@Override\n  public boolean delete(Path f, final boolean recursive) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.DELETE);\n    Path absF = fixRelativePart(f);\n    return new FileSystemLinkResolver<Boolean>() {\n      @Override\n      public Boolean doCall(final Path p) throws IOException {\n        return dfs.delete(getPathName(p), recursive);\n      }\n      @Override\n      public Boolean next(final FileSystem fs, final Path p)\n          throws IOException {\n        return fs.delete(p, recursive);\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "941–941",
            "code": "hdfs.delete(bar, true);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithRenameAndDelete",
            "location": "911–950",
            "code": "/**\n   * Renaming a file/dir then delete the ancestor dir of the rename target\n   * should be reported as deleted.\n   */\n  @Test\n  public void testDiffReportWithRenameAndDelete() throws Exception {\n    final Path root = new Path(\"/\");\n    final Path dir1 = new Path(root, \"dir1\");\n    final Path dir2 = new Path(root, \"dir2\");\n    final Path foo = new Path(dir1, \"foo\");\n    final Path fileInFoo = new Path(foo, \"file\");\n    final Path bar = new Path(dir2, \"bar\");\n    final Path fileInBar = new Path(bar, \"file\");\n    DFSTestUtil.createFile(hdfs, fileInFoo, BLOCKSIZE, REPLICATION, SEED);\n    DFSTestUtil.createFile(hdfs, fileInBar, BLOCKSIZE, REPLICATION, SEED);\n\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    hdfs.rename(fileInFoo, fileInBar, Rename.OVERWRITE);\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n    verifyDiffReport(root, \"s0\", \"s1\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1/foo\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir2/bar\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil\n            .string2Bytes(\"dir2/bar/file\")),\n        new DiffReportEntry(DiffType.RENAME,\n            DFSUtil.string2Bytes(\"dir1/foo/file\"),\n            DFSUtil.string2Bytes(\"dir2/bar/file\")));\n\n    // delete bar\n    hdfs.delete(bar, true);\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s2\");\n    verifyDiffReport(root, \"s0\", \"s2\",\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir1/foo\")),\n        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(\"dir2\")),\n        new DiffReportEntry(DiffType.DELETE, DFSUtil.string2Bytes(\"dir2/bar\")),\n        new DiffReportEntry(DiffType.DELETE,\n            DFSUtil.string2Bytes(\"dir1/foo/file\")));\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.rename",
            "location": "944–976",
            "code": "/**\n   * This rename operation is guaranteed to be atomic.\n   */\n  @SuppressWarnings(\"deprecation\")\n  @Override\n  public void rename(Path src, Path dst, final Options.Rename... options)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.RENAME);\n    final Path absSrc = fixRelativePart(src);\n    final Path absDst = fixRelativePart(dst);\n    // Try the rename without resolving first\n    try {\n      dfs.rename(getPathName(absSrc), getPathName(absDst), options);\n    } catch (UnresolvedLinkException e) {\n      // Fully resolve the source\n      final Path source = getFileLinkStatus(absSrc).getPath();\n      // Keep trying to resolve the destination\n      new FileSystemLinkResolver<Void>() {\n        @Override\n        public Void doCall(final Path p) throws IOException {\n          dfs.rename(getPathName(source), getPathName(p), options);\n          return null;\n        }\n        @Override\n        public Void next(final FileSystem fs, final Path p)\n            throws IOException {\n          // Should just throw an error in FileSystem#checkPath\n          return doCall(p);\n        }\n      }.resolve(this, absDst);\n    }\n  }"
          },
          "invocation": {
            "location": "928–928",
            "code": "hdfs.rename(fileInFoo, fileInBar, Rename.OVERWRITE);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.verifyDiffReportForGivenReport",
            "location": "1471–1497",
            "code": "private void verifyDiffReportForGivenReport(Path dirPath, String from,\n      String to, SnapshotDiffReport report, DiffReportEntry... entries)\n      throws IOException {\n    // reverse the order of from and to\n    SnapshotDiffReport inverseReport =\n        hdfs.getSnapshotDiffReport(dirPath, to, from);\n    LOG.info(report.toString());\n    LOG.info(inverseReport.toString() + \"\\n\");\n\n    assertEquals(entries.length, report.getDiffList().size());\n    assertEquals(entries.length, inverseReport.getDiffList().size());\n\n    for (DiffReportEntry entry : entries) {\n      if (entry.getType() == DiffType.MODIFY) {\n        assertTrue(report.getDiffList().contains(entry));\n        assertTrue(inverseReport.getDiffList().contains(entry));\n      } else if (entry.getType() == DiffType.DELETE) {\n        assertTrue(report.getDiffList().contains(entry));\n        assertTrue(inverseReport.getDiffList().contains(\n            new DiffReportEntry(DiffType.CREATE, entry.getSourcePath())));\n      } else if (entry.getType() == DiffType.CREATE) {\n        assertTrue(report.getDiffList().contains(entry));\n        assertTrue(inverseReport.getDiffList().contains(\n            new DiffReportEntry(DiffType.DELETE, entry.getSourcePath())));\n      }\n    }\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.getSnapshotDiffReport",
            "location": "2445–2478",
            "code": "/**\n   * Get the difference between two snapshots, or between a snapshot and the\n   * current tree of a directory.\n   *\n   * @see DFSClient#getSnapshotDiffReportListing\n   */\n  public SnapshotDiffReport getSnapshotDiffReport(final Path snapshotDir,\n      final String fromSnapshot, final String toSnapshot) throws IOException {\n    statistics.incrementReadOps(1);\n    storageStatistics.incrementOpCounter(OpType.GET_SNAPSHOT_DIFF);\n    Path absF = fixRelativePart(snapshotDir);\n    return new FileSystemLinkResolver<SnapshotDiffReport>() {\n      @Override\n      public SnapshotDiffReport doCall(final Path p)\n          throws IOException {\n        return getSnapshotDiffReportInternal(getPathName(p), fromSnapshot,\n            toSnapshot);\n      }\n\n      @Override\n      public SnapshotDiffReport next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.getSnapshotDiffReport(p, fromSnapshot, toSnapshot);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + snapshotDir + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "1476–1476",
            "code": "hdfs.getSnapshotDiffReport(dirPath, to, from);"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testSnapshotDiffReportRemoteIterator2",
            "location": "1594–1605",
            "code": "@Test\n  public void testSnapshotDiffReportRemoteIterator2() throws Exception {\n    final Path root = new Path(\"/\");\n    hdfs.mkdirs(root);\n    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n    try {\n      hdfs.snapshotDiffReportListingRemoteIterator(root, \"s0\", \"\");\n    } catch (Exception e) {\n      Assert.assertTrue(e.getMessage().contains(\"Remote Iterator is\"\n          + \"supported for snapshotDiffReport between two snapshots\"));\n    }\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.snapshotDiffReportListingRemoteIterator",
            "location": "2346–2390",
            "code": "/**\n   * Returns a remote iterator so that followup calls are made on demand\n   * while consuming the SnapshotDiffReportListing entries.\n   * This reduces memory consumption overhead in case the snapshotDiffReport\n   * is huge.\n   *\n   * @param snapshotDir\n   *          full path of the directory where snapshots are taken\n   * @param fromSnapshot\n   *          snapshot name of the from point. Null indicates the current\n   *          tree\n   * @param toSnapshot\n   *          snapshot name of the to point. Null indicates the current\n   *          tree.\n   * @return Remote iterator\n   */\n  public RemoteIterator\n      <SnapshotDiffReportListing> snapshotDiffReportListingRemoteIterator(\n      final Path snapshotDir, final String fromSnapshot,\n      final String toSnapshot) throws IOException {\n    Path absF = fixRelativePart(snapshotDir);\n    return new FileSystemLinkResolver\n        <RemoteIterator<SnapshotDiffReportListing>>() {\n      @Override\n      public RemoteIterator<SnapshotDiffReportListing> doCall(final Path p)\n          throws IOException {\n        if (!DFSUtilClient.isValidSnapshotName(fromSnapshot) ||\n            !DFSUtilClient.isValidSnapshotName(toSnapshot)) {\n          throw new UnsupportedOperationException(\"Remote Iterator is\"\n              + \"supported for snapshotDiffReport between two snapshots\");\n        }\n        return new SnapshotDiffReportListingIterator(getPathName(p),\n            fromSnapshot, toSnapshot);\n      }\n\n      @Override\n      public RemoteIterator<SnapshotDiffReportListing> next(final FileSystem fs,\n          final Path p) throws IOException {\n        return ((DistributedFileSystem) fs)\n            .snapshotDiffReportListingRemoteIterator(p, fromSnapshot,\n                toSnapshot);\n      }\n    }.resolve(this, absF);\n\n  }"
          },
          "invocation": {
            "location": "1600–1600",
            "code": "hdfs.snapshotDiffReportListingRemoteIterator(root, \"s0\", \"\");"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testSnapshotDiffInfo",
            "location": "730–777",
            "code": "@Test\n  public void testSnapshotDiffInfo() throws Exception {\n    Path snapshotRootDirPath = dir;\n    Path snapshotDirDescendantPath = new Path(snapshotRootDirPath, \"desc\");\n    Path snapshotDirNonDescendantPath = new Path(\"/dummy/non/snap/desc\");\n    hdfs.mkdirs(snapshotDirDescendantPath);\n    hdfs.mkdirs(snapshotDirNonDescendantPath);\n\n    hdfs.allowSnapshot(snapshotRootDirPath);\n    hdfs.createSnapshot(snapshotRootDirPath, \"s0\");\n    hdfs.createSnapshot(snapshotRootDirPath, \"s1\");\n\n    INodeDirectory snapshotRootDir = cluster.getNameNode()\n        .getNamesystem().getFSDirectory().getINode(\n            snapshotRootDirPath.toUri().getPath())\n        .asDirectory();\n    INodeDirectory snapshotRootDescendantDir = cluster.getNameNode()\n        .getNamesystem().getFSDirectory().getINode(\n            snapshotDirDescendantPath.toUri().getPath())\n        .asDirectory();\n    INodeDirectory snapshotRootNonDescendantDir = cluster.getNameNode()\n        .getNamesystem().getFSDirectory().getINode(\n            snapshotDirNonDescendantPath.toUri().getPath())\n        .asDirectory();\n    try {\n      SnapshotDiffInfo sdi = new SnapshotDiffInfo(\n          snapshotRootDir,\n          snapshotRootDescendantDir,\n          new Snapshot(0, \"s0\", snapshotRootDescendantDir),\n          new Snapshot(0, \"s1\", snapshotRootDescendantDir));\n      LOG.info(\"SnapshotDiffInfo: \" + sdi.getFrom() + \" - \" + sdi.getTo());\n    } catch (IllegalArgumentException iae){\n      fail(\"Unexpected exception when constructing SnapshotDiffInfo: \" + iae);\n    }\n\n    try {\n      SnapshotDiffInfo sdi = new SnapshotDiffInfo(\n          snapshotRootDir,\n          snapshotRootNonDescendantDir,\n          new Snapshot(0, \"s0\", snapshotRootNonDescendantDir),\n          new Snapshot(0, \"s1\", snapshotRootNonDescendantDir));\n      LOG.info(\"SnapshotDiffInfo: \" + sdi.getFrom() + \" - \" + sdi.getTo());\n      fail(\"SnapshotDiffInfo construction should fail for non snapshot root \" +\n          \"or non snapshot root descendant directories!\");\n    } catch (IllegalArgumentException iae) {\n      // expected exception\n    }\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.createSnapshot",
            "location": "2233–2258",
            "code": "@Override\n  public Path createSnapshot(final Path path, final String snapshotName)\n      throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.CREATE_SNAPSHOT);\n    Path absF = fixRelativePart(path);\n    return new FileSystemLinkResolver<Path>() {\n      @Override\n      public Path doCall(final Path p) throws IOException {\n        return new Path(dfs.createSnapshot(getPathName(p), snapshotName));\n      }\n\n      @Override\n      public Path next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          return myDfs.createSnapshot(p);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + path + \" -> \" + p);\n        }\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "739–739",
            "code": "hdfs.createSnapshot(snapshotRootDirPath, \"s0\");"
          }
        },
        {
          "relationType": "Call",
          "parentMethod": {
            "entity": "org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testSnapshotDiffInfo",
            "location": "730–777",
            "code": "@Test\n  public void testSnapshotDiffInfo() throws Exception {\n    Path snapshotRootDirPath = dir;\n    Path snapshotDirDescendantPath = new Path(snapshotRootDirPath, \"desc\");\n    Path snapshotDirNonDescendantPath = new Path(\"/dummy/non/snap/desc\");\n    hdfs.mkdirs(snapshotDirDescendantPath);\n    hdfs.mkdirs(snapshotDirNonDescendantPath);\n\n    hdfs.allowSnapshot(snapshotRootDirPath);\n    hdfs.createSnapshot(snapshotRootDirPath, \"s0\");\n    hdfs.createSnapshot(snapshotRootDirPath, \"s1\");\n\n    INodeDirectory snapshotRootDir = cluster.getNameNode()\n        .getNamesystem().getFSDirectory().getINode(\n            snapshotRootDirPath.toUri().getPath())\n        .asDirectory();\n    INodeDirectory snapshotRootDescendantDir = cluster.getNameNode()\n        .getNamesystem().getFSDirectory().getINode(\n            snapshotDirDescendantPath.toUri().getPath())\n        .asDirectory();\n    INodeDirectory snapshotRootNonDescendantDir = cluster.getNameNode()\n        .getNamesystem().getFSDirectory().getINode(\n            snapshotDirNonDescendantPath.toUri().getPath())\n        .asDirectory();\n    try {\n      SnapshotDiffInfo sdi = new SnapshotDiffInfo(\n          snapshotRootDir,\n          snapshotRootDescendantDir,\n          new Snapshot(0, \"s0\", snapshotRootDescendantDir),\n          new Snapshot(0, \"s1\", snapshotRootDescendantDir));\n      LOG.info(\"SnapshotDiffInfo: \" + sdi.getFrom() + \" - \" + sdi.getTo());\n    } catch (IllegalArgumentException iae){\n      fail(\"Unexpected exception when constructing SnapshotDiffInfo: \" + iae);\n    }\n\n    try {\n      SnapshotDiffInfo sdi = new SnapshotDiffInfo(\n          snapshotRootDir,\n          snapshotRootNonDescendantDir,\n          new Snapshot(0, \"s0\", snapshotRootNonDescendantDir),\n          new Snapshot(0, \"s1\", snapshotRootNonDescendantDir));\n      LOG.info(\"SnapshotDiffInfo: \" + sdi.getFrom() + \" - \" + sdi.getTo());\n      fail(\"SnapshotDiffInfo construction should fail for non snapshot root \" +\n          \"or non snapshot root descendant directories!\");\n    } catch (IllegalArgumentException iae) {\n      // expected exception\n    }\n  }"
          },
          "childMethod": {
            "entity": "org.apache.hadoop.hdfs.DistributedFileSystem.allowSnapshot",
            "location": "2133–2159",
            "code": "/** @see org.apache.hadoop.hdfs.client.HdfsAdmin#allowSnapshot(Path) */\n  public void allowSnapshot(final Path path) throws IOException {\n    statistics.incrementWriteOps(1);\n    storageStatistics.incrementOpCounter(OpType.ALLOW_SNAPSHOT);\n    Path absF = fixRelativePart(path);\n    new FileSystemLinkResolver<Void>() {\n      @Override\n      public Void doCall(final Path p) throws IOException {\n        dfs.allowSnapshot(getPathName(p));\n        return null;\n      }\n\n      @Override\n      public Void next(final FileSystem fs, final Path p)\n          throws IOException {\n        if (fs instanceof DistributedFileSystem) {\n          DistributedFileSystem myDfs = (DistributedFileSystem)fs;\n          myDfs.allowSnapshot(p);\n        } else {\n          throw new UnsupportedOperationException(\"Cannot perform snapshot\"\n              + \" operations on a symlink to a non-DistributedFileSystem: \"\n              + path + \" -> \" + p);\n        }\n        return null;\n      }\n    }.resolve(this, absF);\n  }"
          },
          "invocation": {
            "location": "738–738",
            "code": "hdfs.allowSnapshot(snapshotRootDirPath);"
          }
        }
      ]
    }
  }
}